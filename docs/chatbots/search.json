[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to Train your Chatbot",
    "section": "",
    "text": "Preface\nIn November 2022, the world was introduced to ChatGPT, a large language model that quickly became the fastest-growing digital product in internet history. This groundbreaking technology marked a significant milestone in the 60-year-old field of artificial intelligence, providing users with an experience of interacting with a truly intelligent computer. ChatGPT, developed by OpenAI, is a prime example of a large language model, which is a type of artificial intelligence model designed to understand and generate human-like text based on the input it receives.\nLanguage models have been in development for many years, with significant advancements made in recent times. These models are trained on vast amounts of data, enabling them to generate coherent and contextually relevant responses to a wide range of prompts. The development of these models is a testament to the progress made in the field of artificial intelligence, and their potential applications are vast and varied.\nLLMs might seem like science fiction to many, and outside the realm of their possibilities for developers of more traditional type of software. However, this is not the case. The most powerful language models available today can be used via APIs to build all sorts of novel applications, from customer service chatbots, to intelligent journals, to smart assistants integrated to any type of traditional workflow, to writing and coding helpers, to videogames with a level of inmersion never seen before.\nUsing LLMs in practice is something at the reach of any software engineer, and it doesn’t require any special skills or knowledge about machine learning or artificial intelligence. But it does require some knowledge about how these models work, what are their inherent limitations, and how to circunvent some of the most common pitfalls in integrating LLMs in traditional applications.\nThis book will dive into the world of language models, focusing on large language models and their transformative potential. We will explore the inner workings of these models, their capabilities, and the limitations that come with them. By understanding how these models function, we can better appreciate their potential and learn how to use them effectively in various applications.\nThe book is designed for anyone who wants to learn how to use large language models (LLMs) to build practical applications. The book is suitable for anyone with basic programming skills, and we will not use any third-party frameworks or libraries beyond the OpenAI API. This means that what you will learn is universal to all chatbot APIs, and you can quickly adapt it to any existing framework.\nThe main goal of this book is to teach you how to use LLMs in practice without diving too deep into the technical details of how they work. We will cover the most essential techniques related to chatbots and LLMs, including standard prompt engineering techniques, several augmentation methods, and fine-tuning.\nThroughout the book, we will build a dozen or so applications from scratch, using LLMs and various techniques to ask questions of your documents, extract knowledge from natural text, and create stories automatically. Whatever your business domain or area of interest, from building user-facing chatbots to interact with your SaaS product to creating useful tools for office work or research, I promise you’ll find something useful in this book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "How to Train your Chatbot",
    "section": "Structure of the book",
    "text": "Structure of the book\nThe book is divided into three parts:\nPart 1: Principles\nThis section provides a comprehensive understanding of language models, their principles, inherent limitations, and the current state of the art. We will cover the following topics:\n\nUnderstanding Language Models: We will introduce the concept of language models, their history, and their evolution over time. We will discuss how these models are trained and the basic principles that guide their operation.\nCapabilities of Language Models: Here, we will explore the range of tasks that language models can perform, from simple text generation to more complex tasks such as translation, summarization, and question-answering.\nLimitations of Language Models: While language models have made significant strides, they are not without their limitations. We will discuss the challenges that remain in developing truly intelligent language models, including issues related to bias, explainability, and generalization.\n\nPart 2: Techniques\nIn this section, we will discuss three families of techniques that can be employed to harness the power of language models in applications:\n\nPrompting Techniques: These techniques involve carefully designing inputs (called prompts) to steer how language models generate responses. Often the difference between an almost perfect and a mediocre response is in the quality of the prompt. The different prompt techniques we will learn hereallow transforming an otherwise generic LLM into a useful and very specific answer engine.\nAugmentation Techniques: Language models can be enhanced by connecting them with other tools and resources, such as knowledge bases, APIs, and coding sandboxes. These techniques enable language models to access external information, improving their ability to generate accurate and contextually relevant responses, and integrating them into existing applications.\nFine-Tuning Techniques: Fine-tuning involves extending the capabilities of language models by directly modifying their weights and/or architecture. This can be done efficiently without requiring the training of models from scratch, enabling the rapid development of more advanced language models or specializing smaller models in domains where even larger models don’t work as well.\n\nPart 3: Applications\nThe final and largest part of the book is dedicated to building applications that leverage large language models. We will create 15 applications, ranging from simple chat-based tools to more complex systems, demonstrating the potential of these models in real-world scenarios. Some of the applications we will build include:\n\nChatbots: Typical chatbots that can engage in meaningful conversations with users, providing information, and answering questions in traditional scenarios such as customer service.\nText Analysys Tools: Tools that can summarize long texts, extract insights, and answer specific questions while providing references.\nCoding Helpers: Coding assistance tools, that can generate, modify, explain, and debug code, as well as translating code between different programming languages.\nData Analysis Tools: Tools that can analyze structured data, such as tables, and provide accuracte analytics, including predictions and visualizations.\nWriting Assistants: Tools that can enhance your writing providing ideas, outlines, and full drafts, as well as editing and criticizing existing text.\nResearch Assistants: Tools that can search the web for some specific information and build semi-structured reports on a given user question.\nStory Generators: Tools that can generate fictional stories complete with characters, dialogs, settings, and plots, in different styles and genres.\n\nAnd many more!\nBy the end of this book, readers will have a solid grasp of language models, their capabilities, and the techniques required to build applications that leverage their power. They will also have the knowledge and skills to make informed decisions when choosing frameworks and implementing solutions. And you will also have 15 prototype projects that you can extend and turn into your own products or showcase in your next coding interview.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#applications-and-source-code",
    "href": "index.html#applications-and-source-code",
    "title": "How to Train your Chatbot",
    "section": "Applications and source code",
    "text": "Applications and source code\nAll the applications built in this book are based on streamlit. You can check an online version of the demo gallery at https://llm-ebook.streamlit.app. It looks like this:\n\nAll the source code is available for supporters of the book, under a very permissive license that allows you to reuse it without limitation in your private projects. You can get the book in early access mode at https://store.apiad.net/l/chatbots. It’s the best way to help support this project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#notes-for-the-early-access-version",
    "href": "index.html#notes-for-the-early-access-version",
    "title": "How to Train your Chatbot",
    "section": "Notes for the Early Access version",
    "text": "Notes for the Early Access version\nThis is the Early Access version of How to Train your Chatbot. It is a work in progress, and as such, it may and surely contains typos and conceptual mistakes. The content, layout, and associated code might change drastically before the first edition is considered ready.\nPlease send any mistakes, typos, or questions you have to apiad@apiad.net. I’m really thankful for all suggestions and error reports.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/intro.html",
    "href": "part1/intro.html",
    "title": "1  What is a language model?",
    "section": "",
    "text": "The simplest language model: n-grams\nWe’ve been building statistical language models since the early days of AI. The n-gram model is one of the simplest ones, storing the probability of each n-gram’s occurrence. An n-gram is a collection of n words that appear together in common sentences. For example, in a 2-gram model, we count how many times pairs of words appear together in a large corpus, creating a table showing their frequency.\nAs we increase the n-grams to 3, 4, or 5, the table becomes extremely large. Before the internet revolution, Google built a massive n-gram model from the entire internet with up to 5-grams. However, since the combination of all 5 words in English is huge, we only store probabilities for the most common combinations, compressing the table and storing only the larger numbers. This makes our statistical language model an approximation of language.\nThis simple model counts words in a strict context when they’re within a specific window size together. It’s very explicit, as each n-gram has its probability or frequency recorded. To compress this model further, we use embeddings – representing discrete objects in continuous space. For instance, words can be represented as vectors in a 300-dimensional space.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a language model?</span>"
    ]
  },
  {
    "objectID": "part1/intro.html#word-embeddings",
    "href": "part1/intro.html#word-embeddings",
    "title": "1  What is a language model?",
    "section": "Word embeddings",
    "text": "Word embeddings\nEmbeddings aim to transform semantic properties from the original space into numerical properties of the embedding space. In the case of words, we want those that occur together in context to map to similar vectors and cluster in the embedding space where they’re often used together.\nWord2Vec, in 2011, was the first massively successful use of embeddings. They trained a large embedding model using statistics from text all over the internet and discovered an amazing property: directions in the embedding space can encode semantic properties.\nFor instance, if you go from France to Paris, the same vector needed to add to the word France to reach Paris is similar to the vector needed to add to the word United States to reach Washington. This showed that the semantic property is-capital-of was encoded as a specific direction in this space. Many other semantic properties were found encoded this way too.\nThis was an early example of how encoding words in a dense vector space can capture some of their semantics.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a language model?</span>"
    ]
  },
  {
    "objectID": "part1/intro.html#contextual-word-embeddings",
    "href": "part1/intro.html#contextual-word-embeddings",
    "title": "1  What is a language model?",
    "section": "Contextual word embeddings",
    "text": "Contextual word embeddings\nThe issue with Word2Vec is its assignment of a unique vector to each word, regardless of context. As words have different meanings in different contexts, many attempts were made to create contextual embeddings instead of static ones. The most successful attempt is the transformer architecture, with BERT being the first example. The first transformer paper revolutionized natural language processing (NLP) in artificial intelligence, providing a single tool to tackle various NLP problems.\nThe transformer generates a text representation or embedding that considers the entire content of a sentence, for example, or even a larger fragment of text. This means each word’s embedding changes according to its context. Additionally, a global embedding for an entire sentence or paragraph can be computed. Why does this matter? It connects to our previous discussion on vector representations and neural networks.\nNeural networks are among the most powerful machine learning paradigms we have. By using a single representation, we can find embeddings for text, images, audio, categories, and programming code. This enables machine learning across multiple domains using a consistent approach.\nWith neural networks, you can transform images to text, text to image, text to code or audio, etc. The first idea of the transformer was to take a large chunk of text, obtain an embedding, and then use a specific neural network for tasks like text classification or translation. However, sequence-to-sequence architectures were developed, allowing neural networks to receive a chunk of text, embed it into a real-value vector, and generate a completely different chunk of text.\nFor example, in translation, you can encode a sentence in English with a transformer that embeds it into a real-value vector and then decode it in another transformer that “speaks” French. The real-value vector in the middle represents the meaning of the text independent of language. So you can have different encoders and decoders for various languages and translate any language pair.\nOne cool aspect is that you can train on pairs of languages like English-Spanish and German-French and then translate from English to French without ever training on that specific pair. This is due to the internal representation used by all languages. The sequence-to-sequence transformer is a fundamental piece behind technologies like ChatGPT. The next step is training it on massive amounts of text and teaching it to generate similar text.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a language model?</span>"
    ]
  },
  {
    "objectID": "part1/intro.html#large-language-models",
    "href": "part1/intro.html#large-language-models",
    "title": "1  What is a language model?",
    "section": "Large Language Models",
    "text": "Large Language Models\nLarge language models are the latest development in statistical language modeling, evolving from N-Gram models, embeddings, and transformers. These advanced architectures can compute contextual embeddings for extensive text contexts, thanks to innovations that efficiently accommodate thousdands of words in memory. This capacity has increased continuously, with the first version of ChatGPT holding something like 4000 words, and recently Google Gemini’s claim to hold over 1 million words in the context.\nA significant change is the scale of data these models are trained on. BERT was trained on a vast dataset for its time, but it pales in comparison to GPT-2, 3, and 4. Large language models learn from a massive amount of internet text, including technical texts, books, Wikipedia articles, blog posts, social media, news, and more. This exposure to diverse text styles and content allows them to understand various mainstream languages.\nLarge language models, like GPT-2, generate text by predicting the next word in a sentence or paragraph, just like all previous language models. But when you combine the massive scale of the data and computational resources put into making these beasts of language models, and some clever tricks on top, they become something completely beyond what anyone thought possible.\nGPT-2 was a huge leap forward in terms of coherent text generation. Given an initial prompt–say, the introductory paragraph of a fictional story–the model would generate token after token creating a mostly coherent story full with fictional characters and a plot. After a while it would start to diverge, of course, but for short fragments of text, this was already mindblowing.\nHowever, it is with GPT-3 that things really exploded. with GPT-3’s size, emerging capabilities like “in-context learning” appear. And this is where our story really begins.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a language model?</span>"
    ]
  },
  {
    "objectID": "part1/training.html",
    "href": "part1/training.html",
    "title": "2  How do LLMs work?",
    "section": "",
    "text": "How to make an LLM\nHow do you make your language model work? There are three main steps.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How do LLMs work?</span>"
    ]
  },
  {
    "objectID": "part1/training.html#how-to-make-an-llm",
    "href": "part1/training.html#how-to-make-an-llm",
    "title": "2  How do LLMs work?",
    "section": "",
    "text": "Pre-training\nThe first step is called self-supervised pretraining. In this step, you take a raw transformer architecture with uninitialized weights and train it on a massive amount of data to predict the next token. You use a large corpus of data, such as news, internet blog posts, articles, and books, and train the model on trillions of words.\nThe simplest training method is next token prediction. You show the model a random text and ask it what the next token is. Take a random substring from the dataset, remove the last token, show the prefix to the model, and ask for likely continuations. Compute a loss function to determine how mistaken the model was in its predictions and adjust it slightly to improve future predictions.\nSo far this is a standard machine learning approach. We call it self-supervised learning because the targets are not given by humans, but chosen automatically from the input. But deep down, this is just supervised learning at scale.\nNow, that being said, scaling this training process to billions of parameters and trillions of tokens presents a massive engineering challenge. No single supercomputer in the world can handle training GPT-4 from scratch, so you must resort to distributed systems to split the model across used across hundreds or thousands of GPUs for extended periods of time, synchronizing different parts of the model across multiple computers is crucial for efficient training. This just means, while the conceptual part of training an LLM is pretty straightforward, it is nothing short of an engineering prowess to get build like GPT-4.\nOnce pre-training is completed, you have what is called a “base model”, a language model that can continue any sentence in a way that closely resembles existing text. This model is already extremely powerful. Give it any prefix of text with any content whatsoever and the model will complete it with a mostly coherent continuation. It’s really autocompletion on steroids!\nHowever, these base models, as powerful as they are, are still very hard to prompt. Crucially, they do not understand precise instructions, mostly because their training data doesn’t contain a lot of examples of instructions. They are just stochastic parrots, in a sense. The next step is to get tame them.\n\n\nInstruction tuning\nAt this point the LLM already has all the knowledge in the world somewhere hidden in its weights–metaphorically speaking–but it is very hard to locate any concrete piece of knowledge. You must juggle with transforming questions into the right prompts to find a pattern that matches what the model has seen.\nThe way to solve this problem is to include another training phase, but this time much shorter and focused on a very well-curated dataset of instructions and responses. Here, the quality is crucial, much more than the quantity. You won’t teach the model anything new, you will just tune it to expect instruction-like inputs and produce answer-like outputs.\nOnce finished, you have what’s called an instruction-tuned model. These models are much more robust and easy to prompt compared to the base model, and this is the point where most open-source models end. But this is not the end of the story.\nInstruction-tuned models are still not suitable for publicly-facing products for one crucial reason: they can be coerced into answering anything at all, including producing biased, discriminatory, or hate speech and instructions on how to build bombs and deadly poisons.\nGiven base models are trained on the whole Internet, they are full of all the good and bad you can read online–although some effort is put into cleaning the pretraining dataset, but it’s never enough. We must teach the model that some questions are better left unanswered.\n\n\nPreference tuning\nThe final step is to fine-tune the model to produce answers that are more closely aligned with user preferences. This can and is primarily used to avoid biased or hate speech, and to reject any questions that are deemed unethical by the developers training the model. However, it also has the effect of making the model more polite in general, if you wish so.\nThe way this process works is to turn the problem from supervised learning into the real of reinforcement learning. In short, the main difference is that, while in supervised learning we give the model the correct answers (as in instruction tuning), in reinforcement learning we don’t have access to ground truth answers.\nInstead, we use an evaluator that ranks different answers provided by the LLM, and a feedback loop that teaches the LLM to approximate that ranking. In its original inception, this process was performed with a human evaluator, thus giving raise to the term “reinforcement learning with human feedback”, but since including humans makes this process slower and more more expensive, smaller organizations have turned to using other models as evaluators.\nFor example, if you have one strong model, like GPT-4, you can use it to rank responses by a smaller, still in-training model. This is one example of a more general concept in machine learning called “knowledge distillation” in which you attemp to compact the knowledge of a larger model into a smaller model, gaining in efficiency without sacrificing too much in performance.\nAnd finally, we have now something that works like GPT-4. The process was long and expensive: a massive pretraining following by a carefully curated instruction tuning and a human-backed preference tuning. This is the reason why so few organizations have the resources to train a state-of-the-art large language model.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How do LLMs work?</span>"
    ]
  },
  {
    "objectID": "part1/training.html#beyond-pure-language-modeling",
    "href": "part1/training.html#beyond-pure-language-modeling",
    "title": "2  How do LLMs work?",
    "section": "Beyond pure language modeling",
    "text": "Beyond pure language modeling\nOnce a model is deployed into production, the most basic application you can implement is a ChatGPT-clone: a chat interface where you can interact with a powerful model and get it to work for you. But this is far from the limit of what current models can do.\nWith careful prompting and some augmentation techniques, you can get integrate an LLM intro more traditional applications to work either as a powerful natural language frontend, or as a backend tool for language understanding. This is where LLMs can really shine, beyond the basic chatbot application.\nYou have to be careful, though. There are many common pitfals to using these models, including some inherent limitations like the dreaded hallucinations, which, although can be mitigated to a certain extent, are probably impossible to solve altogether without a paradigm shift, as we’ll see in the next chapter.\nHowever, despite their many limitations, large language models are one of the most transformative computational tools we’ve ever invented. Learning to harness their power will supercharge your skills, in whatever field you are working.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How do LLMs work?</span>"
    ]
  },
  {
    "objectID": "part1/capabilities.html",
    "href": "part1/capabilities.html",
    "title": "3  What can LLMs do?",
    "section": "",
    "text": "What can base models do?\nAutocompletion on steroids, as cool as it sounds, doesn’t really ring like anything smart, right? Well, it turns out, if your are very very good completing any text prefix, that implies you must be good at a wide range of cognitive tasks.\nFor example, suppose you want to build a question-answering engine. Take a question like “Who is the current president of the United States”, and turn it into a prompt like “the current president of the United States is…”. If you feed this to a powerfull base LLM, the most likely continuation represents the correct answer to the question. This means, autocomplete on steroids gives you question answering for free.\nAnd you can do this for a whole lot of tasks. Just turn them into an appropriate prefix and continuation. Do you want to translate a sentence? Use the prompt like “An English translation of the previous sentence is…” Do you want to summarize a text? Use a prompt like “A summary of the previous text is…” You get the point.\nBut it goes much farther than that! The scientists at OpenAI discovered that models the size of GPT-3 and above where capable of inferring the semantics of a task given examples, without explicitly telling tham what is the task. This is called in-context learning, and it works wonders. For example, if you want to use an LLM for sentiment analysis, you can use a prompt like the following.\nThat is, you build a prompt with a few examples of inputs and outputs and feed that to the LLM, leaving the last input unanswered. The most likely continuation is the right answer to the last input, so provided the base model has seen similar tasks in its training data, it will pick up the pattern and answer correctly most of the time.\nIn-context learning is a surprising discovery at first, but when you look deep down, it makes total sense. Since base LLMs are completion machines, provided they have seen examples of some arbitrary task in their training set, all you need to do is come up with a text prefix that makes the model “remember” that task. And that prefix is often just a set of examples of a given task, because that is actually what is stored in the LLM weights: a loosely and implicitely connected set of similar text fragments.\nIn a sense, the input to the LLM is a key to retrieve a part of its training set, but not in an accurate way. Since LLMs only store correlations between words, anything you “retrieve” from an LLM is a fuzzy approximation and aggregation of several (possibly millions) of similar training examples. For this reason, we say base models already “know” everything, but it’s very hard for them to “remember” it, because you have to find the right key–i.e., the right context prefix.\nBut what if we could teach the LLM that some arbitary instruction is equivalent to the right key for a given task? That is exactly what instruction tuning is about. By showing the LLM input/output pairs of, this time, precise instructions and the corresponding answer, we are rewiring some of its parameters to strengthen the correlation between the instruction and the response. In a sense, fine-tuning is like finding a path between the input space and the output space in the base model’s fuzzy web of word correlations and connect those two subspaces of words with a shortcut, so next time you input the instruction, the LLM will “remember” where is the appropriate answer.\nIf all of this sounds overly anthropomorphic, it is because we have stretched the analogies a bit to make it easier to understand. In reality, there is no “remembering” or “knowing” happening inside a large language model, at least not in any way akin to how human memory and reasoning works. We will talk about this difference and its implication in Chapter 4. For the time being, please be cognizant that any analogy between LLMs and human brains is bound to break pretty soon and cause major misunderstanding if taken too seriously.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>What can LLMs do?</span>"
    ]
  },
  {
    "objectID": "part1/capabilities.html#use-cases-for-fine-tuned-llms",
    "href": "part1/capabilities.html#use-cases-for-fine-tuned-llms",
    "title": "3  What can LLMs do?",
    "section": "Use cases for fine-tuned LLMs",
    "text": "Use cases for fine-tuned LLMs\nWith a proper fine tuning in a concrete domain, you can turn LLMs into task-specific models for a huge variety of linguistic problems. In this section we’ll review some of the most common tasks in which LLMs can be deployed. These will be rather abstract tasks, and in ?sec-applications we will see many practical applications involving combinations of these tasks.\nWhen discussing the use cases of fine-tuned LLMs, we don’t talk about an “input prefix” anymore because even if, technically, that is still what we are feeding the LLM, the response is not necessarily a direct, human-like continuation of the text. Instead, depending on which dataset it was fine-tuned, the LLM will respond with something that looks more like an answer to a question or an instruction than a pure continuation. Actually, if you give a fine-tuned LLM like GPT-4 an incomplete text prefix, it will often reply back with something like “I didn’t understand you entirely, but it appears what you are trying to do is […]” instead of casually continuing where you left.\nThus, it is often best to interpret this process as “prompting” the LLM with an instruction, and this is the reason why the input text is called a “prompt”, and the process of designing, testing, and optimizing these prompts is called, sometimes undeservedly, “prompt engineering”.\n\nText generation\nThe simplest, most straightforward use case for large language models is of course text generation, whether for fictional content as for technical articles, office work, homework, emails, and anything in-between. But instead of using a base model, where you have to provide a prefix to continue, an instruction-tuned model can be instructed directly to write a paragraph, passage, or even a short essay on a given topic. Depending on how powerful and well-trained the model is, you can even provide hints about the intended audience, the complexity of the language to use, etc.\nText generation–and all instructions in general–often works better the more descriptive the prompt. If you simply ask the LLM to “tell me a fairy story”, yes, it will come up with something plausible, and it might even surprise in the good way. But you most likely want to have finer control over the result, and thus crafting a well-structured and informative prompt is crucial. In Chapter 6 we will learn the most basic strategies to create effective prompts.\nA common issue in text generation, especially in longer formats, is that the LLM can and will often steer away from the main points in the discourse. The longer the response, the most likely some hallucinations will happen, which may be in the form of incoherent or plain contradictory items, e.g., characters acting “out of character” if you’re generating fiction.\nA battle-tested solution for generating coherent, long-form text is the divide-and-conquer approach. Instead of asking for a full text from the begining, prompt the LLM to first generate an outline of the text, and then, sequentially, ask it to fill in the sections and subsections, potentially feeding it with previously generated content to help it mantain consistency. In Chapter 19 we will see this approach taken to one extreme.\n\n\nSummarization\nSummarization is one of the most common and well-understood use cases of LLMs. In a sense, it is a special case of text generation–what isn’t, right?–but it has specific quirks that merit a separate discussion. In general, LLMs excel at summarizing. After all, that’s what they’ve been implicitely trained to do: construct a statistical model of the whole internet, which is rather, in a sense, a summary of the whole human knowledge.\nHowever, summarization isn’t a trivial problem. Besides the usual concerns about the audience, complexity of the language, style, etc., you will probably also want to control which aspects of the original text is the LLM focusing on. For example, rather than a simple compactation of the text, you might want a summary that emphasizes the consequences of whatever is described in the original text, or that highlights and contrasts the benefits and limitations. This is a more abstract form of summary that produces novel value, beyond just being a shorter text.\nThere are important caveats with summarization, though. As we’ll see in Chapter 4, LLMs are very prone to hallucination, and the more you push the boundary between a plain summary and something closer to a critical analysis, the more the LLM will tend to ignore the original text and rely on its own pre-trained knowledge.\nAnd just like before, the best way to counteract any form of rebellious generation is to be very intentinal in your prompt, and make it as structured as necessary. For example, you can first ask the LLM to extract the key points, advantages, and limitations. Then, ask it to cluster the advantages and limitations according to whatever criteria you are seeking. And only then, ask it to provide a natural language summary of that semi-structured analysis. This gives you finer control over the end result and will tend to reduce hallucinations, at the same time being easier to debug, since you can see the intermediate steps.\n\n\nTranslation & style transfer\nThe text-to-text transformer architecture (the precursor and core component of modern language model) was originally design for translation. By encoding the input sentence into a latent space of word correlations, detached from a specific language, and then decoding that sentence in a different vocabulary, these models achieves state-of-the-art translation in the early 2018s. The more general notion of style transfer is, deep down, a translation problem, but instead of between English and French, say, between technical and plain language.\nModern LLMs carry this capability, and will be more than enough for many practical translation tasks. However, beware that plenty of studies show that LLM translation are often poorer in many linguistic notions from professional translations. Translation is an art, as much or more than it is a science. It involves a deep knowledge of the cultural similarities and differences between readers of both languages, to correctly capture all the nuances that even a seemingly simple phrase can encode.\nThat being said, LLMs can help bridge the gap for non-native speakers in many domains where you don’t need–or can’t hope for–a professional translation. An example is inter-institutional communication, e.g., emails from co-workers that don’t speak your native language. In these cases, you must also be careful nothing important is lost in translation, literally, but as long as everyone is aware of the limitations, this is one of the most practical uses cases for LLMs.\n\n\nStructured generation\nContinuing with the topic of text generation capabilities, our next stop is generation from structured data. This is one specific area where LLMs come to mostly solve a long-standing problem in computer science: to generate human-sounding explanations of dry, structured data.\nExamples of this task are everywhere. You can generate a summary of your calendar for the day, and pass it to a speech synthesis engine, so your personal assistant can send you every morning an audio message reminding you what you have to do, with cute linguistic cues like “Oh, and on the way to the office, remember to pick up the your wife’s present.” We will see an example of this functionality in ?sec-planner.\nOther examples include generating summaries of recent purchases for a banking app or product descriptions for an online store–basically anywhere you’d have a dashboard full of numbers and stats, you can have an LLM generate a natural language description of what’s going on. You can pair this capability with the super skills LLMs have for question answering (at least when the answer is explicit in the context) to construct linguistic interfaces to any of number of online services or apps. We will exploit this capability in many of our demos, starting with ?sec-shoping where we build a bot to help you do your shoping.\n\n\nText classification\nText classification is the problem of categorizing a text fragment–be it a single sentence, a whole book, or anything in between–into one of a fixed set of categories. Examples vary from categorizing comments as positive/neutral/negative, determining if an email is spam or not, or detecting the tone and style of a text, to more specific tasks like extracting the intended intention from a user, e.g., chatting with an airline bot.\nTo have an LLM correctly and robustly classify your text, it is often not enough to just instruct it and provide the intended categories. The LLM might come up with a new category you didn’t mention just because it makes sense in that context. And negative instructions, in general, don’t work pretty well. In fact, LLMs are lousy at interpreting negative instructions precisely because of the underlying statistical model. We will see in Section 4.2 why this is the case.\nInstead of a dry, zero-shot instruction, you can improve the LLM classification capabilities substantially with a few examples (also called a k-shot instruction). It works even better if you select the examples dynamically based on the input text, a procedure that eerily similar to k-NN classification but in the world of LLMs. Furthermore, many LLMs tend to be chatty by design, and will often fail to provide a single word classification even if you instruct it to. You can mitigate this by using a structured response prompt, as seen in Chapter 6.\n\n\nStructured data extraction\nA generalization of text classification is the problem of structured data extraction from natural language. A common example is extracting mentions of people, dates, and tasks in a text, for example, a transcription from a video meeting. In the more general case, you can extract any entity-relation schema from natural text and build a structured representation of any domain.\nBut this capability goes much further. If you have any kind of structured input format–e.g., an API call for any online service–you can instruct (and probably k-shot) an LLM to produce the exact JSON-formatted input given a user query. This is often encapsulated in modern LLM providers i a functionality called “function calling”, which we will explore in Chapter 8.\nAs usual, the main caveat with structured generation is the potential for subtle hallucinations. In this case they can be in two forms. The simplest one, when the LLM fails to produce the expected format by, e.g., missing a key in JSON object or providing an invalid type. This type of error is what we call a syntactic hallucination and, although anoying, is often trivial to detect and correct, even if just by retrying the prompt.\nThe second form of hallucination is much more insidious: the response can be in the right format, and all values have the right type, but they don’t match what’s in the text. The LLM hallucinated some values. The reason this is a huge problem is because detecting this form of semantic hallucination is as hard to solve as hallucinations in general. As we’ll see in Section 4.5, we simply have no idea how to ensure an LLM always produce truthful responses, and it might be impossible even in principle.\n\n\nQuestion answering\nQuestion answering is one of the most surprising capabilities of sufficiently large language models. To some extent, question answering can be seen as a form of retrieval, when you’re asking about some facts explicitly mentioned in the training set. For example, if you ask “Who wrote The Illiad”, it is not surprinsing given what whe know of LLMs, that a fine-tuned model can easily generate “Homer” as the most plausible response. The sentence “Homer wrote The Illiad” must have appeared thousands of times in different ways in the training set.\nBut modern LLMs can go way beyond simply retrieving the right answer to a trivia question. You can ask questions that involve a small set of reasoning steps, combining facts here and there to produce a response that is not, at least explicitly, in the training set. This is rather surprising because there is no explicit reasoning mechanism implemented in LLMs. All forms of reasoning that can be said to happen are an emergent consequence of learning to predict the next token, and that is at least very intriguing.\nIn any case, as we will argue in Section 4.2, the statistical modelling paradigm has some inherent limitations that restrict the types of reasoning that LLMs can do, even in principle. THis doesn’t mean that, in practice, for the types of problems you encounter, it can’t work. But in its most general form, long-term reasoning and planning is still an open problem in artificial intelligence, I don’t think LLMs alone are equiped to solve it.\nYou can, however, plug LLMs with external tools to enhance its reasoning skills. One of the most fruitful research lines is to have them generate code to solve a problem, and then run it, effectively making LLMs Turing-complete, at least in principle, even if in practice they may fail to generate the right code. Which leads us to the next use case.\n\n\nCode generation\nSince LLMs are trained to autocomplete text, it may not be that surprising that, when feed with enough training examples of code in several programming languages, they can generate small snippets of mostly correct code. However, for anyone who codes, it is evident that writing correct code is not as simple as just concatenating plausible continuations. Programming languages have much stricter syntax rules that require, e.g., to close all parenthesis and to use explicit and very tight naming conventions. Failing to produce even a single semicolon in the right place can render a program unusable.\nFor this reason, it is at least a bit surprising that LLMs can code. More surprising still, that they can not only autocomplete existing code, but generate code from scratch given natural language instructions. This is one of the most powerful capabilities in terms of integrating LLMs with external tools because code is, by definition, the most general type of external tool. There is nothing you can do in a computer, that you can’t do with code.\nThe simplest use case in this domain is of course using LLMs as coding assistants embedded in developer tools like code editors. But this is just scratching the surface. As implied in the above section, you can have an LLM generate code to solve a problem it would otherwise fail to answer correctly–e.g., perform some complex physics computations. Code generation allows an LLM to analyze large collections of data by computing some statistics and running some formulas. You can even have an LLM generate the code to output some chart, and voilá, you just taught the LLM to draw!\nGiven how powerful code generation as an LLM skill is, we will spend a significant amount of time in ?sec-applications exploting this capability in different demo applications. For example, we will make a data analysis bot in Chapter 17 that can answer questions about some dataset in natural language, but also output charts and tables.\n\n\nCode explanation\nCode explanation is the inverse problem of code generation: given some existing code, produce a natural language explanation or, more generally, answer questions about it. In principle, this is a form of question answering that involves all the caveats about complex reasoning we have already discussed. But it gets harder.\nThe problem is the majority of the most interesting questions about code cannot be answer in general: they are undecidable, meaning no algorithm can exist that will always produce the right response. The most poignant example is the question “Does this function ever returns?”. This is the well-known Halting problem, the most famous problem in computability theory, and the grandfather of all undecidability results. Similar questions such as whether a variable is ever assigned, or a method is ever called, are also undecidable in the general case.\nAnd this is not just a theoretical issue. The Halting problem highlights one crucial aspect of computation: in the general case you cannot predict what an algorithm will do without running it. However, in practice, as anyone who codes knows, you can predict what lots of your code will do, if only because it is similar to code you’ve written before. And this is where LLMs shine: learning to extrapolate from patterns to novel specific instances, even if the general problem is unsolvable.\nTo top it all, we can easily imagine an LLM that, when prompted with a question that seemingly cannot be answered from the code alone, could decide to run the code with some specific parameters and observe its results, drawing conclusions not from the syntax alone but from the execution logs. A debugging agent, if you will. In Chapter 20 we will play a little bit with this idea.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>What can LLMs do?</span>"
    ]
  },
  {
    "objectID": "part1/capabilities.html#conclusions",
    "href": "part1/capabilities.html#conclusions",
    "title": "3  What can LLMs do?",
    "section": "Conclusions",
    "text": "Conclusions\nThese are the most important high-level tasks where LLMs can be deployed, but they span hundreds if not thousands of potential applications. Text classification, for example, covers a huge range of applications, just changing the classification target. In part 3 of the book we will explore many of these tasks, and combinations thereof, in concrete applications.\nOne conclusion you can take from this chapter is that LLMs are some of the most versatile digital technologies we’ve ever invented. While we don’t know if artificial general intelligence is anywhere near, we’re definitely one step closer to general-purpose AI–models that can be easily adapted to any new domain without research or costly training procedures.\nHowever, language modeling is not magic. We have already seen a glimpse of some of the fundamental limitations of this paradigm in the above dicussion. Next chapter, we will go deeper into how these models learn compared to humans, and what this difference entails in terms of their limitations.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>What can LLMs do?</span>"
    ]
  },
  {
    "objectID": "part1/limitations.html",
    "href": "part1/limitations.html",
    "title": "4  The limits of language models",
    "section": "",
    "text": "How do LLMs learn?\nRegarding the differences between how large language models and humans learn language, a few key points stand out. First, the scale is vastly different—large models require billions of training examples to generate grammatically correct sentences, while human children need far fewer. This has sparked debate in linguistics about nature versus nurture in language acquisition. Is there a pre-programmed structure in the brain for learning grammar, or is it all learned during childhood? Although I’m not an expert in linguistics, we know that machine learning models with built-in biases for specific problems can learn more easily and with less data. It makes sense that there might be similar architectural designs in the human brain.\nHowever, we also know that backpropagation, the learning algorithm used in large language models, is biologically impossible. The neural networks in our brains function differently than those in artificial models. This fundamental difference in learning algorithms could impact how much these models truly understand.\nAnother important distinction is symbolic grounding. When humans learn language, they not only learn relationships between words but also connections to real-world objects. Current LLMs lack this grounding, which could affect their understanding of language.\nSymbols, like the sound of cats or the word “cat” in a text, connect to real-life anchors, such as an actual cat. Grounding is crucial in language acquisition because children first learn concepts they experience daily, like mom, dad, food, and home. Only later do they make connections with abstract ideas like ethical values or metacognition, that can’t be directly tied to experiences.\nSo, humans learn language while interacting with the real world. Language is grounded in physical experiences and other sensory inputs like images, touch, sound, and taste. This contrasts with large language models that only learn correlations between words without grounding their meaning in experience.\nFor example, when you say “the apple is red” to a large language model, it recognizes this as a likely true sentence due to context. However, for a human, the same phrase connects the abstract symbols with experiences of what an apple looks like, tastes like, and feels like. This shows that language models reason differently than humans when it comes to real-world concepts.\nOne could argue that humans can reason about purely symbolic things, like math. Even though numbers might be grounded in physical notions of quantity, abstract fields of math involve humans reasoning by manipulating symbols and learning correlations between them. In this sense, there may be a case for language models to be able to reason similarly in certain contexts.\nIn summary, reasoning in LLMs, in its current limited form, involves understanding how words, sentences, and contexts relate and appear together. This provides a certain level of linguistic comprehension. However, this level of understanding differs greatly from that of humans or even animals with primitive intelligence.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The limits of language models</span>"
    ]
  },
  {
    "objectID": "part1/limitations.html#sec-reasoning",
    "href": "part1/limitations.html#sec-reasoning",
    "title": "4  The limits of language models",
    "section": "Reasoning",
    "text": "Reasoning\nLarge language models often seem to be able to reason effectively. They can generate coherent and contextually relevant responses that resemble human reasoning. However, this apparent capability can be misleading.\nLLMs frequently make mistakes when faced with complex problems requiring extensive reasoning chains. Their responses may seem logical initially, but they often lack the depth and accuracy for sound reasoning. This is particularly evident in tasks that involve multiple steps or intricate logical deductions, where the model may falter and produce incorrect conclusions.\nThis article explores the fundamental limitations of large language models (LLMs) in reasoning — highlighting the difference between their advanced outputs and their evident inability to perform logical deductions. By examining the stochastic nature of these models, their computational constraints, and their lack of complete computational capabilities, we will uncover the reasons behind their failures in complex reasoning tasks.\nAdditionally, we will discuss current strategies to enhance LLMs’ reasoning capabilities, including chain of thought prompting and self-critique mechanisms, while critically assessing their effectiveness and underlying challenges. This article aims to foster a deeper understanding of what LLMs can and cannot achieve, urging caution in interpreting their seemingly intelligent responses.\n\nWhy LLMs can’t reason\nOne significant limitation of language models regarding reasoning is their stochastic nature. These models generate outputs based on probabilistic predictions rather than deterministic logical rules. This means that even a well-structured prompt can yield different responses on different occasions due to the randomness in their decision-making process.\nConsequently, an LLM might arrive at a wrong conclusion purely by chance, leading to inconsistencies in reasoning. For example, when asked to solve a mathematical problem or make a logical inference, the model’s response may vary significantly depending on the random seed used during generation, undermining trust in its reasoning capabilities.\nGranted, you may set the temperature to zero effectively forcing the model to fix the output for a given input. But this output is still probabilistic, you’re just sampling the most likely continuation. The fact that the mapping between input and output hinges on a probabilistic distribution that encodes correlations between elements in the input and corresponding elements in the output is already suspicious. It would be very weird, although not impossible, that we just happened to converge on the right probability distribution that produces the correct output for every input, in terms of logical deduction rules.\nHowever, this limitation is still not definitive. But it gets worse.\nBy design, large language models spend a fixed amount of computation per token processed. This means the amount of computation an LLM does before it produces the first output token is a function of just two numbers: the input size and the model size. So, if you ask an LLM to produce a yes or no question for a logical puzzle, all the “thinking” the model can do is some fixed—albeit huge—number of matrix multiplications that only depend on the input size. See where I’m going here?\nNow, consider that you have two different logical puzzles with the same input size, i.e., the same number of tokens. But one is an easy puzzle that can be solved with a short chain of deduction steps, while the other requires a much higher number of steps. Here is the kicker: any LLM will spend exactly the same amount of computation in both problems. This can’t be right, can it?\nA basic result in computational complexity theory is that some problems with very small inputs seem to require an exponentially high computational cost to be solved correctly. These are NP-complete problems, and most computer scientists believe there are no efficient algorithms to solve them. Crucially, a huge number of reasoning problems fall in this category, including the most basic logical puzzle of all—determining if a given logical formula can be satisfied.\nWhen faced with an instance of an NP-complete problem, an LLM will produce an answer after a fixed amount of computation defined solely by the input size. Now, by sheer size, some larger models might just spend enough computation to cover many smaller instances of NP-complete problems. As it happens, a huge constant function can be larger than an exponential function for smaller inputs. But crucially, we can always find instances of NP-complete problems that require, even in principle, a sufficiently large amount of computation to surpass the computational capacity of any LLM, no matter how big.\nBut this means something even more profound. Ultimately, LLMs are not Turing-complete systems but essentially very large finite automata. While they can handle a wide range of tasks and produce outputs that appear sophisticated, their underlying architecture limits the types of problems they can solve.\nTuring completeness is the ability of a computational system to perform any computation given sufficient time and resources. Modern computers and many seemingly simple systems, such as cellular automata, are Turing complete systems. But LLMs are not, ironically.\nThe reason is simple. We know from computability theory that any Turing complete system must be able to loop indefinitely. There are some problems—some reasoning tasks—where the only possible solution is to compute, and compute, and compute until some condition holds, and the amount of computation required cannot be known in advance. You need potentially unbounded computation to be Turing complete.\nAnd this is the final nail in the coffin. LLMs, by definition, are computationally bounded. No matter their size, there will always be problem instances—which we may not be able to identify beforehand—that require more computation than is available in the huge chain of matrix multiplications inside the LLM.\nThus, when LLMs seem to tackle complex reasoning problems, they often solve specific instances of those problems rather than demonstrating general problem-solving capabilities. This might just be enough for practical purposes—we may never need to tackle the larger instances—but, in principle, LLMs are incapable of truly open-ended computation, which means they are incapable of true reasoning. Case closed.\n\n\nImproving LLM reasoning skills\nHowever, we need not throw the hat here. Researchers and practitioners have explored several innovative strategies, including Chain of Thought prompting, self-critique mechanisms, and integrating external tools to improve the reasoning skills of large language models.\nCoT prompting encourages LLMs to articulate their thought processes, allowing them to break complex problems into manageable steps and improve their accuracy in reasoning tasks. On the other hand, self-critique aims to refine outputs through an internal evaluation process, yet it has shown mixed effectiveness in meaningfully correcting errors. Additionally, incorporating external tools such as reasoning engines and code generation systems can significantly augment the LLMs’ capabilities by providing structured logic and formal verification.\nHowever, each approach has its own set of challenges, and their potential and limitations in fostering true reasoning abilities within LLMs need to be carefully examined.\n\nChain of Thought\nChain-of-thought prompting has emerged as a promising technique for enhancing the reasoning capabilities of large language models. By guiding models to articulate intermediate reasoning steps before arriving at a final answer, CoT prompting helps decompose complex problems into manageable parts. This method has improved performance across various reasoning tasks, such as arithmetic and commonsense reasoning.\nCoT prompting instructs LLMs to break down complex problems into simpler, sequential steps and then tackle each step independently. This structured approach enables the model to tackle each component individually, improving response accuracy and precision. Studies have shown that this technique can significantly boost performance on reasoning tasks, particularly when the model has a sufficient number of parameters (around 100 billion) to use the benefits of CoT prompting effectively.\nBy encouraging models to articulate their thought processes, CoT prompting taps into the extensive pool of knowledge that LLMs acquire during training. This mechanism helps models apply relevant information more effectively, addressing their inherent difficulties with logical reasoning and problem-solving.\nAdditionally, CoT makes the LLM “think harder” in the sense it forces the model to produce what we can consider “internal thought” tokens. Thus, we may view it as a way to produce additional computation on the input before deciding on the response.\nHowever, despite its advantages, CoT prompting remains insufficient for several reasons.\nThe effectiveness of CoT prompting is highly contingent on the quality and diversity of the prompts used. If the examples provided are not representative or sufficiently varied, the model may struggle to generate coherent reasoning chains, leading to suboptimal performance. This reliance on effective prompt engineering can limit the technique’s scalability and generalizability.\nAnd again, the stochastic nature of LLMs means that even with CoT prompting, outputs can vary significantly across different runs due to randomness in generation processes. This variability can lead to inconsistent reasoning outcomes, undermining the reliability of the model’s responses.\nUltimately, CoT extends the computation budget by a finite amount. Unless we try some cyclic scheme where the LLM is prompted to continue thinking, potentially indefinitely, until satisfied, their fundamental limitation on Turing incompleteness remains.\n\n\nSelf-critique\nAnother intuitive approach to improving reasoning is self-critique, which involves evaluating and refining an LLM’s responses with the same model, using prompts that instruct the model to read its previous output, highlight potential errors, and try to correct them. A form of after-the-fact chain-of-thought, if you might. However, recent research has highlighted significant limitations in the effectiveness of this self-critique capability.\nWhile LLMs can generate multiple ideas and attempt to critique their initial outputs, studies indicate that they cannot often meaningfully self-correct. The assumption that verification of correctness should be easier than generation—a fundamental idea in computational complexity theory—does not seem to hold true, in general, for LLMs. This is particularly problematic in reasoning tasks where the model struggles to adequately assess its outputs’ accuracy. For example, if an LLM generates a flawed answer, its attempt to critique and revise it can lead to further errors rather than improvements.\nResearch shows that self-correction techniques in LLMs are heavily contingent on the availability of external feedback. In many cases, LLMs perform better when they have access to an external verifier or additional context rather than relying solely on their internal reasoning capabilities. For example, when solving complex problems, such as graph coloring or planning tasks, LLMs often fail to produce reliable solutions without external guidance.\nInterestingly, attempts at self-critique can sometimes degrade performance rather than enhance it. Studies have shown that when LLMs engage in self-critique without external validation, they may generate false positives or incorrect conclusions. If you push harder, you can easily fall into a cycle of self-reinforcement of invalid or erroneous arguments, making the LLM increasingly more certain despite it getting worse and worse.\n\n\nExternal tools\nIntegrating external tools, such as reasoning engines or code generation systems, into large language models represents a promising—and, for me, the only really viable—approach to enhancing their reasoning capabilities.\nConnecting LLMs to external reasoning engines or logical inference tools makes it possible to augment their reasoning capabilities significantly. These tools can handle complex logical deductions, mathematical computations, or even domain-specific knowledge that the LLM might not possess inherently. This integration allows for more accurate and reliable outputs, as the external tools can apply formal logic and structured reasoning that LLMs typically struggle with.\nSimilarly, external code generation systems enable LLMs to produce executable code for specific tasks. This capability can streamline software development processes and improve efficiency in generating functional code snippets. The external systems can provide rigorous checks and balances that help ensure the correctness of the generated code.\nBy leveraging these external resources, LLMs can potentially overcome some of their inherent limitations in logical reasoning and problem-solving. For starters, an external inference engine will be Turing-complete, so we scratch that problem down, right?\nNot so fast. Unfortunately, this approach has many challenges, particularly regarding the LLM’s ability to generate the correct input for function calls or code execution. It all circles back to the original sin of LLMs: stochastic output.\nFirst, the effectiveness of function calling or code generation hinges on the model’s ability to accurately interpret a task and generate appropriate inputs. If the model misinterprets the requirements or generates vague or incorrect prompts, the external tool may produce erroneous outputs or fail to execute altogether. This reliance introduces a potential failure point where the model’s limitations in understanding context and intent become apparent.\nMany reasoning tasks require a nuanced understanding of logic and context that may exceed the capabilities of language models. For instance, when generating inputs for a logical inference engine, the model must understand the problem and articulate it in a way that aligns with the system’s requirements. If the model fails to capture these nuances, it may lead to incorrect deductions or ineffective reasoning processes.\nTranslating text into code or structured queries makes it more complex and can undermine reasoning capabilities. This conversion requires programming syntax and logic knowledge that may not be intuitive for an LLM trained primarily in natural language data. Mistakes in this translation can spread to the external system, causing more errors.\nWhile external tools can, in principle, improve the reasoning capabilities of an LLM by providing structured logic and formal verification, they cannot compensate for LLMs’ basic limitations in generating precise inputs. Therefore, there is no formal guarantee that the outputs from this integration will be logically sound or appropriate for the context, simply because of the age-old adage: garbage in, garbage out.\n\n\n\nConclusions\nWhile large language models may exhibit some reasoning capabilities, their fundamentally stochastic nature and fixed computational architecture hinder their ability to engage in open-ended, arbitrary-length deductions. This underlying limitation means that despite ongoing research and exploring various techniques to enhance reasoning, such as Chain of Thought prompting and self-critique mechanisms, and even duck-taping them with powerful reasoning engines, we still don’t know how to make language models reason using flawless, formal logic.\nThe emergence of models like OpenAI’s o1, which boasts impressive reasoning abilities, may seem like a significant step forward. However, this approach does not represent a fundamentally new paradigm in logical reasoning with LLMs. Deep down, this is “just” a way to explicitly incorporate chain of thought prompting in a fine-tuning phase and teach the model via reinforcement learning to select mostly coherent paths of deduction.\nThus, while definitely an impressive technical and engineering feat, o1 (terrible name) —and any future models based on the same paradigm— will continue to share the same core limitations inherent to all LLMs, only mitigated using some clever tricks. Thus, while they may excel in certain contexts, caution must be exercised in interpreting their outputs as definitive reasoning.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The limits of language models</span>"
    ]
  },
  {
    "objectID": "part1/limitations.html#context",
    "href": "part1/limitations.html#context",
    "title": "4  The limits of language models",
    "section": "Context",
    "text": "Context\n\n\n\n\n\n\nNote\n\n\n\nThis section is under construction.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The limits of language models</span>"
    ]
  },
  {
    "objectID": "part1/limitations.html#biases",
    "href": "part1/limitations.html#biases",
    "title": "4  The limits of language models",
    "section": "Biases",
    "text": "Biases\n\n\n\n\n\n\nNote\n\n\n\nThis section is under construction.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The limits of language models</span>"
    ]
  },
  {
    "objectID": "part1/limitations.html#sec-hallucinations",
    "href": "part1/limitations.html#sec-hallucinations",
    "title": "4  The limits of language models",
    "section": "Hallucinations",
    "text": "Hallucinations\nThe term “hallucination” in the context of AI refers to the phenomenon where a large language model (LLM) or other generative AI system produces outputs that appear plausible and coherent, but do not accurately reflect reality or the intended purpose of the system. These hallucinations manifest as the generation of false, inaccurate, or nonsensical information the AI system presents with confidence, as if it were factual.\nFirst, a caveat. Unlike human hallucinations, which involve perceiving things that are not real, AI hallucinations are associated with the model producing unjustified responses or beliefs, rather than perceptual experiences. The name “hallucination” is therefore imperfect, and it often leads to mistakes as people tend to antropomorphize these models and make erroneous assumptions about how they work and the causes of these failures.\nHowever, we will stick to this name in this article because it is the prevalent nomenclature used everywhere people talk about AI. Just keep in mind we’re talking about something completely different to what the term “hallucination” means in general.\n\nReal-World Implications of AI Hallucinations\nThe occurrence of hallucinations in AI systems, and particularly in large language models (LLMs), can have significant consequences, especially in high-stakes applications such as healthcare, finance, or public safety. For example, a healthcare AI model that incorrectly identifies a malignant skin lession as benign can doom a patient. On the other hand, identifying a benign skin lesion as malignant could lead to unnecessary medical interventions, also causing harm to the patient. Similarly, in the financial sector, hallucinated outputs from an AI system could result in poor investment decisions with potentially devastating economic impacts.\nHowever, even in low-stakes applications, the insidious nature of hallucinations make then a fundamental barrier to the widespread adoption of AI. For example, imagine you’re using an LLM to generate summaries from audio transcripts of a meeting, extracting relevant talking points and actionable items. If the model tends to hallucinate once in a while, either failing to extract one key item, or worse, producing an spurious item, it will be virtually impossible for anyone to detect that without manually revising the transcript, thus rendering the whole application of AI in this domain useless.\nFor this reason, one of the key challenges in addressing the real-world implications of language model hallucinations is the difficulty in effectively communicating the limitations of these systems to end-users. LLMs are trained to produce fluent, coherent outputs that can appear plausible, even when they are factually incorrect. If the end-users of an AI system are not sufficiently informed to review the output of the system with a critical eye, they may never spot any hallucinations. This leads to a chain of mistakes as the errors from the AI system propagate upstream through the layers of decision makers in an organization. Ultimately, you could be making a very bad decision that seems entirely plausible given all the available information because the source of the error –an AI hallucination– is impossible to detect.\nThus, the development and deployment of LLMs with hallucination capabilities raises important ethical considerations. There is a need for responsible AI development practices that prioritize transparency, accountability, and the mitigation of potential harms. This includes establishing clear guidelines for testing and validating LLMs before real-world use, as well as implementing robust monitoring and oversight mechanisms to identify and address hallucinations as they arise.\nCrucially, there are absolutely zero generative AI systems today that can guarantee they don’t hallucinate. This tech is simply unreliable in fundamental ways, so every actor in this domain, from developers to users, must be aware there will be hallucinations in your system, and you must have guardrails in place to deal with the output of unreliable AIs. And this is so perverse because we are used to software just working. Whenever software doesn’t do what it should, that’s a bug. But hallucinations are not a bug of AI, at least in the current paradigm. As we will see in the next section, they are an inherent feature of the way generative models work.\n\n\nWhy Hallucinations Happen?\nThere are many superficial reasons for hallucinations, from data and modelling problems, to issues with prompting. However, the underlying cause of all hallucinations, at least in large language models, is that the current language modeling paradigm used in these systems is, by design, a hallucination machine. Let’s unpack that.\nAs we saw in Chapter 3, generative AI models, including LLMs, rely on capturing statistical patterns in their training data to generate outputs. Rather than storing explicit factual claims, LLMs implicitly encode information as statistical correlations between words and phrases. This means the models do not have a clear, well-defined understanding of what is true or false, they can just generate plausibly sounding text.\nThe reason this mostly works, is because generating plausibly sounding text has a high probabilty of reproducing something that is true, provided you trained on mostly truthful data. But large language models (LLMs) are trained on vast corpora of text data from the internet, which contains inaccuracies, biases, and even fabricated information. So these models have “seen” many true sentences and thus picked up correlations between words that tend to generate true sentences, but they’ve also seen many variants of the same sentences which are slightly or even entirely wrong.\nSo one of the primary reasons for the occurrence of hallucinations is the lack of grounding in authoritative knowledge sources. Without a strong foundation in verified, factual knowledge, the models struggle to distinguish truth from falsehood, leading to the generation of hallucinated outputs. But this is far from the only problem. Even if you only train on factual information—assuming there would be enough of such high-quality data to begin with—the statistical nature of language models make them susceptible to hallucinate.\nSuppose your model has only seen truthful sentences, and learned the correlations between words in these sentences. Imagine there are two very similar sentences, both factually true, that differ in just a couple of words –maybe a date and a name, for example “Person A was born in year X” and “Person B was born in year Y”. Given the way these models work, the probability of generating a mixed-up sentence like “Person B was born in year X” is only slightly smaller than generating either of the original sentences.\nWhat’s going on here is that the statistical model implicitely assumes that small changes in the input (the sequence of words) lead to small changes in the output (the probability of generating a sentence). In more technical terms, the statistical model assumes a smooth distribution, which is necessary because the amount of data the model needs to encode is orders of magnitude bigger than the memory (i.e., number of parameters) in the model. Thus, the models has to compress the training corpus, and compression implies loosing some of the information.\nIn other words, statistical language models inherently assume that sentences very similar to what they have seen in the training data are also plausible sentences. They encode a smooth representation of language, and that’s fine, as long as you don’t equate plausible with factual. See, these models weren’t designed with factuality in mind. They were originally designed for tasks like translation, where plausibility and coherence are all that matters. It’s only when you turn them into answering machines that you run into a problem.\nThe problem is there is nothing smooth about facts. A sentence is eithre factual or not, there are no degrees of truthfulness —for the most part; let’s not get dragged into epistemological discussions here. But LLMs cannot, by design, define a strict frontier between true and false sentences. All the frontiers are fuzzy, so there is no clear cutoff point where you can say, if a sentence has less than X value of perplexity then it is false. And even if you could define such a threshold, it would different for all sentences.\nYou may ask why can’t we avoid using this “smooth” representation altogether. The reason is that you want to generate sentences that are not in the training set. This means you need to somehow guess that some sentences you have never seen are also plausible, and guessing means you have to make some assumptions. The smooth hypothesis is very reasonable —and computationally convenient, as these models are trained with gradient descent, which requieres smoothness in the loss function— again, as long as you don’t care about factuality. If you don’t compress the training data in this smooth, lossy way, you will simply I can’t wait for you to start training your own chatbot and building exciting applications with LLMs!not be able to generate novel sentences at all.\nIn summary, this is the underlying reason why the current paradigm of generative AI will always hallucinate, no matter how good is your data and how ellaborated are your training procedures or guardrails. The statistical language modeling paradigm, at its core, is a hallucination machine. It is concocting plausibly-sounding sentences by mixing and matching words that is has seen together in similar contexts in the training set. It has no inherent notion of whether a given sentence is true or false. All it can tell is that it looks like sentences that appear in the training set.\nNow, a silver-lining could be this idea that even if some false sentences will unavoidably be generated, we can train the system to minimize their ocurrence by showing it lots and lots of high quality data. That is, can we push the probability of a hallucination to a sufficiently low value that, in practice, almost never happens? Recent research suggests that if there is a sentence that can be generated at all, no matter how low its base probability, then there is a prompt that will generate it with almost 100% certainty. This means that if we introduce malicious actors into our equation, we can never be sure our system can’t be jailbroken.\n\n\nMitigating Hallucinations in AI\nSo far we’ve argued that hallucinations are inherently impossible to eliminate completely. But this doesn’t mean we can’t do anything about it in practice. I want to end this article with a short summary of mitigation approaches that are being used today by researchers and developers.\nOne key strategy is to incorporate external knowledge bases and fact-checking systems into the AI models. By grounding the models in authoritative, verified information sources, the risk of generating fabricated or inaccurate outputs can be reduced.\nResearchers are also exploring ways to develop more robust model architectures and training paradigms that are less susceptible to hallucinations. This may involve techniques like increasing model complexity, incorporating explicit reasoning capabilities, or using specialized training data and loss functions.\nEnhancing the transparency and interpretability of AI models is also crucial for addressing hallucinations. By making the models’ decision-making processes more transparent, it becomes easier to identify and rectify the underlying causes of hallucinations.\nAlongside these technical approaches, the development of standardized benchmarks and test sets for hallucination assessment is crucial. This will enable researchers and developers to quantify the prevalence and severity of hallucinations, as well as compare the performance of different models in this regard. Thus, if you can’t completely eliminate the problem, at least you can quantify it and make informed decisions about where and when it is safe enough to deploy a generative model.\nFinally, addressing the challenge of hallucinations in AI requires an interdisciplinary approach, involving collaboration between AI researchers, domain experts, and authorities in fields like scientific reasoning, legal argumentation, and other relevant disciplines. By fostering cross-disciplinary knowledge sharing and research, the understanding and mitigation of hallucinations can be further advanced.",
    "crumbs": [
      "Principles",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The limits of language models</span>"
    ]
  },
  {
    "objectID": "part2/prompting.html",
    "href": "part2/prompting.html",
    "title": "6  Prompt engineering",
    "section": "",
    "text": "Principles for effective prompt engineering\nThe goal of prompt engineering is to design an effective prompt for a given task. The reason this is necessary is because LLMs have inherent limitations and design caveats that make then brittle and prone to fail on an otherwise solvable task if the wrong instructions are given. And wrong doesn’t mean necessarily wrong in any objective sense, just not adjusted to the limitations of the LLM you are using.\nThus, to come up with principles for effective prompt engineering, it will be pay to briefly revisit some of the main limitations of LLMs we saw in Chapter 4. Please read that chapter first for a full picture.\nThe following principles of prompt engineering stem from the basic structure of statistical language modeling: next-token prediction—understanding that an LLM is ultimately an autocompletion machine on steroids, based on word-context correlations learned from data. This realization informs the following principles.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prompt engineering</span>"
    ]
  },
  {
    "objectID": "part2/prompting.html#principles-for-effective-prompt-engineering",
    "href": "part2/prompting.html#principles-for-effective-prompt-engineering",
    "title": "6  Prompt engineering",
    "section": "",
    "text": "Context matters\nSince every new token generated is conditioned on the previously generated tokens, the response you get for a given prompt will be heavily influenced by the content of the promp, down to the exact phrasing you use. Though LLMs are notoriously good at capturing the underlying semantics of text, the actual words, the tone, the style, and even the exact order in which you construct the prompt may determine the quality of the response.\nCurrent research suggests, for example, that LLMs tend to focus more on the beginning and final parts of the prompt and less on the middle, although this may change rapidly as novel architectures are invented. But, regardless of the idisincracies of specific models, the critical insight here is that whatever you put in the prompt will heavily influence the response, so everything important should be explicitly mentioned.\n\n\nFocus matters\nFor the same reason, it is notoriously hard for LLMs to perform many tasks at once. The more focused the instructions are, the better–and more robust–output you’ll get. As we have seen in Chapter 4, LLMs are weak reasoners, and struggle with complicated conditional instructions. Some bigger models may be able to deal with a larger degree of flexibility, but in general you should avoid writing prompts with conditions, and instead make them as straightforward as possible.\nFor example, you can try prompting “if the user says X then reply Y, otherwise reply Z”, hoping the LLM will correctly classify the input and choose the right response in the same API call. This might work in many cases, depending on how good the LLM is. But the problem here is that you’re asking the LLM to solve two problems at once. Instead, you can solve the same problem more robustly with two sequential instructions: first ask the LLM to classify the user query, and then pick the corresponding prompt for that category, thus the LLM never has to explicity choose.\n\n\nReasoning requires verbosity\nAs we say in Chapter 2, LLMs perform a fixed amount of computation per token. This includes input and generated tokens. Thus, intuitively, a larger, more detailed prompt will tend to produce a better response, especially when some complicated reasoning is involved.\nBut crucially, this also applies to output generated with the LLM. The more it talks, the more computation is performing in total. For this reason, asking for very terse output is often not optimal. Instead, your prompt should incite the LLM to be verbose, explain its reasoning, summarize its key points before reaching a conclusion, etc.\n\n\nBut more is not always better\nHowever, keep in mind that just stuffing the prompt with redundant information or instructions is not always better. The information density in the context matters more than its raw length.\nYou should strive for a minimum valuable prompt: the shortest prompt that contains the necessary amount of information to produce a succesful response. To achieve it, consider making your instructions more intentional, using more precise wording and avoiding vague terms. Provide informative examples where necessary.\nBut crucially, do not add irrelevant instructions, as LLMs are lousy at ignoring things. A typical issue is adding negative examples to correct some behavior and discover the LLM doubles down. The reason is simple, everything in the context will influence what the LLM generates, so negative examples are still examples that will bias the sampling process towards similar content.\n\n\nExperimentation rules\nAll of the above being said, prompt engineering is still mostly artisanal, and far from a established science. For this reason, no amount of theory can replace good old experimentation. You should try different orderings of the instructions, different output formats, different writing styles, and see which gives you better results.\nWhile you should write prompts that are, in general, agnostic to the specific LLM you’re using, keep in mind that the optimally-tuned prompt for one model, say GPT-4, might not be the universally best prompt. Different LLMs trained on different datasets and tuned with different strategies might have subtle differences that make one, for example, perform better with terse instructions while the other prefers verbosity. This may go down to the actual selection of words: a single word changed by an appropriate synonym may very well improve results significantly.\nThese principles are high-level insights that should inform how you approach prompt engineering. But keep in mind everything we know about large language models is changing very rapidly at the moment, and many of their current limitations could be fixed or at least reduced considerably with newer models, making some of these principles less relevant in the near future.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prompt engineering</span>"
    ]
  },
  {
    "objectID": "part2/prompting.html#prompt-engineering-techniques",
    "href": "part2/prompting.html#prompt-engineering-techniques",
    "title": "6  Prompt engineering",
    "section": "Prompt engineering techniques",
    "text": "Prompt engineering techniques\nIn the following sections, we will explore specific prompt engineering techniques or patterns that are general enough to be valuable in many contexts and domains. Each of these patterns and techniques is informed by the afforementioned principles, and we will discuss why they seem to work within this framework.\n\nZero-shot instructions\nZero-shot means providing a model with a single instruction and asking it to solve a problem without any additional training data or example. This should be the baseline for any new application, and is useful for complex or novel tasks where there is no existing data to draw from.\nZero-shot learning works by leveraging the model’s ability to generalize from a single instruction. By providing a clear and specific prompt, the model can use its internal knowledge and understanding to generate a solution without needing additional training data.\nSome exaplmes of zero-shot learning are:\n\nGenerating product descriptions for new or unique products.\nTranslating text between languages without parallel data.\nSummarizing long documents or articles into concise overviews.\n\n\n\nFew-shot learning\nFew-shot learning involves adding a small set of examples to the model’s input to help it generalize better. This technique is particularly useful for rare or ambiguous queries, as it allows the model to learn from a small number of examples.\nFew-shot learning works by providing the model with a few examples of similar queries and their corresponding answers. This helps the model learn from these examples and apply the knowledge to new, unseen queries. The few-shot approach also reinforces the output format, thus improving fidelity.\nSome examples of few-shot learning are:\n\nSolving tasks that aren’t easy to explain concisely.\nReinforcing an output format or response style.\nGenerating recomendations of products, activities, etc., based on examples.\n\n\n\nRole playing\nRole playing involves informing the model about the audience, tone, role, and other context-specific details to bias it towards a specific complexity level, extension, or style. This technique is useful for generating responses that are tailored to a specific audience or context.\nRole playing works by providing the model with context-specific information that helps it generate responses that are more relevant and engaging to the target audience. By understanding the audience and context, the model can tailor its responses to meet their needs and expectations.\nExamples of role playing include:\n\nWriting dialogue for characters with distinct personalities and speaking styles.\nGenerating social media posts tailored to different demographics and platforms.\nDrafting emails with appropriate tone and formality for different recipients.\n\n\n\nChain of thought\nChain of thought involves asking the model to output a detailed reasoning process before providing the final answer. This technique is useful for complex queries that require multi-step reasoning or problem-solving.\nChain of thought works by forcing the model to explicitly demonstrate its thought process. This helps ensure that the model’s responses are based on sound reasoning and logic, making them more accurate and trustworthy.\nExamples where chain of thought is useful include:\n\nSolving logic puzzles and brain teasers by breaking down the steps.\nProviding step-by-step instructions for complex procedures or recipes.\nAnalyzing data to draw insights and conclusions.\n\n\n\nStructured output\nStructured output involves instructing the model to produce the output in a structured format, such as JSON. This technique is useful for applications that require structured data, such as database queries or data analysis.\nStructured output works by simplifying the parsing of the response and allowing for easier integration with downstream applications. By providing a structured format, the model can generate responses that are easily consumable and actionable.\nExamples where structured ouput is useful:\n\nGenerating tabular data like schedules, calendars, or price lists.\nProducing API responses in a standardized JSON format.\nExtracting structured information like addresses, dates, or product details from text.\n\n\n\nSelf-reflection\nSelf-reflection involves asking the model to evaluate its own response and determine if, given new context, it would change it. This technique is useful for identifying and correcting errors or inconsistencies in the model’s original output.\nSelf-reflection works by allowing the model to assess its own responses and identify potential errors or inconsistencies. By reflecting on its own output, the model can refine its responses and improve their accuracy and fidelity.\nExamples of using self-reflection include:\n\nIdentifying biased or unethical statements in the model’s own outputs.\nDetecting logical inconsistencies or contradictions in generated text.\nRefining responses based on feedback or additional context provided.\n\n\n\nEnsembling\nEnsembling involves combining the output of several models and asking a final model to produce a consensus answer. This technique is useful for improving the overall accuracy and fidelity of the response.\nEnsembling works by leveraging the strengths of multiple models to generate a more accurate and reliable response. By combining the output of multiple models, ensembling can reduce the impact of individual errors and improve the overall quality of the response.\nExamples of ensembling include:\n\nCombining outputs from models with different specialties or training data.\nAggregating responses from models with different decoding strategies.\nLeveraging models with different strengths to produce well-rounded outputs.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prompt engineering</span>"
    ]
  },
  {
    "objectID": "part2/prompting.html#conclusions",
    "href": "part2/prompting.html#conclusions",
    "title": "6  Prompt engineering",
    "section": "Conclusions",
    "text": "Conclusions\nPrompt engineering is a nascent discipline and much is still unknown about what are core principles and what are caveats and consequences of the current state of development in LLMs. This means you should beware that many of these ideas or principles might not remain relevant in the mid-term, as novel models and architectures are invented.\nPrompt engineering is a very powerful and cualitatively new software development pattern. You can now program a computer to solve novel problems with reasonable effectiveness using natural language! But there is no free lunch, as usual. The main limitations of Large Language Models (LLMs) in prompt engineering stem from their inherent design caveats and constraints.\nLLMs can be brittle and prone to failure on tasks if the instructions provided are not aligned with the model’s capabilities. This means that even solvable tasks can fail if the wrong instructions are given, highlighting the importance of crafting prompts that suit the specific LLM being used. They struggle with complex reasoning tasks and conditional instructions. They may not perform well when faced with intricate conditional prompts, making it challenging for them to handle multiple instructions or conditions within a single prompt.\nLLMs operate with a fixed amount of computation per token, which affects both input and output tokens. This constraint implies that a more detailed and verbose prompt tends to produce better responses, especially for tasks involving complex reasoning. This also means that overly terse prompts may not yield optimal results. However, some balance is important, because overly verbose prompts can be more confusing that informaive.\nPrompt engineering is still an evolving and artisanal process, lacking a standardized approach. Experimentation is essential to determine the optimal prompt for a specific task and LLM. Different LLMs trained on diverse datasets may respond better to varying prompt styles, making it necessary to experiment with different strategies to find the most effective approach.\nThese limitations underscore the complexity and nuances involved in designing prompts for LLMs, highlighting the need for careful consideration and adaptation to maximize the performance of these models in various tasks and domains. In Part 3 of the book we will apply all of these prompt engineering techniques to concrete problems, and we will have plenty of time to explore the contexts in which they perform well.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prompt engineering</span>"
    ]
  },
  {
    "objectID": "part2/rag.html",
    "href": "part2/rag.html",
    "title": "7  Retrieval augmented generation",
    "section": "",
    "text": "Retrieval strategies\nThese are the most common retrieval strategies.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval augmented generation</span>"
    ]
  },
  {
    "objectID": "part2/rag.html#retrieval-strategies",
    "href": "part2/rag.html#retrieval-strategies",
    "title": "7  Retrieval augmented generation",
    "section": "",
    "text": "Vector databases\nIn this approach, each document is split into meaningful chunks of text (e.g, paragraphs or sections) and each chunk is transformed into an embedding. At inference time, the user query is used to obtain a “key” embedding. This key is compared to each stored chunk and the most similar are inserted in the context. For large scale scenarions, you would use an efficient vector store that can quickly locate the most similar vectors.\n\n\nStructured databases\nIf the knowledge is stored in a traditional SQL database, then you must resort to SQL code generation for retrieval. The simplest solution is to have an LLM generate the approriate SQL statement for a given user query in a single shot, but this process can be improved with multiple passes, as we’ll see in ?sec-code-generation.\n\n\nKnowledge graphs\nA compelling alternative for storing well-structured facts about a specific business domains is knowledge graphs. Explaining what is a knowledge graph in detail goes beyond the scope of this book, but in a nutshell, it is a network of the relevant entities in a domain and their interrelationships. For example, if you are in the medical domain, this graph could contain nodes for all known diseases, symptoms, and drugs, and edges relating which disease is associated with which symptoms, and which drugs can be prescribed for each disease.\nQuerying a knowledge graph depends on the underlying implementation. If you are using a graph database, such as Neo4j, this isn’t that much different to querying a traditional database. You will probably use an LLM to generate query statements in an appropriate query language (e.g., Cypher in the case of Neo4).\nHowever, you can also expose the graph structure to the LLM and use it as a controller to navigate the graph. The simplest approaches involve asking the LLM for the relevant entities and relations to focus on, and obtaining the relevant induced subgraph. More advanced approaches involve constructing a relevant subgraph by iteratively querying the LLM about which relations (edges) are worthwile to explore in each iteration.\n\n\nSearch APIs\nFinally, you can store your relevant domain knowledge in a storage service that provides a search API. This can range from locally deployed document databases such as ElasticSearch or MongoDB to cloud-based instances to using third-party search APIs like Google, Reddit, Wikipedia, and a myriad other domain-specific services.\nIn these cases, your retrieval strategy will depend on the idionsincracies of the search service you use. For something simple like ElasticSearch full-text search or the Google API, you may simply submit the user query directly. However, if the search API has relevant parameters, this becomes an instance of function calling, which we’ll see in Chapter 8.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval augmented generation</span>"
    ]
  },
  {
    "objectID": "part2/rag.html#query-strategies",
    "href": "part2/rag.html#query-strategies",
    "title": "7  Retrieval augmented generation",
    "section": "Query strategies",
    "text": "Query strategies\nThe following are ways to use the user query for generating a suitable search query.\n\nSearch by query\nThe most direct strategy is to simply submit the user query to your retrieval engine. In the case of vector databases, this implies embedding the user query directly, while in the case of search APIs this involves sending the user query as the main argument in the API search method.\nThe obvious upside of this strategy is its simplicity, and the fact that is works surprinsingly well in many cases. This of course depends on how robust your search engine is, and more specifically, how closely related is the user query to the expected query language used by your search engine.\n\n\nSearch by answer\nSpecifically in vector databases and embedding-based retrieval, researchers have observed that the user query is often not informative enough to pinpoint the most relevant document chunk. For example, if you knowledge base is composed of research papers or technical documents in general, it is very unlikely that a user query formulated in an imprecise, informal language will have an embedding that is most similar to the exact paragraph that holds the answer, especially if that answer is far from trivial.\nIn these cases, a neat trick is to use the LLM first to generate an answer on-the-fly, and then embed that answer and use it as query key. The reason this works is that, even if the vanilla LLM doesn’t have the precise knowledge to answer the user query in detail, it is often capable of producing at least a plausibily-sounding response that mimics the right language.\n\n\nLLM-guided search\nA more elaborate search strategy involves prompting the LLM to generate a suitable search query. This can work better than both previous strategies if the prompt is carefully constructed. By providing the LLM both the context from which the user is coming as well as the characteristics of the knowledge base, we can leverage the powerful natural language understanding capabilities of the LLM to bridge that gap between what the user says, what the user actually wants, and what the search engine needs.\nThe simplest example of this approach is prompting the LLM to, given a user query, produce a set of relevant search queries for a general-purpose search engine. For example, if you’re building a Medical Q&A bot backed by a custom knowledge base, and the user query is something like “What are the effects of looking straight into the Sun during an eclipse”, it is unlikey this query by itself will result in the right article. However, an LLM can easily determine that an appropriate query would be “Solar eclipses: medical recommendations”.\nIf you enhance this approach with a small set of examples, the LLM can quickly learn to map from fuzzy user queries to much more precise and domain-specific queries. Thus, this approach works best when you’re dealing with custom search engines or knowledge bases that are not as capable as, say, Google, to provide a relevant context for an arbitrary user query.\n\n\nIterated retrieval & refinement\nThis is an extension of the previous approach in which instead of a single shot, we let the LLM iteratively provide more relevant queries. The objective is to construct a relevant context one step at a time, by obtaining a query from the LLM, extracting the relevant chunk, and using self-reflection to let the LLM decide if additional information is required.\nThis approach has the advantage that if the first query is not as informative as necessary, we still get a few shots to pinpoint the exact context we need. However, this can quickly get out of hand and produce a huge, semi-relevant or mostly irrelevant context that will confuse the LLM more than it helps.\nTo counter this effect, we can add a refinement step after each retrieval, as follows. We let the LLM produce a query, find the most relevant chunk, and then ask the LLM to, given the query and the context, extract a summary of the relevant points mentioned in the context. This way, even if we end up extracting dozens of chunks, the final context could be a very concise and relevant summary of the necessary background knowledge.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval augmented generation</span>"
    ]
  },
  {
    "objectID": "part2/functions.html",
    "href": "part2/functions.html",
    "title": "8  Function calling",
    "section": "",
    "text": "How function calling works\nFirst, you define a set of “functions”, which can be anything from real code functions (e.g., Python methods) to API calls. It doesn’t matter what is the underlying implementation, as the LLM will never directly interact with the function. It will just tell you when and how it should be invoked.\nFor that reason, you need to provide the LLM with a natural language description as well as an structured definition of the arguments of every function. This is usually all encapsulated in a standarized JSON schema, such as the following:\nThen, at inference time, a special system prompt instructs the LLM to either respond as normal or to produce a function call. The function call is a special type of structured response in which the LLM provides just a JSON object with the identifier of the function to call and the values for all mandatory arguments. An onversimplified example might be:\nGiven a prompt like the above, an well-tuned LLM should be able to determine whether, given a specific query, it needs to call an API function or not. The developer must capture these function calling replies and, instead of outputting them to the user, call the appropriate function and inject the result back into the LLM context. Then, the LLM will produce an appropriate natural language response.\nAn example of a possible conversation in this fictional setting would be as follows.\nFirst, the user asks for a specific information.\nGiven this query, and an appropriate prompt like the one shown above, the LLM might recognize it needs to call the get_user_info function but it’s missing the user_id argument.\nThe user replies back.\nSince the LLM is receiving the whole conversation story, the second time it’s called it will recognize that it has all the appropriate arguments, and produce a function call.\nThis time, instead of showing this message to the user, the developer intercepts the function call, invokes the API, and outputs the return value, presumably a list of purchases.\nGiven this new information, the LLM can now answer back to the user.\nThis process can occur as many times as necessary in a conversation. With a suitable prompt, the LLM can even detect when some argument value is missing and produce the corresponding natural language question for the user. This way, we can naturally weave a conversation in which the user supplies the necessary arguments for a given function call in any order. The LLM can also call more than one function in the same conversation, giving a lot more flexibility than a rigid RAG cycle.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Function calling</span>"
    ]
  },
  {
    "objectID": "part2/functions.html#how-function-calling-works",
    "href": "part2/functions.html#how-function-calling-works",
    "title": "8  Function calling",
    "section": "",
    "text": "{\n    \"functions\": [\n        {\n            \"name\": \"get_user_info\",\n            \"description\": \"get information about what a user has bought.\",\n            \"arguments\": [\n                {\n                    \"name\": \"user_id\",\n                    \"description\": \"The unique user identifier\",\n                    \"type\": \"string\",\n                    \"mandatory\": true,\n                }\n            ]\n        },\n        {\n            \"name\": \"get_item_info\",\n            \"description\": \"get information about an item's status and location.\",\n            \"arguments\": [\n                {\n                    \"name\": \"item_id\",\n                    \"description\": \"The unique item identifier\",\n                    \"type\": \"string\",\n                    \"mandatory\": true,\n                }\n            ]\n        }\n    ]\n}\n\nThe following is a set of API functions you can invoke to obtain\nrelevant information to answer a user query.\n\n{functions}\n\nGiven the following user query, determine whether an API call is appropriate.\nIf any arguments are missing from the conversation, ask the user.\nIf all arguments are available, output your response in JSON\nformat with the corresponding function call.\nOtherwise, answer to the user in natural language.\n\n{query}\n\n\n\nUSER: Hey, please show me my latest purchases.\n\nASSISTANT: Sure, I will need your user ID for that.\n\nUSER: Of course, my user ID is 12345.\n\nASSISTANT: {\"function\": \"get_user_info\", \"arguments\": {\"user_id\": {\"12345\"}}}\n\nTOOL: {\"function\": \"get_user_info\", \"result\": [ ... ]}\n\nASSISTANT: You have bought 3 items in the last month...",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Function calling</span>"
    ]
  },
  {
    "objectID": "part2/functions.html#use-cases-for-function-calling",
    "href": "part2/functions.html#use-cases-for-function-calling",
    "title": "8  Function calling",
    "section": "Use cases for function calling",
    "text": "Use cases for function calling\nFunction calling is particularly useful for integrating an LLM with an external tool that can be consumed as an API. A typical use case (which we will see in Chapter 16) is building a shopping assistant for an online store that can suggest products, add or remove items from the shopping cart, provide information on delivery status, etc.\nAn interesting trick is to use function calling for structured generation. When you want an LLM to produce a JSON-formatted output, it’s typically hard to guarantee you always get the exact schema you need–except maybe when using the best models. However, even some of the smaller models, once fine-tuned for function calling, are extremely robust in generating the exact argument names and types for any function. Thus, if you can frame your generation prompt as an API function call, you get all this robustness for free.\nBut the possibilities don’t end here. Whatever service you can encapsulate behind a reasonably well-structured and fine-grained API, you can now stamp an LLM upfront and make your API queryable in natural language. Here are some typical examples:\n\nCustomer support: Integrate an LLM with a company’s knowledge base, product information, and customer data to create an intelligent virtual agent for customer support. The LLM can handle common queries, provide product recommendations, look up order status, and escalate complex issues to human agents.\nInformation systems: Connect an LLM to a query API that provides realtime information about some specific domain, from weather to stocks. Use it for internal tools connected to a company dashboard and integrate a conversational-style interface with a traditional graphical user interface.\nWorkflow automation: Connect an LLM to APIs for various business tools like CRM, project management, HR systems etc. Allow users to automate common workflows by querying the LLM in natural language, e.g. “Create a new Salesforce lead for this email”, “Schedule a meeting with the team next week”, “Approve this time off request”.\nCollaborative writing: Integrate an LLM with document editing and collaboration tools to assist with writing tasks. The LLM can help brainstorm ideas, provide feedback on tone and structure, check for grammar and spelling, and even generate content based on prompts. We will see an example of this use case in Chapter 19.\nSoftware development: When combined with the powerful code generation skills of language models, another possibility opens up: connecting an LLM to code repositories, documentation, and APIs to create an AI programming assistant. Developers can ask the LLM to explain code, debug issues, suggest improvements, and even generate new code based on high-level requirements. We will see an example of this use case in Chapter 20.\n\nThe key is to identify areas where humans currently interact with APIs and information systems, and see how an LLM can make those interactions more natural, efficient and productive.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Function calling</span>"
    ]
  },
  {
    "objectID": "part2/functions.html#some-caveats-and-limitations",
    "href": "part2/functions.html#some-caveats-and-limitations",
    "title": "8  Function calling",
    "section": "Some caveats and limitations",
    "text": "Some caveats and limitations\nAs usual with LLMs, there are significant caveats and limitations to any integration. Although in general you can mitigate hallucinations considerably, the LLM can still hallucinate a wrong function call by, e.g., passing the wrong arguments. In the simplest case, maybe you can catch that error when arguments have the wrong type or are out of range. However, subtle hallucinations might result in a function call that succeeds but wasn’t the user intention.\nFor this reason, in all critical systems it is crucial that you don’t simply call an API blindly on behalf of the user, specially when doing so can have irreversible effects. For example, in a banking app, your LLM might hallucinate an incorrect destination in a transference, effectively sending the user money to an arbitrary third party. Furthermore, hackers might find a way to mess with your prompt and trigger the hallucination.\nIn these cases, you should always make the user explicitely trigger the final action, and make sure they have reviewed and understood the implications of such action. This enhances reliability at a small cost in usefulness, turning the LLM into an assistant that fills in the data for you, but doesn’t click the red button.\nAnother possible source of concern is when the LLM hallucinates the response, even though it made the right call and received the right data. This is the same problem we had with RAG: even if the context contains the right answer, there is no guarantee the LLM will pick it. One easy fix in many cases is to display the function result next to the LLM interpretation, so the user can double check the response.\nOne final caveat that may be relevant in many cases is regarding privacy. If you are interacting with a private API–say, a banking app–using a commercial LLM, you are effectively sending to OpenAI (or any other provider) you users’ information as part of the prompts, and this may include user IDs, addresses, financial details, etc. This underscores the need for powerfull open source LLMs that companies can self-host for added privacy and security.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Function calling</span>"
    ]
  },
  {
    "objectID": "part2/functions.html#conclusions",
    "href": "part2/functions.html#conclusions",
    "title": "8  Function calling",
    "section": "Conclusions",
    "text": "Conclusions\nFunction calling can be seen at the same time a special case and a generalization of retrieval augmented generation. It is a special case because it involves injecting external information in the prompt to enhance the capabilities of an LLM. It is a generalization because you can implement RAG with function calling, simply by encapsulating your search functionality in a function call specification.\nThis pattern is extremely flexible, but at the same time it’s very repeatible. However, to make it work, it is crucial to get the prompt right. Since prompts are, in general, not entirely portable across different models, implementing this workflow from scratch every single time is a chore.\nFor this reason, most LLM services provide a native way to perform function calling, basically abstracting away the fragile prompt engineering component. Moreover, the LLM provider might have fine-tuned their model to a specific function-calling prompt and formatting. And since most LLM providers implement the OpenAI API specification, porting function calling between different providers is way easier.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Function calling</span>"
    ]
  },
  {
    "objectID": "part2/code.html",
    "href": "part2/code.html",
    "title": "9  Code generation",
    "section": "",
    "text": "How code generation works\nIn the simplest case, you can think of code generation as a subset instance of text generation. If your model is trained on mixed natural language and code input, and/or fine-tuned with coding instructions, it will naturally learn to answer to some prompts with actual code. For example, you can train an LLM on programming contests, where the input is a problem statement, and the output is the code of the solution in some programming language.\nIt is at least somewhat surprising that vanilla LLMs, trained on code, can learn to write code at all. The reason to be skeptic is that programming languages have very strict syntax rules which make it hard, at least in principle, for a purely statistical language model to produce something that is just parseable code, not to mention semantically correct. Failing to produce a single semicolon in the exact location can make incorrect an otherwise perfect piece of code. Yet, LLMs learn to code, almost without additional effort.\nIn fact, most general-purpose models now available have at least some general capabilities for code generation, if only because they are trained of vast datasets that contain, among the many types of text modalities, lots and lots of code. And even if you don’t want an LLM explicitely for code generation, training on code and text (rather than just text) has shown to improve the general reasoning capabilities of a model, even for text-only tasks! But although you can get reasonably good code generation almost for free, the best coding LLMs are fine-tuned on precise text-and-code datasets.\nThere are many reasons to prefer a model fine-tuned for coding to a general one. The simplest argument is that, contrary to natural language, highly plausible code can still be incorrect. Fine-tuning a model specifically on code reinforces the syntax rules and makes it much less likely to generate almost-correct but still syntactically wrong code.\nIn the same vein, since programming languages are much more rigid in terms of syntax than natural language, fine-tuning can make a smaller model as good or even better than larger, general models, if focused on a specific language. Likewise, even if your general LLM can code Python, it may not know the specific framework you’re interested in, or code with the exact style you want.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code generation</span>"
    ]
  },
  {
    "objectID": "part2/code.html#code-generation-use-cases",
    "href": "part2/code.html#code-generation-use-cases",
    "title": "9  Code generation",
    "section": "Code generation use cases",
    "text": "Code generation use cases\nIn this section we’ll look at code generation from a high-level perspective, to understand what are the most interesting use cases it unlocks. We won’t go in-depth into technical details of how to make these use cases work in practice, as we will have plenty of time in Part 3 to see concrete examples in action.\n\nCode completion\nThe simplest use case for code generation is straightforward code completion. This can be as simple as asking ChatGPT the code to solve a concrete problem without any additional context. However, this use case becomes much more interesting when you can provide the LLM with relevant context (existing code, class hierarchies, function declarations, etc.) and have it produce code that fits right into an existing codebase.\nCode completion, at its core, is just a particular form of text completion, with all the quirks and caveats. The same prompt can produce different results, and slightly different prompts can vary widely in the quality of the output. All prompt techniques we’ve discussed so far apply: format your prompt carefully, be precise, don’t overshare, use examples whenever possible (no negative examples, please), and overall be intentional and cognizant of all usual gotchas.\n\n\nCode maintainance\nCode maintainance is an slightly more advanced form of code completion, in which we ask the model not code to support new functionality, but rather to modify existing code (or add to the existing code) to improve its quality, maintainability, clarity, etc. A typical example is automatic refactoring: asking the model to, e.g., extract some functionality into its own function, or otherwise abstract or encapsulate relevant fragments of code.\nThis process can be enhanced with the use of prompt templates for common tasks, including generating boilerplate code or running typical refactors like splitting methods, encapsulating functionality, or changing style from, e.g., a recursive implementation to an iterative one.\nAnother form of code maintainance is adding unit tests. A careful explanation of the relevant functional requisites, in natural language, might be enough to have a model generate reasonably good unit tests for the most common use cases.\n\n\nTranslation and explanation\nThe previous use cases are mostly examples of language to code generation. On the other hand, we can have code to code and code to language generation.\nThe first case is useful for code translation. A simple example is to translate code in one programming language to another, perhaps because you found the solution to your problem online but it’s not in the right programming language. But you can also translate between two versions of the same language, say, from Python 2 to Python 3, to update an implementation. Or translate between different frameworks, or different implementation styles.\nThe second case is useful for automatically adding comments to existing code, or otherwise generate explanations in any level of detail. As usual, the accuracy of the explanation is subject to how powerful the model is, and how complex the code. In general, it is impossible to understand what a given code will do without executing it, but you can get pretty far at least in the most common scenarios.\n\n\nUsing external tools\nNow we get into the domain of code as means to an end rather than the end in itself. You can use code generation to interface with external tools that either don’t have a high-level functional API or that, by their nature, are code based.\nAn example of the former is using any of the myriad utilities in your terminal, via Bash scripts. If you want your LLM to be capable of, e.g., creating files, making commits, downloading stuff, compressing and moving things around, etc., it is very likely that a reasonably good LLM can generate a Bash one-liner or small script to work these tasks out.\nIn the latter case, you may want to interface with tools that are code-based, such as SQL databases, or any number of special-purpose computing engines, from Wolfram Alpha to a SAT solver or an in-house tool. If the language used by that tool is not mainstream—meaning, the LLM won’t be trained on it—you’ll need to fine-tune on it.\nAnd finally, you can interface with programming frameworks that have, e.g., a Python-based API. Again, unless the framework is very well-known—e.g., sklearn—you may need to fine-tune your model to teach it how to use that concrete API. But in many cases the model might generalize from its basic Python knowledge to specific APIs with a small set of carefully curated k-shot examples.\n\n\nEnhanced reasoning\nAs we’ve seen in Chapter 4, LLMs are lousy at mathematical and logical reasoning. This is rather surprising at first, because computers are supposed to be precise mathematical machines. However, when you understand how LLMs reason, you realize they don’t have any explicit mechanism for even the simplest mathematical operations. But you know what does? Python! (and any other programming language).\nCode generation is the most effective way to enhance the mathematical skills of LLMs. Instead of having the model directly answer questions involving mathematical operations, make it generate a short code expression that computes the right formula, run it, and feed the result back to the LLM. This way you can “teach” an LLM to solve complex mathematical problems by doing the same we humans do: using the right tool.\nBut the possibilites go far beyond simple (or fairly involved) mathematical operations. You can pair an LLM with any of the many specialized inference engines the AI community has invented over the decades. Make your LLM generate Prolog code and voilá, you have a general purpose logic reasoning engine from natural language, or make it generate SymPy expressions and you have a symbolic reasoner.\nBut this is no silver bullet, of course. Your LLM can simply fail to generate the right code. So even if you have the perfect computing engine that solves the right problem, getting a language model to generate semantically correct code for that engine is an open problem, and one which is ultimately unsolvable according to basic computatibility theory.\nHowever, for many practical cases, given enough examples for k-shot or a small fine-tuning process, you can get an LLM to learn how to solve interesting problems reasonably well. And this is an active area of research, so we can only expect these capabilities to improve in the near future.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code generation</span>"
    ]
  },
  {
    "objectID": "part2/code.html#prompting-tips-for-code-generation",
    "href": "part2/code.html#prompting-tips-for-code-generation",
    "title": "9  Code generation",
    "section": "Prompting tips for code generation",
    "text": "Prompting tips for code generation\nIn many common cases, you can make an LLM write code simply by asking. A prompt like “Generate a Python function to find the third maximum element from a list” will work almost flawlessly in any sufficiently capable language model you can find today. And this works fine for many use cases where the code is all you need. At least is no worst than searching for a similar snippet of code online.\nHowever, there are several drawbacks with this KISS approach. First, most LLMs you’ll find online are fine-tuned for chat, so they are… chatty. Instead of the raw code, they might answer with something like “Sure, here is a function in Python to do ….” and then the code. This makes it hard to integrate them with external tools that need just the code, because then you have to parse the response.\nIn many cases, you can get away by adding an explicit instruction like “Please answer just with the source code”, but still, some models may refuse. And even if they comply, different models output code in different formats. Some will enclose the code in markdown-style code block annotations while other models might indent the code. It depends heavily on their training data.\nAnother problem you may face is when asking for one-liners, i.e., single instructions or expressions that you want to evaluate with, e.g., the eval function in Python. If you ask for a single pandas expression to, say, group and filter a dataset, the model may sometimes produce a propper expression—e.g., df.groupby(...).agg(...)—and other times an instruction—e.g., df = df.groupby(...). You may work around these issues by doing some checking and post-processing of the response, like removing anything before the last = sign, but this is a very brittle approach.\nIn these cases, some of our well-known prompt techniques also apply. Be very intentional with the prompt and provide positive examples of the exact response format you expect. While none of this will 100% guarantee you’ll get a response in the format you need, when paired with a try-and-repeat strategy, you can often get away with the performance you need. For example, if the model makes a mistake 10% of the time, you’ll need to redo one in ten queries on average, which is not that terrible all things considered.\nIn many cases, when retrying the same code generation task, it helps including in the prompt the previous answer and the error. This can often be automated simply by trying to run the code, capturing any exceptions, and feeding the model back with the exception message asking it to try and fix it.\nFinally, with some tricks, we can force the LLM to produce syntactically-correct code—even if not guaranteed to be semantically valid. The trick is to restrict the sampling step to only select among the tokens that would be syntactically valid.\nSome open-source LLM inference engines, like llama.cpp, support passing a formal grammar that defines the syntax of the programming language. During sampling, the engine will select among the top-k tokens only those that are valid according to the production rules of the grammar. This can be done efficiently with a linearly bound automaton that can be constructed automatically from the formal grammar definition. While this is a relatively novel and, arguably, rather advanced feature, some commercial APIs, like &lt;fireworks.ai&gt; are starting to support it.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code generation</span>"
    ]
  },
  {
    "objectID": "part2/code.html#limitations-and-caveats",
    "href": "part2/code.html#limitations-and-caveats",
    "title": "9  Code generation",
    "section": "Limitations and caveats",
    "text": "Limitations and caveats\nNeedless to say, code generation is full of subtle and not-so-subtle problems. For starters, some hallucinations are going to happen, and this might result in several different types of problems. The simplest case is getting code that is not syntactically-correct, that is, code that doesn’t parse. If this is your main problem, then you’re lucky because this is simple to check. Just run a linter for your target language and retry if you find any syntax errors.\nA more complicated issue is when your model generates syntactically-correct code that throws an exception. This is still not so terrible because you can run the code and check for exceptions. However, running code generated by an LLM is a bad idea if you don’t have some guardrails in place. For all you know, the code may have an instruction to wipe out your hard drive. So you must always run LLM-generated code in a sandboxed environment. This is especially true when you’re running code generated by a user-facing LLM. Someone will hack that LLM to generate some system-breaking instruction.\nThe third level of problem is when your LLM generates code that runs without exceptions but doesn’t do the right thing. This is, in general, impossible to detect beforehand. However, depending on your specific use case, you may be able to check the result is what you expect, and even roll-back any potential side effects if that isn’t the case. For example, you can have your code work on a copy of the relevant data, and check for any unexpected changes before merging that data back.\nThis is, however, the most important open problem in program synthesis from natural language, and one that, I believe, will require a new paradigm that goes beyond statistical language modeling to fully solve it.\nGrammar-restricted output is is one of the most effective ways to make code generation more robust. Still, this process is ad-hoc, not baked into the training process. Thus, it is entirely possible for the LLM to get stuck simply because it doesn’t give a high probability to any valid tokens. If the LLM wouldn’t naturally produce, at least with some non-trivial probability, the right response, there is no ad-hoc filtering mechanism that can force it to generate correct code.\nThis means adequate prompting and possibly fine-tuning for specific domains will remain relevant strategies in the near term.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code generation</span>"
    ]
  },
  {
    "objectID": "part2/code.html#conclusions",
    "href": "part2/code.html#conclusions",
    "title": "9  Code generation",
    "section": "Conclusions",
    "text": "Conclusions\nCode generation is one of the most surprising (weakly) emergent capabilities in the current language modeling paradigm. The fact that pure statistical correlations between tokens give rise to something that can mostly code—granted, at the level of maybe a 2nd year CS college student, in the best of cases—is something I wouldn’t expect to be possible even three years ago. On the other hand, strong code generation is one of the most powerful and versatile capabilities in language models. It opens the door for a myriad of integrations with existing and newly created tools.\nA general-purpose language model paired with a general-purpose code interpreter is one step closer to AGI, there is no doubt. If Turing is right—and would you bet he isn’t?—, any problem that can be solved with an effective, repeatable, systematic method, can be solved with any of the modern general-purpose programming languages. An LLM that can code at the level of the best human programmers would, almost by definition, have a general intelligence.\nThe only gap that needs bridging is getting the model to produce the right code. But this might well be the hardest problem in computer science. We know that it is, in general, impossible to know what a program will do without running it. But this doesn’t mean machines are necessarily any less capable than humans in principle. Who says our brains aren’t just very powerful machines?\nBut all of that is hypothetical. In the meantime, even with the many limitations of modern LLMs, code generation is one of the most useful tools you have at your disposal to build practical, useful applications using language models. We will spend a lot of time in Part 3 playing around with different strategies to turn LLMs into effective coders.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code generation</span>"
    ]
  },
  {
    "objectID": "part2/agents.html",
    "href": "part2/agents.html",
    "title": "10  Agents",
    "section": "",
    "text": "Anatomy of an LLM-powered agent\nAn LLM agent is, ultimately, just a plain-old AI agent with some linguistic capabilities. So let’s review the general definition of agent in the AI field, and explore some of the basic agent architectures that have been invented.\nIn its simplest conception, an agent is a software component embedded within a larger computational system we call the environment. This environment can be anything from a computer simulation, to the whole Internet, including other agents (computational or otherwise). THe agent always has some goals—which can as abstract and ambiguous as you want—that it must further by performing some actions. For this purpose, the agent has some sensors by which it can obtain information from the environment, and some actuators that allow it to exert changes in the environment. Finally, the agent has some internal reasoning process to decide which actions to perform given the current environment state (see Figure 10.1).\nflowchart LR\n    agent(Agent)\n    env((Environment))\n\n    env-- sense --&gt;agent\n    agent-- reason --&gt;agent\n    agent-- act --&gt;env\n\n\n\n\nFigure 10.1: A basic agent architecture.\nThis picture is complete from the outside, but if we look inside the agent, we can further refine it. Here is where literature is—quite literally—overflown with variants of “standard” agent architectures. So I won’t attempt to cover the majority of variants or even be too formal. Rather, I’ll explain what are the main ingredients in most used agent architectures, using a casual nomenclature.\nThe most basic agent that does some form of reasoning needs to do two separate things: keep track of the state of the world (which may include it’s own state) and then reason about that world to decide which actions to take. This reasoning can be cyclical, updating its own state with new information inferred from previous state. We can thus come up with a sensible basic agent architecture (see Figure 10.2).\nflowchart TB\n    subgraph Agent\n        sensors[Sensors]\n        state(Internal\\nRepresentation\\nof World State)\n        reason[Reasoning\\nModule]\n        actuators[Actuators]\n    end\n\n    subgraph Environment\n        env((World State))\n    end\n\n    env-- perceive --&gt;sensors\n    sensors -- update --&gt;state\n    state -- inform --&gt;reason\n    reason -- update --&gt;state\n    reason -- choose --&gt;actuators\n    actuators -- execute --&gt;env\n\n    classDef white fill:#fff\n    class Agent white\n    class Environment white\n\n\n\n\nFigure 10.2: Inside the basic agent architecture.\nThis architecture is sufficiently detailed for us to discuss different variants, while being sufficiently abstract to allow many different use cases and implementations. For example, we haven’t explicitely defined what goes in the internal representation of the world: it could be anything from a structured set of values to a list of natural language facts to a full-blown database. We also haven’t specified what sensors or actuators actually are, which depend heavily on the concrete use case.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#anatomy-of-an-llm-powered-agent",
    "href": "part2/agents.html#anatomy-of-an-llm-powered-agent",
    "title": "10  Agents",
    "section": "",
    "text": "Adding natural language\nSo far we haven’t used the fact our agent is LLM-powered, and for good reason. The basic agent architecture we just defined is agnostic to implementation details. Adding an LLM to the agent architecture just means some, or all of the modules are implemented using language models: that is, with prompts, RAG, function calling, and everything else we’ve seen so far.\nFirst, perception may be LLM-based: your agent can receive information from the environment in the form of natural language documents. Likewise, the actuators can be LLM-based, e.g., via function calling. But more importantly, the reasoning process and the internal representation may involve language models as well. The internal representation can be a straightforward collection natural language claims about the environment, which the agent updates and refines via reasoning. And the reasoning itself can be guided with some clever prompting and thus occur entirely in a linguistic framework. Finally, the environment itself can contain other language models, or otherwise be implemented also as a language-based simulation with clever prompts.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#use-cases-for-llm-agents",
    "href": "part2/agents.html#use-cases-for-llm-agents",
    "title": "10  Agents",
    "section": "Use cases for LLM agents",
    "text": "Use cases for LLM agents\nThe most obvious use case for an LLM-based agent is some sort of always-on assistant; your personal Jarvis, if you will. Unfortunately, most AI assistants today are more tools than agents. They turn on after a user command, do their thing, and go back to iddle. All interactions between you and Siri, Cortana, or Google Assistant, are started by you.\nMoving towards more agentic behavior, what we should expect is an AI assistant that has long-term goals—e.g., keeping you informed, or keeping your calendar well organized—and is always consuming information and taking actions to further those goals. So here are a few possible, very broad use cases for such a type of agent.\n\nThe planning agent\nThis is one of the most useful and yet unexplored use cases. An agent that is connected to all your input sources: email, work and family chat groups, etc., collecting all information about upcoming events and tasks. Its goal would be to help you maintain a healthy work-life balance by keeping your calendar up to date. Potentially, the agent could reply back to a meeting email and ask for a reschedule if it knows you’ll be busy that day. Now imagine everyone has such an agent, and they all negotiate with themselves and find the optimal schedule for the group meeting.\n\n\nThe research agent\nThis is an agent whose goal is to answer some complicated, nuanced research question. It can be something related to your work, like mapping a business opportunity, or something more akin to a typical scientific research question. The agent would search online for relevant sources, extract insights, update its internal representation and generate new follow-up questions, building a web of interconnected pieces of information for a given domain.\n\n\nThe coding agent\nThis is the software engineering holy grail: an agent that continuosly scans and improves an existing codebase, perhaps adding unit tests, comments in code, or refactoring existing functions. Additionally, this agent could pick low-complexity issues (like obvious bug fixes) and attempt to solve them automatically, even crafting a pull request when all test cases pass. Furthermore, it could revise existing pull requests and make suggestions for improvements or general comments before a human revisor takes over the final merge.\n\n\nThe gossip agent\nAnd this is more tongue-in-cheek but, imagine an agent that is monitoring all your social accounts: Facebook (who has that?), Twitter/X, LinkedIn, your friends’ WhatsApp group chats, etc. Everytime some event happens—e.g., someone announces they got a job, or are traveling somewhere, etc.—the agent updates it’s internal representation of your social network. And periodically, or whenever something important changes, it will send you updates on who’s doing what or going where.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#beyond-the-single-agent-paradigm",
    "href": "part2/agents.html#beyond-the-single-agent-paradigm",
    "title": "10  Agents",
    "section": "Beyond the single agent paradigm",
    "text": "Beyond the single agent paradigm\nAll of the above are reasonably straightforward applications of the agent paradigm using LLMs. While the devil is in the details, and implementation is often much harder than one first thought, I can definitely see all of the above examples working in the near future, at least within reasonable constraints. However, single-agent architectures are just scratching the surface of what this paradigm can bring. The real deal is in multi-agent environments.\nIn a multi-agent setup, different agents coexist, collaborating and/or competing to achieve common or distinct goals. The power of mulit-agent architectures is that a complex problem may be very hard to model with a single agent, but feasible when subdivided into distinct goals and skills. Take, for example, the coding agent scenario mentioned before. Now, instead of a single agent taking all these tasks one at a time, imagine multiple agents working in parallel, some optimized to fix bugs, others to comment code, and others yet to revise, add tests, make suggestions, etc.\nAnd this is more than just N different agents working in parallel on a codebase. These agents may have to coordinate to solve a big task. For example, maybe one “software architect” agent will take a complex issue and make plan, dividing it into subtasks for different features, and then other “implementation” agents will work out the simpler tasks and do the actual coding.\nFurthermore, you can have gatekeeper agents that revise code and emit some sort of qualitative evaluation. They can even ask questions, and the implementors must convince them of their solution. A lot of both old and recent research suggests than, when combined in such a manner, multiple agents can solve problems that none of them was able to tackle on its own.\nWhy would this work better than just a super coder LLM? You can consider it an advanced form of ensembling mixed with self-reflection, which are in turn advanced forms of chain-of-thought. By combining multiple LLMs with different tasks (i.e., different prompts) in iterated interactions, we give them more chances to revise and regenerate, which translates in more computational power. At the same time, we’re leveraging the ensemble effect of pooling different solutions to the same problem which tends to produce better solutions on average as mistakes are smoothed out.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#symbolic-learning-in-llm-agents",
    "href": "part2/agents.html#symbolic-learning-in-llm-agents",
    "title": "10  Agents",
    "section": "Symbolic learning in LLM agents",
    "text": "Symbolic learning in LLM agents\nPutting a bunch of LLM-powered agents to work together is neat, but we can go further. The next step is make our agents actually learn from their interactions with the environment.\nIn the classic agent architecture we’ve discussed so far, there is no explicit learning happening inside the agent. To some extent, you can think of updating the internal representation as some sort of learning about the environment, but there is no learning about the strategies the agent can use. The reasoning module, however is implemented, has no way (at least so far) to update itself so that, given the same environment, it could perform better in future ocassions instead of repeating the same mistakes.\nThe standard solution to this problem is reinforcement learning. Assume some optimizable inference architecture inside the agent’s reasoning module (e.g., a neural network) and use feedback from the environment to update its parameters. But what if we could achieve something similar using a purely linguistic approach?\nOne way to achieve this is to give the agent the ability to reason about its own performance, and generate hypotheses (which are nothing more than natural language claims) that describe what it has learned. These hypotheses can be stored in a private knowledge base (separate from the environment description) and retrieved during reasoning.\nIn the simplest case, this is a long-term memory of what the agent has tried in different situations, and what has worked. In future situations, a RAG approach could provide examples of things that have worked before. However, this can be refined further, by progressively asking the LLM to produce high-level hypotheses that abstract and summarize similar situations, eventually building operative knowledge, more general than any specific past situation.\nWhat’s incredible is this approach has been shown to improve reasoning and problem-solving skills in domains as complicated as medical question answering. In a recent paper, researchers put agents in a simulated hospital environment, and generated thousands of cases of patients with different conditions. The whole simulation is run by linguistic agents, from patients to nurses to doctors, who interact with each other and decide on a treatment. External rules (from a medical knowledge base) are used to simulate if the treatment is effective or not. And after each case is completed, the doctor agents write a detailed report of the case, which is stored in their internal knowledge base.\nAfter ten thousand or so iterations, the doctor agents had built a case database that contained a distilled knowledge about every possible rundown of diseases, symptoms, treatments, etc., they had seen. This is equivalent to what medical residents see in two years of training. And incredibly, this database, when provided to a GPT-4 single-shot question answering bot, improved its results in a standard medical question benchmarks by almost 10%.\nThis is something akin to reinforcement learning, but it is parameter-free and 100% transparent and explainable. Instead of updating some opaque weights in some neural networks, your LLM-powered agent is learning like humans do: by analysing its actions, taking notes, and constructing hypotheses. While extremely experimental, this is one of the most exciting use cases for LLM agents I’ve seen recently.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#caveats-and-limitations",
    "href": "part2/agents.html#caveats-and-limitations",
    "title": "10  Agents",
    "section": "Caveats and limitations",
    "text": "Caveats and limitations\nAs in everything related to LLMs, you must beware of hallucinations and biases. Ensembles in general, and agents in particular are more robust to random errors because of their distributed nature. This means that unlikely hallucinations that could be deal-breakers in single-LLM scenarios might get smoothed away when you pool together multiple LLMs taking turns to make choices.\nHowever, biases are not random but systematic errors. Using the same model to power all your agents means they are all susceptible to making the same judgement mistakes over and over. In general, there is no algorithmic solution to reducing biases that doesn’t attack the original problem: biased data.\nAnother major caveat of LLM agents is that, compared to a single zero-shot or a simple chain-of-thought prompt, they are significantly more costly. Your agent is always running, which means you’re constantly calling the LLM API, even if there is nothing new to be done. This underscores once again the need for in-house, open-source language models that can be scaled to these heavy-duty scenarios in a cost-effective manner.\nAnd finally, agents in AI have a long history of being abused as an overengineered solution to problems that could otherwise be solved with more direct methods. Do your own experiments and test whether simple prompts, or other, more straightforward augmentation techniques are sufficient for your use case before going the length of building an entire simulated environment.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/agents.html#conclusions",
    "href": "part2/agents.html#conclusions",
    "title": "10  Agents",
    "section": "Conclusions",
    "text": "Conclusions\nThe agent model is one of the most exciting paradigms in artificial intelligence, with a long and rich history of architectures and approaches. It is also one of the hardest to put into practice, at least until now, because it always required far more intelligence that was possible with previous approaches. But with LLMs, agents are seeing a rebirth as one of the most promising approaches to building reliable and truly intelligent AI solutions.\nI am, however, fairly biased. This is one of my dearest research topics and I may well be over-excited about it. So take everything I said here with a grain of salt. Modern agent architectures are still brittle and hard to deploy in large-enough scale for their distributed strenth to be noticeable. Still, they hold great promise, and are one of the few paths forward that may take us all the way to general AI.\nIn Part 3 we will build some agent-based simulations, especially towards the final chapters. Due to their complexity and cost, we won’t spend as much time building agents and simulations as I would like, but you’ll get a taste of how powerful this paradigm can be in ?sec-dnd and ?sec-storyteller.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Agents</span>"
    ]
  },
  {
    "objectID": "part2/finetune.html",
    "href": "part2/finetune.html",
    "title": "11  Fine-tuning",
    "section": "",
    "text": "Why Fine-Tuning?\nIn short, fine-tuning allows you to adapt pre-trained models to domains, tasks, or general requirements for which they were not designed initially with minimum effort. This is achieved with additional training, but some clever techniques are used to make it as efficient as possible. Let’s look at some examples of cases where you may want to fine-tune a model.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "part2/finetune.html#why-fine-tuning",
    "href": "part2/finetune.html#why-fine-tuning",
    "title": "11  Fine-tuning",
    "section": "",
    "text": "Solving a Novel Task or Domain\nAs the landscape of machine learning evolves, new tasks frequently emerge that require specialized understanding and capabilities. Fine-tuning enables existing models to be tailored to these novel tasks without starting from scratch. For instance, a general-purpose language model can be fine-tuned for specialized applications such as legal document analysis, medical diagnosis, or customer sentiment analysis.\nTraining a model from the ground up is often resource-intensive, requiring substantial computational power, time, and large amounts of labeled data. Fine-tuning offers a more efficient alternative by building upon the knowledge already embedded in pre-trained models.\nSince these models have already learned general language patterns and structures during their initial training phase, fine-tuning can often focus on optimizing only a fraction of the parameters relevant to the new task.\n\n\nReusing Large Models\nThe ability to reuse pre-trained models across different downstream tasks is one of the most significant advantages of fine-tuning. Instead of developing separate models for each task, organizations can maintain a single base model that serves multiple purposes through fine-tuning.\nThat is, you can have a large model trained on a large, general purpose corpora, and a bunch of small “adapter” models that you can plugin to steer the big model as necessary.\nFor example, a base LLM trained on general text can be fine-tuned for tasks such as document classification or question-answering in the same domain by adding small adapters for each subtask. This means instead of dozens of big models—one for each task—you can have one even bigger model with dozens of small pluggable parts, maximizing the utility of existing resources and reducing redundancy. This not only streamlines development processes but also fosters consistency across applications.\n\n\nIncorporating User Preferences\nUser preferences and requirements can evolve over time, requiring adjustments to how models respond or behave. Fine-tuning provides a mechanism to incorporate these changes incrementally without retraining from scratch. By training on new datasets that reflect updated user preferences or feedback, organizations can enhance user satisfaction and engagement with their applications.\nFor instance, a conversational AI can be fine-tuned to align more closely with specific customer service protocols or corporate idioms or styles. This responsiveness not only improves the model’s relevance but also fosters trust and loyalty among users.\n\n\nReducing Costs\nIn many cases, organizations may seek to reduce costs associated with deploying large models due to infrastructure limitations or budget constraints. Fine-tuning allows you to adapt smaller models to achieve performance levels comparable to larger base models.\nBy hyper-focusing on specific tasks—and forgetting general-purpose linguistic capacities you may not need, such as the ability to answer Wikipedia-like trivia questions—smaller models can be optimized to deliver high-quality outputs without incurring the expenses associated with larger counterparts. This approach enables organizations to maintain competitive performance while minimizing operational costs.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "part2/finetune.html#flavors-of-fine-tuning",
    "href": "part2/finetune.html#flavors-of-fine-tuning",
    "title": "11  Fine-tuning",
    "section": "Flavors of Fine-Tuning",
    "text": "Flavors of Fine-Tuning\nFine-tuning techniques can be categorized based on their computational cost and efficiency. Here’s an overview of the main fine-tuning strategies, sorted from the most costly to the most efficient.\n\n1. Full Parameter Fine-Tuning\nIn full parameter fine-tuning, all parameters of the pre-trained model are updated during the training process. This approach allows for maximum flexibility and potential performance improvement on specific tasks, as the model can fully adapt to new data. However, it is computationally expensive and requires significant memory resources, making it impractical for very large models or when working with limited hardware.\n\n\n2. Partial Fine-Tuning\nPartial fine-tuning involves updating only a subset of the model’s parameters while keeping others frozen. Typically, this method focuses on the upper layers of the model, which are more task-specific, while lower layers remain unchanged. This approach strikes a balance between performance and resource efficiency, allowing for faster training times and lower memory requirements compared to full fine-tuning.\n\n\n3. Parameter-Efficient Fine-Tuning\nParameter-efficient fine-tuning techniques, such as LoRA (Low-Rank Adaptation) and adapters, involve modifying only a small number of additional parameters while freezing most of the pre-trained model’s weights. This drastically reduces computational costs and storage requirements. PEFT methods maintain comparable performance to full fine-tuning but are much more efficient, making them suitable for low-resource environments and enabling easier deployment across multiple tasks.\n\n\n4. Prompt & Prefix Tuning\nPrompt tuning involves adding trainable prompt embeddings to the input data rather than modifying model parameters directly. This technique allows models to adapt to new tasks by optimizing these prompts while keeping the rest of the model frozen. It is a lightweight approach that requires significantly fewer resources than traditional fine-tuning methods.\nSimilarly, prefix tuning adds trainable tensors to each transformer block in a model. These tensors act as context that guides the model’s output without altering its core parameters. Prefix tuning is efficient and effective for certain applications but may not achieve the same level of performance as more comprehensive fine-tuning methods.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "part2/finetune.html#tips-for-effective-fine-tuning",
    "href": "part2/finetune.html#tips-for-effective-fine-tuning",
    "title": "11  Fine-tuning",
    "section": "Tips for Effective Fine-Tuning",
    "text": "Tips for Effective Fine-Tuning\nFine-tuning can significantly enhance the performance of language models, but to maximize its effectiveness, practitioners should consider several key strategies. As usual, it is critical not to miss the forest for the trees and not fall prey to early optimization. Here are essential tips for effective fine-tuning.\n\n1. Exhaust Prompt Engineering\nThis is the most important advice and the most critical mistake I see small organizations making every single time. Before even thinking about fine-tuning, ensure you have thoroughly explored prompt engineering.\nIf a state-of-the-art model like GPT-4 cannot solve your task with a well-structured prompt and perhaps some few-shot examples, fine-tuning a smaller model is unlikely to yield better results. Effective prompt engineering can often resolve issues without requiring extensive fine-tuning, which is always more costly.\n\n\n2. Quality Trumps Quantity\nIf you have already decided fine-tuning is the way to go, prioritize data quality. Focus on gathering high-quality, diverse examples accurately representing the new task or domain. However much data you can gather will be minuscule compared to the base training set, so quality is the only thing you can control.\nKeep in mind that while you can leverage large models with clever prompt engineering to synthesize additional training data, you must always validate these examples with human experts to ensure their relevance and correctness. One great novel example beats 100 excellent but similar ones.\n\n\n3. Start Small, Grow As Needed\nBegin your fine-tuning efforts with parameter-efficient techniques, such as adapters or Low-Rank Adaptation (LoRA). These methods require fewer resources and are often easier to implement than full parameter tuning.\nYou’ll find tons of open-source implementations of efficient fine-tuning methods, so don’t let the prospect of technical difficulty scare you. Likewise, several LLM providers will let you fine-tune comercial or open source models on their infrastructure, effectively bypassing all need for self-hosting models. This way you get the best of both worlds: a model just for you that someone else takes care of.\nIf and only if parameter-efficient fine-tuning does not meet your needs, then consider transitioning to full parameter tuning as a subsequent step, but unless you’re swimming in private data—and I mean, terabytes of data—you’re most likely safely on the efficient fine-tuning side.\n\n\n4. Stay in the Loop\nFinally, remember the field of AI is evolving extremely fast, with new models and techniques appearing basically every single week. A task that may require fine-tuning today could potentially be solved through effective prompting in next-generation models released tomorrow.\nTherefore, avoid over-investing in ad-hoc fine-tuning pipelines that may become obsolete in a couple of months. Also, assess the capabilities of newer models regularly and be prepared to pivot away from fine-tuned proprietary models as soon as a good prompt shows marginally better results. Prompts are far more portable than models.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "part2/finetune.html#conclusion",
    "href": "part2/finetune.html#conclusion",
    "title": "11  Fine-tuning",
    "section": "Conclusion",
    "text": "Conclusion\nFine-tuning is an excellent strategy for small organizations tackling domain-specific tasks effectively. As AI continues to evolve, the ability to adapt large pre-trained models to meet unique business needs is no longer a luxury but a necessity. For smaller organizations and individual developers, fine-tuning offers a pathway to harness the power of advanced language models without the prohibitive costs associated with training from scratch.\nAlso, leveraging fine-tuning allows small organizations to capitalize on their private data, tailoring models to reflect their specific industry requirements and user preferences. The moat is no longer having the bigger model since some of the best models are open source. The moat is your private data, and fine-tuning lets you get the best combo: a world-class base model trained on privileged data none of your competitors have.\nMoreover, in a competitive landscape where larger organizations like Microsoft, Google, and Meta rely solely on base models, fine-tuning gives small businesses an advantage in niche domains. By optimizing existing models with proprietary data, they can differentiate themselves through improved performance and specialized capabilities that address the specific market needs.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html",
    "href": "part2/optimization.html",
    "title": "12  Optimization of LLMs",
    "section": "",
    "text": "Weight prunning\nPruning is the process of completely removing a number of parameters, thus making the model smaller. The usual approach involves finding a set of parameters that are minimally important, that is, weights that are as close to zero as possible. By setting these weights to exactly zero, we can compress a model and thus reduce the download time and the inference cost. Here are the main variants of weight pruning, along with their advantages and caveats.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#weight-prunning",
    "href": "part2/optimization.html#weight-prunning",
    "title": "12  Optimization of LLMs",
    "section": "",
    "text": "Unstructured Weight Pruning\nUnstructured weight pruning is like trimming the leaves of a tree. Instead of cutting off entire branches, you carefully snip off the individual leaves that are less important. In the context of neural networks, these “leaves” are the individual weights, and the goal is to remove the ones that don’t contribute much to the overall performance of the model.\nThe process works by identifying the weights that are closest to zero. These weights are considered less significant, so they get the chop. By removing these near-zero weights, you can shrink the size of the model without losing too much of its accuracy.\nOne advantage of this approach is its simplicity. It’s easy to understand and implement, and you can apply it at any point in the network. This gives you a lot of flexibility in deciding which weights to remove and where. Another perk is the potential for high compression. If you can identify a large number of unimportant weights, you can really pack down the size of the model, making it more efficient to store and run.\nHowever, there are a couple of downsides to keep in mind. First, the resulting model might end up with a scattered distribution of zero weights, which can be tricky for certain types of hardware to work with efficiently. Second, removing weights can sometimes mess with the intricate relationships that the model has learned, leading to a drop in its overall accuracy.\n\n\nStructured Weight Pruning\nStructured weight pruning is like giving your model a haircut instead of just trimming individual strands of hair. Instead of snipping away at individual weights, this method focuses on removing whole sections of the network, such as entire neurons, filters, or channels. By taking out these larger structures, you can make the model smaller while keeping its overall shape intact.\nOne of the great things about structured pruning is that it can lead to improved efficiency. When you remove entire neurons or filters, the model can run faster on hardware that’s designed for dense computations. This means you can get better performance without sacrificing too much accuracy.\nAnother benefit is that structured pruning tends to have a milder impact on the model’s performance compared to unstructured pruning. Because you’re preserving the overall architecture and the relationships between different parts of the model, it often results in less accuracy loss. It’s like giving your model a neat trim rather than a drastic change.\nHowever, there are some challenges to consider. This method can be a bit more aggressive, meaning it might alter the model’s architecture significantly, which isn’t always what you want. Plus, deciding which structures to prune can be more complex than just picking off individual weights. You need to carefully choose which neurons or filters to remove, which can take some extra thought and experimentation.\n\n\nDynamic Weight Pruning\nDynamic weight pruning is a more flexible approach that adjusts the weights during the training process based on their importance. Unlike static methods, which prune weights after training is complete, dynamic pruning continuously evaluates and prunes weights as the model learns. This means that as the model trains, it can adaptively remove weights that are deemed less significant, allowing for a more nuanced and responsive pruning process.\nOne of the main advantages of dynamic weight pruning is its adaptability. Since the model is constantly assessing which weights are important, it can make more informed decisions about what to prune. This often leads to better retention of crucial weights, which helps maintain or even improve overall model performance compared to static pruning methods.\nHowever, this approach does come with some trade-offs. For one, it can increase training time since the model must continuously evaluate and adjust weights throughout the training process. Additionally, the complexity of implementing dynamic pruning can be a challenge, requiring careful tuning of the pruning criteria and schedules.\nWhen you contrast dynamic weight pruning with the static variants like unstructured and structured pruning, the differences become clearer. Unstructured pruning is straightforward and focuses on removing individual weights based on their magnitude after training, which can lead to scattered zero weights in the model. Structured pruning, on the other hand, removes entire neurons or filters, resulting in a more organized model but potentially altering its architecture significantly.\nIn summary, dynamic weight pruning offers a more adaptive and responsive method for pruning, allowing the model to maintain performance while reducing size. However, it requires more effort in terms of training time and implementation complexity compared to the more straightforward static methods. Each pruning strategy has its own strengths and weaknesses, making the choice largely dependent on the specific needs of the model and the deployment context.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#model-quantization",
    "href": "part2/optimization.html#model-quantization",
    "title": "12  Optimization of LLMs",
    "section": "Model quantization",
    "text": "Model quantization\nModel quantization is a technique used to reduce the memory footprint and computational requirements of neural networks by representing weights and activations with lower precision. Instead of using the standard 32-bit floating-point numbers, quantization allows for smaller formats, such as 16-bit or even 8-bit integers.\nThis reduction in precision means that each weight becomes an approximate representation of its original value. While this can introduce some mathematical differences in computations, the inherent approximations in language modeling often mean that a well-quantized model can still perform similarly to its full-precision counterpart, all while significantly decreasing memory usage and speeding up inference times.\nThese are some of the most common variants of model quantization:\n\nPost-Training Quantization (PTQ)\nPost-Training Quantization (PTQ) is a technique that involves applying quantization to a model that has already been trained, without any additional training steps. In this process, the model’s weights and activations are converted to lower precision formats after the initial training is complete.\nOne of the main benefits of PTQ is its simplicity; it is easy to implement because it does not require retraining the model. This allows for quick deployment, making it possible to rapidly quantize existing models and prepare them for use.\nHowever, there is a downside to this approach. The model may experience a drop in accuracy if the quantization process does not effectively capture the characteristics of the original model. This means that while PTQ is efficient, it can lead to reduced performance in some cases.\n\n\nQuantization-Aware Training (QAT)\nQuantization-Aware Training (QAT) is a method used during the training of a model to prepare it for quantization. This technique simulates the effects of quantization while the model is being trained, allowing it to learn how to handle reduced precision from the beginning. By incorporating quantization effects into both the forward and backward passes of training, the model becomes more robust to the eventual reduction in precision.\nOne of the key benefits of QAT is that it typically results in better accuracy retention compared to models that are quantized after training. Since the model is aware of quantization during training, it can adapt its weights and activations accordingly, leading to more reliable performance. This adaptability helps the model cope with the noise introduced by quantization.\nHowever, QAT comes with some challenges. The training process becomes more complex and resource-intensive because it requires additional operations and adjustments to the loss function. Implementing QAT demands careful tuning and validation to ensure that the model accurately simulates quantization effects. As a result, QAT often requires more computational resources and a longer training time compared to simpler methods like Post-Training Quantization (PTQ).\n\n\nDynamic Quantization\nDynamic quantization is a method where the weights of a model are converted to lower precision during inference, while the activations are quantized based on their observed range at runtime. This means that instead of using fixed lower precision for everything, the model adapts the precision of activations dynamically as it processes data.\nOne of the main advantages of dynamic quantization is its flexibility. By adapting to the input data, it can help maintain accuracy even with lower precision. This adaptability allows the model to perform well across a variety of inputs without needing extensive modifications.\nAdditionally, dynamic quantization is simpler to implement compared to techniques like Quantization-Aware Training (QAT). It can often be applied to existing models without requiring significant changes, making it a practical choice for many applications.\nHowever, there are some downsides. Dynamic quantization may not achieve the same level of compression as other quantization methods, since activations remain in floating-point format, which can lead to larger memory usage during inference. Moreover, careful tuning is required to ensure that the quantization parameters are optimized for the best performance, which can add complexity to the implementation process.\n\n\nDynamic Range Quantization\nDynamic range quantization is a specific form of dynamic quantization that aims to strike a balance between full integer quantization and standard floating-point inference. In this approach, the weights of the model are quantized to 8-bit integers during conversion, while other tensors, like activations, remain in floating-point format. However, during inference, the activations are dynamically quantized to integers based on their observed range, allowing the model to maintain higher accuracy while still benefiting from reduced memory usage and faster computations.\nOne of the main advantages of dynamic range quantization is its ability to achieve significant speed improvements similar to full integer quantization while maintaining higher accuracy. This method also has a simpler pipeline compared to full integer quantization, making it easier to implement. The dynamic adjustment of activation quantization allows for better utilization of the quantized bits, maximizing the accuracy of the model.\nHowever, there are some downsides to consider. While dynamic range quantization reduces the model size, it may not achieve the same level of compression as full integer quantization since activations are still stored in floating-point format. Additionally, although it generally maintains good accuracy, it’s important to evaluate the quantized model to ensure that performance degradation is acceptable. Some optimizations may not be fully realized if the target hardware does not support dynamic quantization efficiently.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#knowledge-distillation",
    "href": "part2/optimization.html#knowledge-distillation",
    "title": "12  Optimization of LLMs",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a technique used to train a smaller “student” model to replicate the behavior of a larger “teacher” model. The student model learns to match the outputs or intermediate representations of the teacher model, allowing it to absorb essential knowledge while being more compact and efficient. This method is particularly beneficial for deploying models in resource-constrained environments, as it helps maintain high performance with reduced computational demands.\nThere are several approaches to knowledge distillation, each with its own methodology and use cases. The three primary variants are offline distillation, online distillation, and self-distillation.\n\nOffline Distillation\nThis is the traditional approach where the teacher model is trained first. After the teacher has been trained, the student model is trained separately using the soft labels generated by the teacher. These soft labels provide more nuanced information than hard labels, enabling the student to learn from the teacher’s predictions effectively. The main advantage of offline distillation is its straightforward implementation, as the teacher’s weights remain unchanged during the training of the student. However, this method requires a well-trained teacher model in advance, which can be resource-intensive.\n\n\nOnline Distillation\nThis approach addresses scenarios where a pre-trained teacher model may not be available or when the teacher model is too large to store or process efficiently. In this approach, the teacher and student models are trained simultaneously, allowing the student to learn from the teacher dynamically during training. This method can be particularly useful for handling non-stationary or streaming data. While online distillation can lead to faster training times and adaptability, it requires that both models share the same architecture, which can complicate the setup.\n\n\nSelf-Distillation\nA variant where the student and teacher are the same model, but the model is trained multiple times. In this case, the model first learns from the data and then refines its predictions by treating its own outputs as soft labels in subsequent training iterations. This approach can help improve the model’s performance without needing a separate teacher model. The advantage of self-distillation is its simplicity and reduced resource requirements, but it may not capture the full range of knowledge that a larger teacher model could provide.\n\n\nAdvantages and Disadvantages of Knowledge Distillation\nKnowledge distillation offers several benefits. It significantly reduces the size of the model, making it more feasible to deploy on devices with limited storage and computational power. Distilled models can also process data more quickly, leading to faster inference times, which is crucial for real-time applications. Additionally, training a student model using knowledge distillation is less resource-intensive than training a large model from scratch, as it often requires less data and computational power.\nHowever, there are some drawbacks to consider. The distillation process necessitates a well-trained teacher model, which can be a barrier in terms of the required computational resources and training time. Furthermore, while distilled models retain much of the accuracy of their larger counterparts, they may lose some minor decision-making nuances that the more complex model captures.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#factorization",
    "href": "part2/optimization.html#factorization",
    "title": "12  Optimization of LLMs",
    "section": "Factorization",
    "text": "Factorization\nFactorization is a general technique used to simplify neural network models by breaking down weight matrices into products of smaller matrices. This process reduces the number of parameters in the model, making it more efficient in terms of storage and computation. By using factorization, we can maintain performance while creating more compact models.\nTwo very common approaches are low-rank factorization and block-term decomposition.\n\nLow-Rank Factorization\nLow-rank factorization involves decomposing a large weight matrix into two smaller matrices. The idea is that many weight matrices in neural networks can be approximated well by using fewer parameters. By representing the original matrix as a product of two smaller matrices, we can significantly cut down on the number of parameters that need to be stored and processed. But this is not always feasible, because it may be difficult to find a large number of near-zero weights.\nBlock-term decomposition (BTD) is a more advanced factorization technique that breaks down a weight matrix into a sum of products of smaller matrices. This method allows for a more nuanced representation of the original matrix by capturing different patterns and structures within the weights.\nBTD offers a higher compression ratio compared to low-rank factorization, which means it can reduce the model size even further. This is particularly beneficial when dealing with convolutional layers, as it helps preserve the spatial relationships in the data, leading to better performance. However, BTD is more complex to implement and requires careful tuning of the sizes of the smaller matrices. Like low-rank factorization, optimizing these sizes for each layer can also be resource-intensive.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#sparse-architectures",
    "href": "part2/optimization.html#sparse-architectures",
    "title": "12  Optimization of LLMs",
    "section": "Sparse Architectures",
    "text": "Sparse Architectures\nSparse architectures are neural network designs that only require a subset of weights to be active during inference. This approach aims to improve efficiency by reducing the computational and memory requirements of the model. The most common example of a sparse architecture is the mixture of experts (MoE) model.\n\nMixture of Experts (MoE)\nIn a MoE model, several sub-networks, called experts, are trained in parallel on different parts of the input space. During inference, a gating network selects one or a few of the most relevant experts to process the input, while the other experts remain inactive. This sparse activation of experts leads to computational and memory savings compared to a dense model where all experts are active for every input.\nThe Mixture of Experts (MoE) architecture has several advantages that enhance its performance and efficiency. By activating only a subset of experts for each input, MoE models achieve improved computational and memory efficiency compared to dense models. This selective activation allows MoE models to scale effectively, accommodating a large number of experts that can specialize in handling diverse inputs. As a result, the ability to choose relevant experts for each input can lead to better performance, particularly on complex or varied datasets.\nHowever, there are also challenges associated with MoE models. The increased complexity of training a MoE model arises from the need for additional components, such as the gating network, which can complicate the overall training process and extend the time required to train the model. Additionally, there is a risk of load imbalance; if the gating network assigns inputs unevenly among the experts, some may be underutilized while others are overburdened. This imbalance can hinder the model’s efficiency. Furthermore, the sequential nature of expert selection can limit opportunities for parallelization, which is crucial for efficient inference.\n\n\nOther Sparse Architectures\nWhile MoE is the most prominent example, there are other sparse architecture designs:\n\nSparse Convolutional Neural Networks (Sparse CNNs): These models exploit the inherent sparsity in convolutional layers by only storing and computing non-zero weights and activations. Sparse CNNs can achieve significant memory and computational savings compared to dense CNNs.\nSparse Transformer Models: Transformer models, widely used in natural language processing, can be made sparse by introducing sparsity in the attention mechanism. Sparse Transformers aim to reduce the quadratic complexity of standard attention by only computing attention scores for a subset of token pairs.\nSparse Recurrent Neural Networks: Sparsity can also be introduced in recurrent neural networks by selectively activating neurons or connections during inference. This can lead to more efficient processing of sequential data.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part2/optimization.html#conclusions",
    "href": "part2/optimization.html#conclusions",
    "title": "12  Optimization of LLMs",
    "section": "Conclusions",
    "text": "Conclusions\nWhile various optimization techniques for large language models (LLMs) such as weight pruning, quantization, knowledge distillation, factorization, and sparse architectures offer significant benefits, none of these approaches can be deemed universally superior. Each method comes with its own set of trade-offs that must be carefully considered based on the specific requirements of the application. Plus, they can often be combined.\nFor instance, weight pruning can effectively reduce model size and improve efficiency, but it may lead to accuracy loss if important weights are removed. Quantization can significantly lower memory usage and speed up inference, yet it can also introduce precision-related errors that affect performance. Knowledge distillation allows for the creation of smaller, more efficient models but relies on the availability of a well-trained teacher model. Factorization techniques can simplify models and reduce parameters, but they may require complex tuning to maintain accuracy. Sparse architectures, particularly mixture of experts, enhance efficiency by activating only a subset of parameters, but they introduce additional complexity and potential load balancing issues.\nThe most successful modern LLMs that can operate effectively on commodity hardware often employ a combination of these techniques. By integrating sparse architectures with clever quantization strategies, these models can achieve a balance between performance and resource efficiency. Additionally, many smaller models are distilled from larger ones, allowing them to retain essential knowledge while being more compact and easier to deploy. This synergy among various optimization methods can be seen as a kind of “free lunch,” where the benefits of one approach can complement another.\nWhen there is a need to further enhance performance, practitioners can explore additional methods or refine existing techniques. The key lies in understanding the specific needs of the application and the constraints of the deployment environment. By leveraging the strengths of multiple optimization strategies, developers can create efficient, high-performing models that meet the demands of real-world applications.",
    "crumbs": [
      "Techniques",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optimization of LLMs</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html",
    "href": "part3/01_chatbot.html",
    "title": "13  The Chatbot",
    "section": "",
    "text": "Initial setup\nWe will use the Python library streamlit to build the frontend of our application. Streamlit is a very straightforward framework to create data science apps with a focus on productivity over fancyness. This means our artistic freedom is greatly restricted, and all our apps will look pretty much the same boring way, but we will need zero HTML or CSS and development will be a breeze.\nAs for the LLM, I’m currently inclined to use Mistral.ai as the provider. They’re one of the major LLM makers out there, just behind OpenAI, Google, and Meta in terms of scale. They are very reliable, and they distribute several of their models as open source, which means even though we’ll be using their cloud-hosted models in this book, you won’t be tied to their platform. You can take the same models and host them on your own infrastructure whenever you choose to.\nThe setup of this project will thus be simple. Our only major dependencies are streamlit and mistralai. We’ll start by creating a virtual environment and installing these dependencies.\nAfter the initial creation, it will be convenient to dump our environment setup so we can reconstruct the exact same environment later.\nWith the environment set up, we’re ready to star hacking our ChatGPT clone.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#initial-setup",
    "href": "part3/01_chatbot.html#initial-setup",
    "title": "13  The Chatbot",
    "section": "",
    "text": "# Create a new virtual environment\n$ python3 -m venv .venv\n\n# Activate it\n$ source .venv/bin/activate\n\n# Install dependencies\n$ pip install streamlit mistralai\n\n$ pip freeze &gt; requirements.txt\n\n\n\n\n\n\nUsing the book’s source code?\n\n\n\n\n\nIf you are using the source code provided with the book, try the following instead so you get the exact same environment I used:\n# Create a new virtual environment\n$ python3 -m venv .venv\n\n# Activate it\n$ source .venv/bin/activate\n\n# Install dependencies\n$ pip install -r requirements.txt",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#basic-chat-app",
    "href": "part3/01_chatbot.html#basic-chat-app",
    "title": "13  The Chatbot",
    "section": "Basic chat app",
    "text": "Basic chat app\nA streamlit application can be as simple as a Python file. Here’s the basic layout of a chat app that just echoes what you type.\n1import streamlit as st\n\n2st.set_page_config(page_title=\"Chatbot\", page_icon=\"🤖\")\n\n3msg = st.chat_input()\n\n4with st.chat_message(\"user\"):\n    if msg:\n        st.write(msg)\n\n1\n\nImport the streamlit module.\n\n2\n\nSet the page configuration.\n\n3\n\nInstantiate a user input and get the input text.\n\n4\n\nEcho the input text if it’s not None.\n\n\nAssuming this code is located at code/chatbot.py, we can run it with:\n$ streamlit run code/chatbot.py\nA browser will automatically open and show something like Figure 13.1.\n\n\n\n\n\n\nFigure 13.1\n\n\n\nThis is just a barebones app with no chat functionality at all. It will simply write anything you type in the input box back into the chat screen. But it will serve us to understand the basic application lifecycle in streamlit.\nThe code above is the blueprint for a typical Streamlit app. You start by importing streamlit and setting up the page title and icon, and then proceed to include any number of Streamlit statements. The code is executed top-to-bottom synchronously by the Streamlit server, and all commands are sent via websockets to a web app. There are no callbacks or hidden state.\nIf this sounds alien, don’t worry, it just means a Streamlit app works just like a regular Python script: you can have global variables, methods, classes, and use them in the same you would in a terminal script, and it will (mostly) magically work.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#first-interaction-with-an-llm",
    "href": "part3/01_chatbot.html#first-interaction-with-an-llm",
    "title": "13  The Chatbot",
    "section": "First interaction with an LLM",
    "text": "First interaction with an LLM\nOk, it’s time for the real thing. Let’s send our first message to a language model! This is easy using the mistralai package. We just need to instantiate a MistralClient and call it’s chat message.\nFirst, we will need an API key from Mistral.ai. This token is a password that grants your code access to the model. Beware not to share it with anyone, or they can use the API on your behalf (and you’ll pay for it). Paste the API token into .streamlit/secrets.toml like this:\n# File .streamlit/secrets.toml\nmistral_api_key=\"your-mistral-api-key\"\nNow we can use Streamlit’s native secret management to inject this token into our application without having to copy/paste it every single time.\nimport streamlit as st\n1from mistralai.client import MistralClient\n\nst.set_page_config(page_title=\"Chatbot\", page_icon=\"🤖\")\n\n2client = MistralClient(api_key=st.secrets.mistral_api_key)\n\n1\n\nImport the MistralClient class from mistralai.\n\n2\n\nInstantiate the client with your private API token.\n\n\nOnce our client is set up, we just need a brief change to our chat workflow to actually call the LLM API and output that response.\n#... in the preamble\n\n1from mistralai.models.chat_completion import ChatMessage\n\n#... after the initialization code\n\n2msg = st.chat_input()\n\nif not msg:\n    st.stop()\n\nwith st.chat_message(\"user\"):\n    st.write(msg)\n\n3response = client.chat(\n    messages=[ChatMessage(role=\"user\", content=msg)],\n    model=\"open-mixtral-8x7b\"\n)\n\n4with st.chat_message(\"assistant\"):\n    st.write(response.choices[0].message.content)\n\n1\n\nImport the ChatMessage class for calling the LLM.\n\n2\n\nGet the user input, and echo it.\n\n3\n\nConstruct and execute the API call.\n\n4\n\nOutput the response content to Streamlit.\n\n\nHit RR on Streamlit to hot-reload the code, and type something fancy on the input box. Maybe something like “What is the meaning of life?”. In the blink of an eye (or maybe, a couple of lazy eyes) your message will be sent to the LLM API and the response streamed back to your application. Voilá, we have a ChatGPT clone!\n\n\n\n\n\n\nFigure 13.2",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#persisting-the-chat-state",
    "href": "part3/01_chatbot.html#persisting-the-chat-state",
    "title": "13  The Chatbot",
    "section": "Persisting the chat state",
    "text": "Persisting the chat state\nNot so fast! You will quickly notice there is something really unsatisfying with out current implementation. Every time you send a new message, the whole chat history is cleared!\nThis is due to Streamlit’s simple lifecycle. Since the script is executed top-to-bottom for every interaction, every button or key pressed essentially works like if the app was just started. There is no magic here, we haven’t done anything to store the conversation, so there is no place Streamlit could get it from. Let’s fix that first.\nStreamlit provides a built-in way to deal with state that persists across executions, in the object st.session_state. This is a dict-like structure that can hold whatever we want, and survives re-runs (but not refresing the page). Technically, it’s session-associated data, meaning every different user of our web application gets their own private storage. But enough technobabble, let’s see some code; it’s easier to just show how it works.\nFirst, we initialize the history key with an empty list, the first time the app is loaded. This goes right after the client initialization.\nif 'history' not in st.session_state:\n    st.session_state.history = []\nNext, before getting into the user/assistant chat workflow, we have to reconstruct the previous chat history, and render all the messages.\nfor message in st.session_state.history:\n    with st.chat_message(message['role']):\n        st.write(message['content'])\nThen, when we retrieve the new user input, we have to remember to store it in the history.\nmsg = st.chat_input()\n\nif not msg:\n    st.stop()\n\nwith st.chat_message(\"user\"):\n    st.write(msg)\n    st.session_state.history.append(dict(\n        role=\"user\",\n        content=msg\n    ))\nAnd do the same with the LLM response.\nresponse = client.chat(\n    messages=[ChatMessage(role=\"user\", content=msg)],\n    model=\"open-mixtral-8x7b\"\n).choices[0].message.content\n\nwith st.chat_message(\"assistant\"):\n    st.write(response)\n    st.session_state.history.append(dict(\n        role=\"assistant\",\n        content=response\n    ))\nIf you run this update, you’ll notice the chat history persists throughout the conversation. However, one thing is still missing. Even though the chat interface shows all the previous messages, the LLM is only receiving the last message. This is evident in Figure 13.3 where the chatbot fails to remember information that was given just before.\n\n\n\n\n\n\nFigure 13.3",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#remembering-the-context",
    "href": "part3/01_chatbot.html#remembering-the-context",
    "title": "13  The Chatbot",
    "section": "Remembering the context",
    "text": "Remembering the context\nMost LLM APIs have a straightforward way to submit a full conversation history. In the case of mistralai, the messages parameter in the client.chat method receives a list of ChatMessage instances. Each message specifies the content and a role, which can be:\n\nsystem: Used to send behavior instructions to the LLM. We will see an example right away.\nuser: Used to specify the user input.\nassistant: Used to specify the previous assistant input.\n\nWhen you send a request to the LLM API, you can submit the previous messages (both the user’s and the LLM’s), and simulate as if the chatbot remembered the conversation. There is no actual memory in the LLM service, as it’s a stateless API. So it’s your responsibility to reconstruct the conversation history in a way that is meaningful.\nIn our app, since we are already storing the conversation history in the session storage, we just need to construct the propper ChatMessage instances:\n# ...\n\nmessages = [\n    ChatMessage(**msg) for msg in st.session_state.history\n]\n\nmessages.append(ChatMessage(role=\"user\", content=msg))\n\nresponse = client.chat(\n    messages=messages,\n    model=\"open-mixtral-8x7b\"\n).choices[0].message.content\n\n# ...\nThis update seems to magically add memory to our chatbot, although you need that, under the hood, there is no magic. It’s you who’s keeping track of the conversation.\n\n\n\n\n\n\n\nNote\n\n\n\nThe interesting thing is that, since it’s you who’s keeping track of the conversation, you can actually “cheat” and send a made-up history, and the chatbot will play along. We will use this trick in other applications to simulate certain specific behaviours.\n\n\nBefore closing, let’s take a look at the system prompt. By incorporating messages with role=\"system\" you can guide the chatbot behaviour. In principle, this is no different to what you can do with a well crafted user input, but some models are trained to pay special attention to system messages and prioritize them over user messages.\nThis means that, for example, you can override a previous instruction that you gave as a user, but it will much harder to do so if the instruction was given as a system message. Thus, these messages are often used to instruct the LLM to behave politely, to avoid answering biased questions, etc. You can also use it to set the tone and style of the conversation.\nTo try this, let’s add an input box for the system messages, so we can experiment with different prompts. By default, we’ll instruct the LLM to be nice.\nsystem_prompt = st.sidebar.text_area(\"System Prompt\", value=\"\"\"\nYou are a polite chatbot that answers concisely and truthfully.\n1\"\"\".strip())\n\n# get the user input\n\nmessages = [\n    ChatMessage(**msg) for msg in st.session_state.history\n]\n\n2messages.insert(0, ChatMessage(role=\"system\", content=system_prompt))\nmessages.append(ChatMessage(role=\"user\", content=msg))\n\nresponse = client.chat(\n    messages=messages,\n    model=\"open-mixtral-8x7b\"\n).choices[0].message.content\n\n1\n\nGet a custom system prompt (as a sidebar widget).\n\n2\n\nInsert the system prompt at the beginning of the message list.\n\n\nAnd we can try it out with a not-so-nice instruction. Delightful.\n\n\n\n\n\n\nFigure 13.4\n\n\n\nFinally, before moving on, let’s add a very needed reset button for when we want to restart the converstation.\n# initialize history\n\nif 'history' not in st.session_state:\n    st.session_state.history = []\n\nif st.sidebar.button(\"Reset conversation\"):\n    st.session_state.history.clear()\n\n# ... re-render conversation\nThis code must go just before we re-render the conversation history. Since Streamlit will re-run the whole script, placing this call just before rendering the conversation will have the effect of emptying the list when the button is clicked.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#streaming-messages",
    "href": "part3/01_chatbot.html#streaming-messages",
    "title": "13  The Chatbot",
    "section": "Streaming messages",
    "text": "Streaming messages\nAs a final tweak, let’s turn the chatbot response into a nice typing animation. We need to do two things. First, we’ll call a different API method that returns the response as a generator instead of the whole output at once. Then we’ll wrap that with our own generator to unpack the API response and extract the text chunks.\ndef stream_response(messages):\n1    result = []\n\n2    for chunk in client.chat_stream(\n        messages=messages,\n        model=\"open-mixtral-8x7b\"\n    ):\n        text = chunk.choices[0].delta.content\n3        result.append(text)\n4        yield text\n\n5    st.session_state.history.append(dict(\n        role=\"assistant\",\n        content=\"\".join(result)\n    ))\n\n1\n\nWe’ll save the chunks for later.\n\n2\n\nCall the chat_stream method, and iterate over the response.\n\n3\n\nSave the current chunk.\n\n4\n\nAnd yield it.\n\n5\n\nDon’t forget to update the history (this is why we needed that result list).\n\n\nFortunately, Streamlit has a nice functionality to render a stream of text.\nwith st.chat_message(\"assistant\"):\n    st.write_stream(stream_response(messages))\nGo ahead and run this update, and you’ll see how the chatbot response is streamed almost realtime (depending on how fast the API responds, of course). This has the upside that the user doesn’t have to wait until the generation is completed to see something, which improves the experience a lot.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#tidying-up",
    "href": "part3/01_chatbot.html#tidying-up",
    "title": "13  The Chatbot",
    "section": "Tidying up",
    "text": "Tidying up\nAnd now, for real the final thing. Let’s tidy up a bit and extract the key functionalities we will need to reuse in future chapters. There are two pieces of functionality that we will reuse over and over: the history management and the streaming animation. The latter is already encapsulated in a function, so let’s focus on the conversation workflow.\nTo encapsulate the conversation functionality, let’s wrap both the storing and retrieval of the conversation history in a class.\n# file code/utils.py\n\nimport streamlit as st\nfrom mistralai.models.chat_completion import ChatMessage\nfrom mistralai.client import MistralClient\n\n\nclass Chatbot:\n    def __init__(self, system_prompt:str, model:str) -&gt; None:\n        self.system_prompt = system_prompt\n        self.model = model\n        self.client = MistralClient(api_key=st.secrets.mistral_api_key)\n\n        if \"history\" not in st.session_state:\n            st.session_state.history = []\n\n    def reset(self):\n        st.session_state.history.clear()\n\n    def store(self, role, content):\n        st.session_state.history.append(dict(content=content, role=role))\n\n    def history(self, context=0):\n        messages = st.session_state.history[-context:]\n        return [ChatMessage(**m) for m in messages]\nThis class takes care of initializing and resetting the conversation history when necessary, as well as storing and retrieving intermediate messages.\nWe can also encapsulate the actual submission of a new message with the whole streaming functionality:\n# file code/utils.py\n\nclass Chatbot:\n    # ...\n\n    def _stream(self, messages):\n        result = []\n\n        for chunk in self.client.chat_stream(\n            messages=messages,\n            model=self.model,\n        ):\n            text = chunk.choices[0].delta.content\n            result.append(text)\n            yield text\n\n        self.store(\"assistant\", \"\".join(result))\n\n    def submit(self, content, context=0):\n        messages = self.restore(context)\n\n        messages.insert(0, ChatMessage(role=\"system\", content=self.system_prompt))\n        messages.append(ChatMessage(role=\"user\", content=content))\n        self.store(\"user\", content)\n\n        return self._stream(messages)\nThis is just a refactoring of our previous code, there is new functionality here. With this redesign we can now tidy up our chatbot application into a very concise code:\nimport streamlit as st\nfrom utils import Chatbot\n\nst.set_page_config(page_title=\"Chatbot\", page_icon=\"🤖\")\n\nsystem_prompt = st.sidebar.text_area(\"System Prompt\", value=\"\"\"\nYou are a polite chatbot that answers concisely and truthfully.\n\"\"\".strip())\n\nbot = Chatbot(system_prompt, 'open-mixtral-8x7b')\n\nif st.sidebar.button(\"Reset conversation\"):\n    bot.reset()\n\nfor message in bot.history():\n    with st.chat_message(message.role):\n        st.write(message.content)\n\nmsg = st.chat_input()\n\nif not msg:\n    st.stop()\n\nwith st.chat_message(\"user\"):\n    st.write(msg)\n\nwith st.chat_message(\"assistant\"):\n    st.write_stream(bot.submit(msg, context=5))\nThat’s it! Just 30 lines of code to re-implement a chatbot complete with conversation history, customizable system prompt, and message streaming.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/01_chatbot.html#conclusions",
    "href": "part3/01_chatbot.html#conclusions",
    "title": "13  The Chatbot",
    "section": "Conclusions",
    "text": "Conclusions\nAnd that’s it, our first ChatGPT clone! Nothing too fancy, but if you’ve never coded a chatbot before, this can look like magic. It certainly did for me!\nThe patterns we learned in this chapter will help us tremendously in the upcomming applications. Just to recap, this is what we’ve mastered:\n\nHow to setup a basic chat workflow, complete with history.\nHow to configure the chatbot with a system prompt.\nHow to stream the LLM response to get a fancy typing animation.\n\nNow that we have the basic setup ready, we can start building some cool stuff.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chatbot</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html",
    "href": "part3/02_pdfbot.html",
    "title": "14  The PDF Reader",
    "section": "",
    "text": "The plan\nThe workflow of this demo will be the following:\nHere is a screenshot of the final result we want to achieve.\nFor this workflow to work, we need to solve two basic problems:\nFirst, let’s tackle those problems at a conceptual level, and then we’ll see how to code the whole thing.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#the-plan",
    "href": "part3/02_pdfbot.html#the-plan",
    "title": "14  The PDF Reader",
    "section": "",
    "text": "The user uploads a PDF file.\nThe file gets indexed in a suitable format.\nThe user makes a query.\nGiven that query, an appropriate subset of the PDF document is extracted.\nThe model is given the context and the user query to respond.\n\n\n\n\n\nhow to index the PDF document such that we can retrieve the relevant fragments for a given query, and\nhow to provide that context in an appropriate prompt for the model.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#indexing-and-retrieval",
    "href": "part3/02_pdfbot.html#indexing-and-retrieval",
    "title": "14  The PDF Reader",
    "section": "Indexing and retrieval",
    "text": "Indexing and retrieval\nThere are many ways to store a large document for retrieval. One of the most common is called a vector store (or vector database). In this approach, we split the document into meaningful chunks of text (say, each paragraph) and compute an embedding for each chunk. Then, the user query is also embedded and compared with each of the chunks’ vectors. The closest ones represent those fragments of text that are more closely related (semantically) with the query–if your embeddings are good, that is.\nThis is the simplest approach, and it works good enough for most applications. However, the embedding of the query might not be the most useful key to that index. This happens, for example, if the query is something very abstract like “what are the main topics in this document”. If the document happens to contain a chunk that mentions “the main topics”, that will work. But oftentimes, these types of general questions need a further processing to summarize and abstract the content of the document.\nAlthough we will not deal with these issues in this chapter, the general workflow we will develop here can be easily adapted to more complex scenarios. We’ll say more about these extensions at the end of the chapter.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#giving-context-to-the-chatbot",
    "href": "part3/02_pdfbot.html#giving-context-to-the-chatbot",
    "title": "14  The PDF Reader",
    "section": "Giving context to the chatbot",
    "text": "Giving context to the chatbot\nOnce we have identified the, say, top 3 chunks of text that most likely contain relevant information to answer a user query, we need to give that context to the chatbot.\nEnter prompt engineering.\nSo far, we have provided the chatbot with the exact user query, but there is nothing that forces us to do that. On the contrary, we can augment that query with any additional information we have that might enhance the conversation. The simplest way to do so is to construct a prompt that contains the user query and extra instructions or information.\nHere is one example:\nThe following is a relevant extract of a PDF document\nfrom which I will ask you a question.\n\n## Extract\n\n... [chunks extracted from the vector store] ...\n\n## Query\n\nGiven the previous extract, answer the following query:\n... [user query] ...\nWe then feed this message as the user prompt, interpolating the right context. From the user perspective, they only asked one simple question, but from the chatbot perspective, the question comes with a big chunk of text from where to extract the answer. That is the magic of prompt engineering.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#implementing-the-vector-store",
    "href": "part3/02_pdfbot.html#implementing-the-vector-store",
    "title": "14  The PDF Reader",
    "section": "Implementing the vector store",
    "text": "Implementing the vector store\nAn efficient and scalable implementation of a vector store must be able to retrieve the closest \\(k\\) vectors to a given input as fast as possible. Professional implementations of this functionality require advanced data structures like kd-trees and many low-level tricks to scale to search in millions of vectors.\nHowever, for illustrative purposes, we’ll build our own, extremely inneficient and vector store. This has the advantage you won’t need to install anything to try this code, and you can always later switch it for a production-ready solution.\nOur vector store is a simple collection of paired texts and numpy vectors.\nclass VectorStore:\n    def __init__(self) -&gt; None:\n        self._data = []\n        self._vectors = []\n\n    def __len__(self):\n        return len(self._data)\n\n    def add(self, texts):\n        self._data.extend(texts)\n        self._vectors.extend(self.embed(texts))\n\n    # ....\nTo compute embeddings, we will resort to another method in the Mistral.ai API, but basically any embedding model will do.\nclass VectorStore:\n    # ...\n\n    def embed(self, texts):\n        try:\n            client = MistralClient(api_key=st.secrets.mistral_api_key)\n            response = client.embeddings(\"mistral-embed\", texts)\n            return [np.asarray(d.embedding) for d in response.data]\n        except Exception as e:\n            if not \"Too many tokens in batch\" in str(e):\n                raise\n\n            left, right = texts[: len(texts) // 2], texts[len(texts) // 2 :]\n            return self.embed(left) + self.embed(right)\n\n    # ...\n\n\n\n\n\n\nNote\n\n\n\nIf you notice something weird in the embed method, is because we are being a bit lazy. The embedding endpoint has a maximum token lenght, and will return an error if we send too large a batch.\nWe want to send the largest batch possible to save time, so to avoid computing precise token counts in the client, we simply try with all the texts, and if that fails, we split them into two subsets and try again, until we find the sweet spot.\nThis has the obvious downside of being slower, but for illustrative purposes, it is good enough.\n\n\nFor searching, we will simply compute the euclidean distance between an input vector and all the database.\nclass VectorStore:\n    # ...\n\n    def search(self, text, k=1):\n        # Get the document embedding\n        v = self.embed([text])[0]\n        # Find the closest K vectors\n        idx_min = sorted(\n            range(len(self._vectors)),\n            key=lambda i: np.linalg.norm(v - self._vectors[i]),\n        )[:k]\n        # Retrieve the corresponding texts\n        return [self._data[i] for i in idx_min]\n\n    # ...\nAnd that’s it. In little more than 30 lines of code we have a barebones vector store that we can use for small documents. Again, it doesn’t make sense to optimize this any further, as there are plenty of production-ready vector databases out there. Just google it.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#managing-the-prompt",
    "href": "part3/02_pdfbot.html#managing-the-prompt",
    "title": "14  The PDF Reader",
    "section": "Managing the prompt",
    "text": "Managing the prompt\nThe final piece of the puzzle is to build that augmented prompt which includes the relevant chunks for our indexed document. The actual prompt template I’m using in this demo is the following:\nUSER_PROMPT = \"\"\"\nThe following is a relevant extract of a PDF document\nfrom which I will ask you a question.\n\n## Extract\n\n{extract}\n\n## Query\n\nGiven the previous extract, answer the following query:\n{input}\n\"\"\"\nThe extract argument will interpolate the extracted chunks, and the input argument will contain the actual user query.\nNow, the first idea that might come to your mind is simply submitting the prompt (with the interpolated content) to our chatbot. And while this works, it has an ungly downside.\nYou see, our Chatbot implementation (taken directly from Chapter 13) does all the conversation management automatically. This means that whatever we give it as a user message, will come back to us in the conversation history. Thus, if we send the full augmented prompt as the user message, our chat history will become polluted with all those injected chunks of text. Ugly!\nTo solve this, we’ll make the Chatbot instance aware of our prompt engineering. We’ll supply a user template at initialization.\nclass Chatbot:\n    def __init__(\n        self,\n        model: str,\n        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n        user_prompt=DEFAULT_USER_PROMPT,\n    ) -&gt; None:\n        self.model = model\n        self.client = MistralClient(api_key=st.secrets.mistral_api_key)\n\n        self.system_prompt = system_prompt\n        self.user_prompt = user_prompt\n\n    # ...\nIn the submit method, we’ll include any optional arguments, which presumable contain whatever content the template expects.\nclass Chatbot:\n    # ...\n\n    def submit(self, content, role=\"user\", context=0, **kwargs):\n        messages = self.history(context)\n        self.store(role, content)\n\n        messages.insert(0,\n            ChatMessage(role=\"system\", content=self.system_prompt)\n        )\n\n        if role== \"user\":\n            content = self.user_prompt.format(input=content, **kwargs)\n\n        messages.append(ChatMessage(role=role, content=content))\n\n        if role == \"user\":\n            return self._stream(messages)\n    # ...\nThis way, the Chatbot class itself will perform the interpolation, but store in the conversation history only the clean user input.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#the-final-application",
    "href": "part3/02_pdfbot.html#the-final-application",
    "title": "14  The PDF Reader",
    "section": "The final application",
    "text": "The final application\nWith all the pieces in place, it’s time to build the final puzzle. Since we have dutifuly encapsulated all the important functionality, our application will be a very straightforward Streamlit app.\nWe begin, as usual, with the preamble.\nimport streamlit as st\nfrom utils import Chatbot, VectorStore\nfrom pypdf import PdfReader\n\nst.set_page_config(page_title=\"PDF Bot\", page_icon=\"🤖\")\nNext, we set up the file upload widget. The app will stop at this point until the user submits a file.\nfp = st.sidebar.file_uploader(\"Upload a PDF file\", \"pdf\")\n\nif not fp:\n    st.warning(\"Please upload your PDF\")\n    st.stop()\nWith a proper PDF file at hand, we can initialize our vector store. We are using the st.cache_data to ensure that as long as the file doesn’t change, we won’t recompute embeddings every single time.\n@st.cache_data(show_spinner=\"Indexing PDF...\")\ndef get_store(pdf):\n    store = VectorStore()\n    texts = [page.extract_text() for page in PdfReader(pdf).pages]\n    store.add(texts)\n    return store\n\n\nstore = get_store(fp)\nst.sidebar.write(f\"Index size: {len(store)} pages.\")\nNext, we intialize our bot, and set up the restart conversation functionality, just like in the previous chapter. Notice that this is the moment we configure the user prompt template.\nUSER_PROMPT = \"\"\"\nThe following is a relevant extract of a PDF document\nfrom which I will ask you a question.\n\n## Extract\n\n{extract}\n\n## Query\n\nGiven the previous extract, answer the following query:\n{input}\n\"\"\"\n\nbot = Chatbot(\"open-mixtral-8x7b\", user_prompt=USER_PROMPT)\n\nif st.sidebar.button(\"Reset conversation\"):\n    bot.reset()\nThe usual drill comes next: recreate the conversation history, and get the user input.\nfor message in bot.history():\n    with st.chat_message(message.role):\n        st.write(message.content)\n\nmsg = st.chat_input()\n\nif not msg:\n    st.stop()\n\nwith st.chat_message(\"user\"):\n    st.write(msg)\nAnd finally, we extract the most relevant chunks and submit our augmented prompt.\nextract = store.search(msg, 3)\n\nwith st.chat_message(\"assistant\"):\n    st.write_stream(bot.submit(\n        msg,\n        context=2,\n        extract=\"\\n\\n\".join(extract)\n    ))\nAnd… we’re done! Our first retrieval-augmented application, in less than 100 lines of code total. Who said chatbots where hard?",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/02_pdfbot.html#conclusions",
    "href": "part3/02_pdfbot.html#conclusions",
    "title": "14  The PDF Reader",
    "section": "Conclusions",
    "text": "Conclusions\nIn this chapter we built our first non-vanilla chatbot application. It may seem like a small step for you, but it is a giant leap for… well, you get the point.\nThe crucial lesson in this chapter is how, with proper context, we can turn a standard chatbot into a powerful question-answering machine with deep expertise in any specific domain. You can use this formula to build smart assistants for your company or organization. You turn this into a research helper by feeding it papers or books (although we will build a proper research assistant in ?sec-research). Or you can feed it with any book you’re currently reading and use it to extract insights and summaries.\nOne key limitation of this simplistic approach to RAG is that, since you’re embedding the user query to find relevant chunks, you might miss the answer to some question altogether if the relevant chunk doesn’t happen to have a similar enough embedding with the input. Many strategies can be used to mitigate this, and you’ll find a few suggestions summarized in ?sec-augmentation. Furthermore, you can combine this approach with any of the advanced prompting techniques in Chapter 6 for maximum coolness.\nThe basic architecture we designed in this chapter will be the basis of much of what’s to come. The complexity will be in finding the right context, and reformulating the user query in a convenient way.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The PDF Reader</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html",
    "href": "part3/03_search.html",
    "title": "15  The Answer Engine",
    "section": "",
    "text": "The plan\nKeeping in line with the previous chapter, I will first explain at a broad level what is the workflow and the necessary functionalities for this app, and then we’ll go into implementation details. Since much of this demo builds on previous functionality, I won’t show you the whole code, only the novel parts. You can always check the full source code distributed with the book.\nAt a high level, this demo looks like a poor man’s search engine. We’ll have a simple search bar where the user can type a query. But actually, this isn’t just a search engine, it’s an answer engine! Instead of listing the matching websites, we’ll take it a step further and crawl those websites, read them, find a relevant snippet that answers the user query, and compute a natural language response with citations and all. Furthermore, we won’t even pass the exact user query to Google, but instead ask the LLM to suggest a small set of relevant queries that are more suitable for search.\nHere is a screenshot of the final result, showing our answering engine talking about a recent news (the launch of GPT-4o) which happened way after it’s training:\nHere is the general workflow:\nThere are several problems we must solve to implement this workflow, and we will go over them one at a time.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#the-plan",
    "href": "part3/03_search.html#the-plan",
    "title": "15  The Answer Engine",
    "section": "",
    "text": "Note\n\n\n\nYou can check a working version of this demo online at https://llm-book-search.streamlit.app.\nThis demo is provided on a best-effort basis and might be taking offline at any moment.\n\n\n\n\n\n\n\nThe user types a question.\nThe LLM provides a set of relevant related queries.\nFor each query, we obtain a set of search results from Google.\nFor each search result, we crawl the webpage (if possible) and store the raw text.\nGiven the original user question and the crawled text, we build a suitable context.\nThe LLM answers the question with the provided context.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#getting-the-relevant-queries",
    "href": "part3/03_search.html#getting-the-relevant-queries",
    "title": "15  The Answer Engine",
    "section": "Getting the relevant queries",
    "text": "Getting the relevant queries\nThe first step is to compute a set of relevant queries related to the user question. Why? Well, first, because the user question might not be the type of query that search engines are good at solving. If you’re using Google, this might be less of an issue. But if you apply this strategy to self-hosted traditional search engines, like ElasticSearch, they instead just use some fancy keyword-based retrieval, with no semantic analysis.\nBut more importantly, there simply might not be a single best document (or webpage) with an answer to the user question. For example, if the user asks about a comparison between two products, but there is no actual comparison in the data. All you have is separate documents describing each product. In this case, the best strategy is to perform separate queries for each product and let the LLM do the comparison, effectively leveraging the best of both paradigms.\nHere is a prompt to achieve this. It’s a simple but effective combination of prompting strategies we saw in Chapter 6. At its core, this is a zero-shot instruction, but we also include a brief chain-of-thought to nudge the model into a more coherent answer structure. Finally, we instruct the model to produce a structured response so we can parse it more easily.\nGiven the following question, provide a set of {num_queries}\nrelevant Google queries that would answer the question.\nFor that, first think about the user query and provide\nyour own interpretation.\nThen generate the relevant queries.\n\nQuestion: {question}\n\nAnswer only with a JSON object containing a list of the relevant queries,\nin the following format:\n\n{{\n    \"interpretation\": \"...\",\n    \"queries\": [ ... ] # list of relevant queries\n}}\nFrom this prompt, any reasonably tuned model will produce something like the following, for the query “what is gpt4-o and how does it compare to gpt4”:\n{\n  \"interpretation\": \"The user is asking about GPT-4 and\n   a potential GPT-4o, and how it compares to GPT-4\n   in terms of functionality, features, or performance.\",\n  \"queries\": [\n    \"GPT-4 vs GPT-4o comparison\",\n    \"What is GPT-4o and its features\",\n    \"GPT-4o performance compared to GPT-4\"\n  ]\n}\nNotice that, from the interpretation, we can see the model “knows” what GPT-4 is but not what is GPT-4o. Yet, it produced pretty sensible search queries, including one individual query for GPT4-o, which is very close to what we would do if we were researching this subject on our own.\nImplementing this structured response prompt in our system takes a bit more effort than just passing the prompt. The reason is we want to force the model as much as possible to produce a well-formed JSON object and, while careful prompting can get us pretty far, it is still possible for the model to deviate from just producing a JSON object, and adding some chatty messsages like “Of course, here is your JSON object:” which would make it harder to parse the response.\nFor this purpose, most OpenAI-compatible APIs implement something called JSON mode which forces the response to be a parseable JSON object. It won’t guarantee that you get the JSON structure you asked for (this is, in general, not solvable without modifying the sampling method) but it will guarantee that, if the model responds at all, it will be a well-formed JSON2.\nTo take advantage of this API feature in our implementation, we will add a JSON mode method to our Chatbot class, which also skips the conversation history workflow, because we usually don’t want these messages to be part of a traditional conversation, but rather use them for one-off instructions.\n# File: utils.py\n\nclass Chatbot:\n    # ...\n\n    def json(self, content, context=0, model=None, retries:int=3):\n        messages = self.history(context)\n\n        # Build the message list, including the\n        # system message\n        messages.insert(0, ChatMessage(\n          role=\"system\",\n          content=self.system_prompt)\n        )\n        messages.append(ChatMessage(role=\"user\", content=content))\n\n        # Get the response\n        response = (\n            self.client.chat(\n                messages,\n                self.model,\n                # Explict JSON-mode response format\n                response_format=ResponseFormat(\n                  type=ResponseFormats.json_object\n                ),\n            )\n            .choices[0]\n            .message.content\n        )\n\n        # ...\nSo far, this is a pretty standard setup, just adding the explicit JSON mode argument. However, this still does not ensure the response is in the right format, as the LLM might decide to change to keys–e.g., adding a ` symbol, or something else equally weird.\nLikewise, even if the result is correct JSON, it might not have the structure you requested. While we cannot entirely guarantee the result, we can still make it easier for the LLM to answer correctly. We’ll do so in two steps. First, we will allow for a small number of retries, in case the first time the response is invalid.\nclass Chatbot:\n    # ...\n\n    def json(self, content, context=0, model=None, retries:int=3):\n        # ...\n\n        try:\n            result = json.loads(response)\n\n            # Attempt to parse the result into a dataclass\n            # to ensure a consistent schema\n            if model:\n                return _parse_dataclass(result, model)\n\n            return result\n        except:\n            # If the response fails to parse\n            # we print the original result for debugging\n            # and try again\n            print(response, flush=True)\n\n            if retries &lt;= 0:\n                raise\n\n            return self.json(content, context, model, retries - 1)\nAnd second, we will allow the user to pass a dataclass type that defines the expected structure. If the JSON object doesn’t exactly matches the dataclass structure (because maybe key names have superfluos characters), we will allow some flexibility in the parsing.\ndef _parse_dataclass(json_obj, model):\n    result = dict()\n\n    for field in fields(model):\n        value = json_obj.get(field.name)\n\n        if not value:\n            for k,v in json_obj.items():\n                if field.name.lower() in k.lower():\n                    value = v\n                    break\n\n        if value:\n            result[field.name] = field.type(value)\n\n    return model(**result)\nThe _parse_dataclass function expects a dict-like object and a type, which must be decorated as a dataclass. During the construction of the dataclass, we will first attempt to find an exact match for the fields of the class, and resort to approximate matching otherwise.\nIn the main application loop, we simply call this method and access the queries key to find the queries. We can define a Response class to ensure a schema.\n@dataclass\nclass Response:\n    interpretation: str\n    queries: list",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#searching-online",
    "href": "part3/03_search.html#searching-online",
    "title": "15  The Answer Engine",
    "section": "Searching online",
    "text": "Searching online\nThis is probably the easiest part of the demo. We can use any number of Google Search wrappers to obtain a list of web pages given a set of queries. In this case, I’m using googlesearch-python, which is one of the libraries with most Github stars, but feel free to experiment.\nThe first step is to combine all the results from the different queries into a single set so that we don’t crawl a web page twice.\nqueries = bot.json(\n    QUERY_PROMPT.format(question=user_query, num_queries=3),\n    model=Response,\n    retries=3,\n)\n\nurls = set()\n\nfor query in queries.queries:\n    search_results = list(\n        googlesearch.search(\n            query,\n            num_results=num_results,\n            advanced=True,\n        )\n    )\n\n    for i, result in enumerate(search_results):\n        urls.add(result.url)\n\n\n\n\n\n\nNote\n\n\n\nThis isn’t the exact code in our application, because we’d have some streamlit-specific instructions in there to print some status messages while crawling.\n\n\nThe next step is to crawl each of these results, skipping the ones that fail (because they are either not HTML content, or take too long to load, etc.). We use BeautifulSoup to parse the HTML and obtain a blob of continuous text extracted from every HTML node that has any text at all. This isn’t the prettiest or most robust way to parse an HTML file, especially if you want to show it to a human, but for our purposes it works pretty well because the LLM will be able to sift through it and extract the relevant parts.\ntexts = []\n\nfor i, url in enumerate(urls):\n    try:\n        html = requests.get(url, timeout=3).text\n\n        if \"&lt;!DOCTYPE html&gt;\" not in html:\n            continue\n\n        text = bs4.BeautifulSoup(html, \"lxml\").get_text()\n        text = text.split()\n\n        if len(text) &lt; 50:\n            continue\n\n        for j in range(0, len(text), chunk_size):\n            chunk = \" \".join(text[j : j + chunk_size])\n            texts.append(chunk)\n    except:\n        pass\nNotice that, at the same time we’re parsing the HTML, we split the resulting text in chunks of, say, 256 words, and store each chunk separately. You can probably already guess why, right?",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#finding-the-right-context",
    "href": "part3/03_search.html#finding-the-right-context",
    "title": "15  The Answer Engine",
    "section": "Finding the right context",
    "text": "Finding the right context\nThe next problem we have to solve is giving the LLM a reasonably short fragment of the relevant web pages where the answer to the user query might be. Depending on how you configure this demo, the search step might have extracted hundreds of chunks with thousands of words in total, many of which might be irrelevant. For example, you may be asking for a specific date of some event and get a whole Wikipedia page where that event is passingly mentioned in one of the paragraphs.\nTo solve this problem we will resort again to the most effective augmentation strategy, retrieval augmented generation, and our old friend the VectorStore class. We will index the extracted chunks on-the-fly and immediately extract the most relevant ones.\nstore = VectorStore()\nstore.add(texts)\n\nchunks = [\n    dict(id=i+1, url=url_by_text[c], text=c)\n    for i, c in enumerate(store.search(query, k=num_chunks))\n]\nThe result of this snippet is a chunks list containing a small number of relevant chunks, formatted as Python dictionaries with the following structure:\n{\n  \"id\": 1,\n  \"url\": \"https://en.wikipedia.org/wiki/GPT-4o\",\n  \"text\": \"[...] is an AI model that [...]\"\n}\nThe id field, computed as an incremental index, will be useful later on for printing explicit references.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#building-the-final-response",
    "href": "part3/03_search.html#building-the-final-response",
    "title": "15  The Answer Engine",
    "section": "Building the final response",
    "text": "Building the final response\nAll that’s left is formatting a proper RAG-enabled prompt and inject the relevant content extracted from in the previous step. Here is the prompt we’re using:\nHere is an extract of relevant context from different web pages:\n\n---\n{chunks}\n---\n\nGiven the provided search context, please answer the following user query.\nFor every substantive claim extracted from the context, please\nprovide between square brackets the corresponding chunk ID number,\nsuch as [1], [2], etc.\n\nQuery: {input}\nNotice how we explicitly instruct the model to produce square-bracketed references whenever possible. The chunks we inject in the context are pretty printed JSON objects from the previous section that contain a convenient id field.\nAnd that’s it, all that remains is a few streamlit-specific bells and whistles here and there to get this demo up and running. For example, we add a few numeric inputs to let the user play around with the parameters of the search (how many queries to perform, how many chunks to extract, etc.):\nnum_queries = st.sidebar.number_input(\n  \"Number of different queries\", 1, 10, 3)\nnum_results = st.sidebar.number_input\n  (\"Top K results to crawl\", 1, 10, 3)\nnum_chunks = st.sidebar.number_input(\n  \"Number of context chunks\", 1, 10, 3)\nchunk_size = st.sidebar.number_input(\n  \"Chunk size (~tokens)\", 128, 2048, 256)\nAnd, if you check the full source code, we also have a few instructions here and there to make this semi-fancy layout in two columns with a search bar at the top.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#conclusions",
    "href": "part3/03_search.html#conclusions",
    "title": "15  The Answer Engine",
    "section": "Conclusions",
    "text": "Conclusions\nRetrieval augmented generation is an extremely versatile paradigm. In the previous chapter we saw the standard approach, using just vector-based retrieval from a static text source. In this chapter, we use a generic search engine as the data source, taking advantage of the massive complexities hidden behind what looks like a simple Google search. We’re leveraing years and years of innovation in collection, storage, and lightning fast retrieval from billions of web pages. We’re standing on the shoulders of giants, quite literally.\nThe structured response prompt pattern is crucial in this context, and we will continue to rely on this functionality the more tight we want to integrate and LLM with other tools. Thus, building this almost-certain flexible parsing strategy will pay off in future chapters. Keep in mind, thought, there is not way to absolutely guarantee (without changing the sampling process) that the output of an LLM matches any specific format.\nYou can adapt this demo to any search engine that provides a text search interface. A common use case is getting all your institutional knowledge in a self-hosted search-enabled database like ElasticSearch, and use it to build an in-house Q&A bot.\nBy now, you should be starting to see a pattern in this integration of large language models with other traditional technologies. We will use the model to both transform the user input into something structured that we can feed our underlying tool, compute some results, and then use the model again to produce a natural language response. This is why we talk about LLMs as a natural language user interface. It’s basically a wrapper for any traditional computational tool that adds a very convenient conversational input and output, but the basic computation is still performed by the tool.\nThis combination of LLMs with traditional tools helps bridging the gap between the incredible versatility of language models and the efficiency and robustness of more traditional tools, while minimizing (althought not entirely eliminating) many of the inherent limitations of LLMs like hallucinations and biases.\nIn future chapters, we will stretch this paradigm to its limits, making our LLM interact with all sorts of APIs and even produce its own executable code.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/03_search.html#footnotes",
    "href": "part3/03_search.html#footnotes",
    "title": "15  The Answer Engine",
    "section": "",
    "text": "To be fair, Google has had synthetic answers for a while, and more recently, almost all search engines are incorporating explicit answer generation using… can you guess what? Yes, language models!↩︎\nThe way this works is pretty straightforward and naive. The response given by the model is simply passed through a JSON parser and all tokens that would break the parser are skipped. This means you still have to instruct the model explicitly to output JSON or you could get an empty response.↩︎",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The Answer Engine</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html",
    "href": "part3/04_shoping.html",
    "title": "16  The Salesbot",
    "section": "",
    "text": "The plan\nOur goal for this chapter is to build a shoping assistant for an arbitrary (randomly generated) online store. Our bot should support three basic functions:\nHere is a screenshot:\nA typical interaction between a user and our bot can go as follows:\nTo implement this functionality we need to solve the following problems:\nLet’s tackle them one at a time.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#the-plan",
    "href": "part3/04_shoping.html#the-plan",
    "title": "16  The Salesbot",
    "section": "",
    "text": "Search for and suggest products.\nAdd products to a shoping cart.\nGive information on the current shoping cart.\nClear the shoping cart.\n\n\n\n\n\nThe user asks for information about some products or needs.\nThe chatbot invokes an API function to search for relevant products related to the user query.\nThe search API provides a list of relevant products.\nThe chatbot replies back to the user with an elaborated response using the retrieved information.\nThe user asks for some of these products to be added to the cart.\nThe bot identifies the right product and quantity and invokes the API function to modify the cart.\nAnd so on…\n\n\n\nHow to determine when to call an API function, which function, and with which parameters.\nHow to allow the chatbot invoke the corresponding function, and inject the results of the function call back in to the chatbot conversation.\nHow to implement the specific functions needed in this example.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#function-calling-from-scratch",
    "href": "part3/04_shoping.html#function-calling-from-scratch",
    "title": "16  The Salesbot",
    "section": "Function calling from scratch",
    "text": "Function calling from scratch\nAs we saw in Chapter 8, the traditional approach to function calling involves building a detailed prompt that includes definitions for the available functions, their arguments, and suitable descriptions that help the LLM determine the best function for a given context. Since this process is repetitive and error-prone, most LLM provides have a dedicated API for function calling, enabling developers to submit a structured schema for the function definitions.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#encapsulating-function-calling",
    "href": "part3/04_shoping.html#encapsulating-function-calling",
    "title": "16  The Salesbot",
    "section": "Encapsulating function calling",
    "text": "Encapsulating function calling",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#building-a-shoping-bot",
    "href": "part3/04_shoping.html#building-a-shoping-bot",
    "title": "16  The Salesbot",
    "section": "Building a shoping bot",
    "text": "Building a shoping bot",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#bonus-generating-random-stores-and-products",
    "href": "part3/04_shoping.html#bonus-generating-random-stores-and-products",
    "title": "16  The Salesbot",
    "section": "Bonus: Generating random stores and products",
    "text": "Bonus: Generating random stores and products",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "part3/04_shoping.html#conclusions",
    "href": "part3/04_shoping.html#conclusions",
    "title": "16  The Salesbot",
    "section": "Conclusions",
    "text": "Conclusions",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Salesbot</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "The Future of Language Modeling",
    "section": "",
    "text": "As for the future of LLMs, there are several trends we can anticipate. First, we’ll keep scaling up our models as there’s still untapped potential in our compute and data. Building bigger models and training them on more data for longer periods will yield still better results. However, there will come a point when scaling models ceases to be effective due to diminishing returns or running out of natural linguistic data.\nTraining on synthetic data alone poses challenges, as it can lead to odd and incoherent model behavior. While there are ways to mitigate this, fresh human data is ideal. As we continue scaling models, we’ll need better training algorithms that require less data but maintain efficiency.\nAnother trend is the separation between large foundational models trained on vast amounts of data and smaller, more specialized models. These smaller models can be easily fine-tuned for specific tasks, offering performance similar to GPT-4 with a fraction of the size when adjusted with the right data and algorithm.\nLastly, we’ll see increased integration between models and other computational methods. Combining GPT with systems like Python code or Wolfram Alpha demonstrates the potential of these hybrid approaches.\nFormal computational languages enable precise, non-statistical calculations, providing formal guarantees of correctness. A significant trend will be using language models as interfaces for these systems, such as in code generation. Language models will be integrated into various applications wherever it makes sense, and often where it won’t too.\nHowever, traditional user interfaces will still play a role in tasks requiring fine control, like adjusting the volume of an audio track or the contrast of an image. Replacing such controls with language isn’t practical, you cannot just say “increase the volume by 0,76 db”, it’s too cumbersome. Yet, for applications with discrete interfaces involving buttons and checkboxes, we’ll likely see an increasing shift towards a linguistic layer that translates user input into commands.\nThis is linked to the improvement of language models for code generation. As these models become better at understanding language and translating it into programming code, integration into diverse applications becomes easier.\nAnother development is multi-modality, where text, images, and sounds are combined for simultaneous reasoning. By embedding various qualities into a shared space, language can be grounded in multiple types of experiences. This allows for better context understanding through not only words but also visuals and audio inputs.\nIn summary, we can easily predict a couple of decades of continued improvement in LLMs and an increased level of integration of this technology into almost everything from coffee machines to rockets. However, we still don’t know where the next big wall is, and what we will need to overcome it. There are many who believe LLMs can scale up to AGI, but others—including myself—are skeptics, and we think there are still fundamental discoveries to be made in Artificial Intelligence.\nAnd that’s the fun part, right?",
    "crumbs": [
      "The Future of Language Modeling"
    ]
  },
  {
    "objectID": "part1/index.html",
    "href": "part1/index.html",
    "title": "Principles",
    "section": "",
    "text": "In this part we will dive into the theory of large language models, and understand how they work, what are their capabilities, and some of their current and inherent limitations.",
    "crumbs": [
      "Principles"
    ]
  },
  {
    "objectID": "part2/index.html",
    "href": "part2/index.html",
    "title": "Techniques",
    "section": "",
    "text": "In this part we will explore the different techniques you can use to integrate LLMs into general applications, and unlock their most impressive capabilities. We will begin with an introducion to prompt engineering in 6  Prompt engineering. Here, we will learn how to format our request (also called “prompt”) to the LLM to maximize its utility.\nThen, we will explore augmentation techniques, which allow extending the capabilities of an LLM in several ways without retraining the models. We can extend the breadth of their knowledge by providing relevant context taken from external knowledge sources, or allowing them to directly query specialized search engines or APIs. Second, we can extend their reasoning and problem-solving capabilities by integrating them with other tools that perform specific tasks, or letting them generate and run specialized code. Finally, we can give LLMs a full agentic behavior and let them decide how and when to interact with an environment, receive information, and take actions.\nFinally, we will look at fine-tunning: the process to modify an LLM’s weights to incorporate new knowledge or develop new skills. We will understand when fine-tuning is useful or necessary, and how to select the appropriate fine-tuning method according to our budget and the complexity of the task we tackle.\n\n\n\n\n\n\nNote\n\n\n\nThis part is under construction.",
    "crumbs": [
      "Techniques"
    ]
  },
  {
    "objectID": "part3/index.html",
    "href": "part3/index.html",
    "title": "Applications",
    "section": "",
    "text": "In this part we will build several applications integrating LLMs in different roles, from frontend chatbots that interact with the user to hidden in the backend. These chapters are best read in order, although they are mostly self-contained, and you can skip some of them if you aren’t interested in a particular type of application. In any case, whenever a chapter builds on top of a previous application, we will clearly state it in the corresponding introductory section.\n\n\n\n\n\n\nNote\n\n\n\nThis part is under construction.\n\n\nIn Chapter 13  The Chatbot we build our first LLM-powered application: a basic chatbot. We will learn how to set up a conversation loop and how to keep track of previous messages to simulate short-term memory for the bot. We will also learn how to stream messages for a better user experience (simulating a typing animation instead of waiting for the full response).\nThis one is a must-read chapter, because it will lay the groundwork for the rest of the applications. It is also the only chapter where we’ll build the application step by step, showing how to go from zero to a functioning bot. In the rest of the chapters, we’ll start with a working application and digest it to understand how it works, but we will move much faster to cover a wider range of functionalities.\nNext up, in Chapter 14  The PDF Reader we tackle our first augmented chatbot: a PDF question-answering system. We will build our own version of a vector store, and learn how to convert a large document into indexable chunks that can be retrieved at query time and injected into the bot prompt.\nLeveling up a bit, in Chapter 15  The Answer Engine we build a search-powered chatbot that can browse the web and provide relevant answers with proper citations.\nThen, in ?sec-shoping we will build a more advanced version of retrieval augmentation. This time is a shoping helper than can search items on behalf of the user, add them to the checkout cart, buy them, and track their delivery status—all based on a simulated online store. We’ll learn how to teach an LLM to call methods from an arbitrary API and write our own plugin system.\nIn Chapter 17  The Data Analyst we start playing with chatbots that can execute code on their own. We’ll build a very simple data analyst that can answer questions from a CSV file by running Pandas queries and generating charts. We’ll learn how to decide whether a textual response or a code/chart is more appropriate and how to generate structured responses.\nNext, in Chapter 18  The Hackerbot we’ll take a quick detour from our standard UI to build a command-line bot that has access to your terminal and can run commands on it, so you’ll never have to google how to unzip a tar.gz file again.\nJumping from code execution to more traditional text generation, in Chapter 19  The Writer we’ll code a writing assistant that will help us create an article or text document in general. We’ll learn prompt techniques to summarize, outline, proofread, and edit textual content. We will also learn how to use an LLM to interactively modify a document, adding, modifying, or deleting arbitrary sections.\nAnd then, building on top of that same workflow, in Chapter 20  The Coder we build a coding assistant, who can interact with a codebase adding, modifying, refactoring, and deleting code and files.\n\nAnd that’s it! At least, that’s the plan for now.8 applications of LLMs to the most varied problems, many of which could become the Product Hunt product of the day if you take them to completion.",
    "crumbs": [
      "Applications"
    ]
  }
]