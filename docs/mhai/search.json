[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mostly Harmless AI",
    "section": "",
    "text": "Preface\nWelcome to Mostly Harmless AI! This short book features 10 essays I wrote during 2023 and 2024 subscribers of my blog, Mostly Harmless Ideas. Each essay relates to AI, with some focusing on technical aspects and others delving into philosophical or societal issues. All of them reflect my deep concern for humanity’s future and my hope that we can build a better world by using AI responsibly.\nThese essays appear in this book almost as originally published on the blog, with only minor edits to fix typos and discuss other small issues. Together, these essays offer insights into the potential impact of AI on our lives and our world.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Mostly Harmless AI",
    "section": "About the Author",
    "text": "About the Author\n… todo …",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-find-in-this-book",
    "href": "index.html#what-you-will-find-in-this-book",
    "title": "Mostly Harmless AI",
    "section": "What you will find in this book",
    "text": "What you will find in this book\n… todo …",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-you-should-read-this-book",
    "href": "index.html#why-you-should-read-this-book",
    "title": "Mostly Harmless AI",
    "section": "Why you should read this book",
    "text": "Why you should read this book\n… todo …",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-keep-in-touch",
    "href": "index.html#how-to-keep-in-touch",
    "title": "Mostly Harmless AI",
    "section": "How to keep in touch",
    "text": "How to keep in touch\nIf you’d like to stay updated on these discussions, subscribe to my blog, Mostly Harmless Ideas, and receive new posts directly in your inbox. I look forward to having you join the conversation!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "foreword.html",
    "href": "foreword.html",
    "title": "Beyond Optimism and Doom: Finding a Third Path in AI Discourse",
    "section": "",
    "text": "A Foreword to Mostly Harmless AI by Nicolas Potkalitsky, Ph.D.\n\nWe are truly living through a moment of incredible change in the sciences and technology. As an educator, scholar of narrative and literature, and parent of two young children, I find myself uniquely positioned to witness these transformations, particularly in how they reshape our understanding of learning and knowledge creation.\nI wake up each day with an incredible sense of gratitude. The human capacity to adapt to AI disruption is evident in my work every day with students, colleagues, administrators, and researchers. Major challenges, such as the AI efficiency-accountability paradox where we struggle to balance the speed of AI tools against our need for verifiable student learning, can become real learning opportunities in the hands of skillful instructional designers and experienced classroom teachers. And yet, this gratitude mingles with both anxiety and anticipation – anxiety about the potential loss of fundamental skills in the face of new systems, and anticipation knowing that today’s systems, around which we plan curriculum and interventions, will transform before we fully grasp them.\nWhen ChatGPT 3.5 dropped in November, 2022, most educators had only an incipient understanding of artificial intelligence and its potential impact on pedagogy, instruction, and assessment. Here I number myself amongst this group. We knew that artificial intelligence was operative in our phones’ mapping systems and voice messaging tools. We had seen it at work in the previous decade’s rise of individualized test preparation and assessment, particularly in large initiatives surrounding data-driven decision making. We witnessed AI’s advancement across grammar and spell checking software as Grammarly became more and more adept at transforming student work. But when GPT 3.5 ripped out its first cogent essay, we knew something fundamental had changed. I personally felt devastated. I just knew I couldn’t continue to do things the way I was, and to be honest, I didn’t really know if I wanted to change everything at that moment.\nAfter working through my initial denial and anger, I got to work, orienting myself to AI disruption as if it were a graduate-level research project. I spent the entirety of the summer of 2023 reading, reflecting, and writing, composing a school-response plan that helped guide its first steps out of AI denialism and into AI proactivism. It was at this point that I decided to start up a Substack, Educating AI, to chronicle my investigations and transformations in my own classroom and school. I knew little about Substack, except that my division director Dr. Blair Munhofen read several of them on obscure topics, loved the intimacy and intellectual depth of these inquiries, and highly recommended I start one up whenever the spirit moved me—knowing of my own love for obscure topics like object-oriented ontology, Augustinian volitional theory, modal logic, 21st-century ethical realism, ancient epigraphy, and more.\nBeginning a weekly practice of writing has been life-transforming, not just because of the way it has pushed me to engage more deeply and reflectively with our schools’ response to AI disruption, but because it has affirmed to me—in a way that I did not even experience when completing my dissertation—that we write (and by extension, think) best in communities. Within several weeks of being on Substack, I came in contact with several amazing writers including Nat of The AI Observer, Michael Woudenberg of Polymathic Being, and Alejandro Piad Morffis of Mostly Harmless Ideas.\nAt the time, Morffis was running a digest of Tech Writers and invited me to join. This invitation led me to learning more about him: a beloved computer science instructor in Cuba, a skillful applications and tools designer, a highly gifted writer on technical topics, and a connector of people towards common goals. While at time the two predominant modes of discourse surrounding AI were self-described as “techno-optimist” and “doomerist,” Morffis and the small but influential community that gravitated around him were seeking out a third path—one rooted in a deep understanding of both human cognition and the developing nature of AI and LLMs.\nIn many ways, I have Morffis and other guides like Daniel Bashir and his wonderful podcast The Gradient to thank for transforming Educating AI from a newsletter of to-do lists and tool endorsements to something much richer and deeper. As a perennial student of the nature of thought, I came to recognize when surrounded by similarly oriented scholars in the machine-learning community that every technological disruption—every advancement in tool-being—every expansion of the tool-oriented social-actor network—sparks an opportunity to reassess what thinking is. But such work needs a theoretical container. Ideally, the container would emerge from the substance of inquiry. And yet, history always plays such a powerful role in our analytical process.\nAnd so, Morffis and I, inspired greatly by the writings of Rob Nelson, who in a piece, “On Techno-pragmatism” redeploys William James’s pragmatism in the AI-infused contemporary, dusted off the aged mantle of pragmatism and used it to frame our shared and individual movements forward. Morffis wrote a beautiful piece, not included in this collection, but available on his Substack, called “The Techno-Optimist’s Manifesto” that captures the feeling of “our” moment at the end of 2023, as LLM development appeared to be slowing down and as techno-optimist and doomerist AI discourse was gradually becoming replaced by something more akin to AI frustration and resignation.\nThe core of pragmatism in Morffis’s work hinges on his commitment to human agency to find solutions to problems within limits prescribed by larger (formal or material) systems and processes. Here note how he shapes one of his tenets in the Manifesto: “Techno-pragmatism is accepting that the future is not predetermined. That we have the power to decide among many potential futures and the responsibility to make that choice based on reason and evidence, respecting the plurality of interests of all our fellow humans and being thoughtful about our planet and future generations.” The word “decision” is one of great impact for Morffis. Decisions of the kind described primarily operate in open-ended contexts and thus rely on the highest level of reason to be enacted skillfully and toward the benefit of others.\nIn the following, I will offer a short overview of “Mostly Harmless AI: Essays on Artificial Intelligence and its Impact in our Society.” When sending me the manuscript, Morffis described it simply as “a short collection of essays from my blog” implying something sporadic or serial in nature. But when read in the current sequence, as I recommend readers do, something of a deeper logic emerges. Each essay prepares the way for the conversation in the next—laying down essential terminology and concepts—that later are developed, redeployed, and expanded.\nIn the manuscript’s early essays, Morffis provides his own novel and pointed definitions of artificial intelligence rooted in a concise unfolding of the history of not only this technology but the human preoccupation with “cognitive automation.” His writing style is a hybrid between extremely personable, comprehensible real-world examples and the rigor of a mathematical logician. Indeed, Morffis’s bent toward logical precision is one of his greatest assets as he engages with foundational questions about the possibility of artificial general intelligence and automated reasoning–the most insistent conceptual thematics of the manuscript.\nIn the manuscript’s first essay, “The Road to AGI,” Morffis argues that current AI is capable of out-of-training generalization, out-of-distribution generalization, but not out-of-domain generalization. Importantly, these modes of generalization can be mapped somewhat stably onto Margaret Boden’s three types of creativity—combinatorial, exploratory, and transformational—the third type which many, including Boden herself, regard as beyond the capacity of machines. The rest of the manuscript in part is an effort to show along primarily logical and computational lines why “out of distribution generalization” will remain a remote possibility (remember, he is a pragmatist) for AI systems in the near future.\nTo understand Morffis’s argument, you have to understand something about the nature of formal systems. Luckily in Morffis, we have an expert guide who is also a skillful communicator. Setting the question of LLMs aside, Morffis advises that “if any computational model can reason–to the full extent of the meaning of this word in the context of artificial intelligence–, it must be able to perform Turing-complete computations, for if a model is not Turing-complete, that means there are decibel problems it cannot solve.” Then Morffis adds the pragmatic and practical kicker or follow-up: “One key aspect of Turing completeness is that it requires potentially unbounded computation.”\nAs a builder of machines, Morffis then shifts into solutions-mode: How do we get around this limit? “Now, by design, GPT-4, or any pure language model, cannot think forever….There is a way out, however. GPT-4 can generate the necessary code to answer [semi-decidable] question[s] and run [them]. And that is precisely what the newest iteration of ChatGPT does with Code interpreter.” But Morffis adds that this technology is still very much in rudimentary form and may only result in a kind of surface-level Turing completeness.\nThe incredible work that Morffis does in passages like these is to shift the conversation about artificial general intelligence away from value-laden, culturally-bound idealization of what it means to be human to something more clear and precise, and yet extraordinarily beautiful in its simplicity and exactitude. As Morffis adds, “human beings are also bound by this formal limitation”–meaning the limitation of the nature of formal systems–and yet by a variety of different metrics are regarded as capable of reasoning even when confronted with numerous semi-decidable problems simultaneously. Or do humans just appear to reason in these cases? This becomes a deeper preoccupation of the manuscript in its later chapter, “Large Language Models Cannot Reason,” which readers should know sparked off an internet firestorm when first published on Substack after the release of OpenAI’s ChatGPT o1 models.\nSo what does Morffis mean by reason? Here again, we encounter his characteristic clarity and precision. For him: “When we AI folks claim LLMs cannot reason, we are not talking about any abstract philosophical sense of the word”reason”, nor any of the many psychological and sociological nuances it may entail. No, we have a very specific, quantifiable, simplified notion of reasoning that comes straight out of math. Reasoning is simply put, the capacity to draw logically sound conclusions from a given premise.”\nThe fundamental grounds for Morffis’s argument that “LLMs cannot reason” lies in their constituent nature: “their stochastic nature.” Morffis is not among the camp that perhaps includes Turing himself that the mere performance of reasonable steps, or as in the case of advanced LLMs like OpenAI’s most recent ChatGPT o1 and o3, the detailed description of chain of thought processes including detailed explanations for each step in the chain of thought, constitutes reasoning in the above sense. Let’s see how he proceeds.\nFirst, Morffis further extrapolates on the nature of LLMs: “These models generate outputs based on probabilistic predictions rather than deterministic logical rules.” Here, the terms “deterministic” and “logical” do a lot of work. Even as we improve on accuracy through RHLF, we cannot completely limit the occurrence of spontaneous incorrect conclusions: “An LLM might arrive at a wrong purely by chance, learning of inconsistencies in reasoning.”\nSecond, Morffis doubles down on his early thoughts about practical limits regarding computational time. “Large language models spend a fixed amount of computation per token processed.” Here we might recall that inside formal systems there are problems where it is difficult if not impossible in advance to figure out compute time: “Crucially, we can always find instances of NP-complete problems that require, even in principles, a sufficiently large of amount of computation to surpass the computational capacity of any LLM, no matter how big.”\nHere, the work is as impressive as it is rigorous. For some, we may be drifting too far away from what we think of as everyday reasoning. Do we really mean the grappling with NP-complete problems when we ask questions like “Do LLMs reason?” The ARC Prize formulated by François Challot seems like a step down in complexity in comparison to Morffis’s framework, but in the slippage of current AI discourse around reasoning, the risks are great if we simply concede that probabilistic processes or what Morffis calls “very large finite automata” operate at a level of efficiency, accuracy, and logical precision—absolute equivalence—with their makers. Not only is the nature of language itself at stake, but so much that we hold dear as humans.\nIn the remaining chapters of Morffis’s book, he explores offshoots from these foundational questions and conclusions, focusing on the all-too-human situations and conundrums that result. If an LLM can code, should we learn to code too? Yes, Morffis answers emphatically. Are Chatbots a conduit or obstacle to critical thinking? How will AI change the classroom? Should we be worried about AI’s taking over the world? What would it mean for an AI to share human values? What should the future of AI industry and development look like? In each case, Morffis proceeds with a careful, nuanced, and pragmatic eye to the multi-dimensional nature of each of these questions.\nWhat makes this collection particularly valuable is how it bridges the gap between technical rigor and humanitarian concerns. As an educator who began this journey from a place of trepidation, I find Morffis’s measured approach both reassuring and intellectually invigorating. He demonstrates that we can embrace technological advancement while maintaining a clear-eyed view of its limitations. In doing so, he provides a framework not just for understanding AI, but for teaching and learning in an AI-transformed world. This is not just a collection of essays; it is a roadmap for educators, technologists, and thinkers who seek to navigate the complex intersection of artificial and human intelligence with both wisdom and hope.",
    "crumbs": [
      "Beyond Optimism and Doom: Finding a Third Path in AI Discourse"
    ]
  },
  {
    "objectID": "prologue.html",
    "href": "prologue.html",
    "title": "The Age of Artificial Intelligence",
    "section": "",
    "text": "The road so far\nLet’s start by looking back at the last few years in generative AI, from the inception of the transformer architecture to the release of the first widely useful AI application.\nWhen discussing generative AI, we can address different domains: language/text, images, and audio/music. Arguably, the most impactful advancements are in the text domain due to the emergence of large language models. The other domains have also seen incredible progress with the development of, e.g., diffusion models allowing for controllable text-to-image generation and realistic text-to-speech and speech-to-text conversion.\nThus, I want to specifically focus on language, first because it’s my area of expertise, and second, because the majority of economically valuable jobs projected to be automated in the near future will be due to the existence of language models. While the other domains certainly have exciting applications in art, marketing, video games, and content creation, the potential reach of very powerful language models with real language understanding surpasses that of generative sound, music, and image combined.\nThe reason is simple. Language is the primary means of human communication, interaction with the social environment, collaboration, and task-solving. Natural language is frequently used to describe tasks and their solutions. Therefore, solving natural language understanding and developing a machine capable of comprehending instructions, following them, and communicating back in a natural language could potentially be the most transformative technology we could ever invent. We’re still far from full natural language understanding, though. But, at the same time, we’re so much father ahead than anyone seriously involved in AI in the late 2010’s could predict.\nLet’s turn our attention to language modeling, then. At the moment of writing, there is no doubt GPT-4 is the state of the art, at least in terms of being the most powerful, widely available general-purpose language model. That might not be the case in a couple of months when Gemini is finally out, but still, looking over at the story of the GPT series seems like the best way to understand how we got here and where we might go next.",
    "crumbs": [
      "The Age of Artificial Intelligence"
    ]
  },
  {
    "objectID": "prologue.html#the-road-so-far",
    "href": "prologue.html#the-road-so-far",
    "title": "The Age of Artificial Intelligence",
    "section": "",
    "text": "How to train your chatbot\nThe history of chatbots goes all the way back to ELIZA in 1962, but their most recent incarnation started, at least in the public mind, with the advent of GPT-2 in 2019 (Radford et al. 2019). Its ability to fabricate stories about talking unicorns found in the Peruvian mountains, a significant leap from anything we’d seen before, blew our minds. Sure, we had made strides in translation and text classification with bare transformers, but nothing came close to GPT-2’s coherence in generating general-purpose text. Suddenly, an AI model could write a short story!\nThis was also the first time researchers decided not to immediately release the weights of the model, apparently not for any commercial reasons but for fear of it being misused to generate fake news and impersonate people online. Oh, the irony!\nNext year, GPT-3 shook things up with its paper on large language models as few-shot learners, marking the first significant breakthrough in transfer learning. Transfer learning is a pivotal area in AI, exploring how quickly we can adapt to new tasks similar to ones we’ve already learned without starting from scratch. This was the focus of the GPT-3 paper, and the first time we had a hint that such a thing as general-purpose language models could exist.\nThe GPT-3 paper reveals something many linguists already believed: language understanding is a general-purpose task. Furthermore, text completion is also a very general-purpose task. If you excel at completing sentences across various domains, you also excel at a wide range of tasks that aren’t trivially just text completion, as many tasks can essentially be boiled down to “please complete the following text.” Summarizing text, translating to another language, or solving math problems are all examples of tasks easily framed as text completion.\nHowever, there’s a catch. While models like GPT-3 excel at predicting and completing text, they struggle to follow specific instructions. This happens because the training data doesn’t resemble instructions. So, even though the model may know how to summarize or translate, you have to figure out how these tasks are presented in the training data to prompt the model effectively. For example, to summarize, you would give the model your text and append “TL-DR;.”\n\n\nLearning to follow instructions\nWhat if you want to give clear instructions to the model that allow for some flexibility in how you provide them? This is crucial because multiple ways exist to fill in a given text. For instance, when you ask GPT-3 a question, the text can be completed by providing an answer. Still, it is also valid to continue with similar questions, as vast collections of questions are available on the internet. The model doesn’t know whether you want more questions or the answer to the previous question.\nThat’s where instruction fine-tuning comes in, as described in the InstructGPT paper. This approach enables you to train your model to better understand and respond to your instructions, whether asking it to solve an equation, summarize a text, generate ideas, or perform other tasks.\nWith instruction fine-tuning, the model can be guided to prioritize answering questions over simply executing completion. This approach functions by adjusting the likelihood of responses to questions. For this method to be effective, the model must possess a probable and correct answer to the question; we are just nudging it to value that answer more likely than other non-answer completions.\nHowever, this is still not enough. For widespread use, you need a model willing to reject problematic instructions, friendly, and committed to providing factual and accurate information. The desired responses should align with user preferences and maintain a respectful and non-discriminatory tone, using generally informal yet polite language. In other words, you need a model that is knowledgeable (pretrained in the whole internet), helpful (can follow instructions), and nice. This is where reinforcement learning with human feedback (RLHF) comes into play, also presented in this paper.\n\n\nDon’t be evil… please?\nIn RLHF, we take an instruction fine-tuned model and further adjust it to score better answers that align with user preferences —such as being more or less concise, using more or less formal language, or choosing to answer specific topics. This cannot be accomplished with supervised training because the range of possible human preferences is extensive and challenging to model in a dataset.\nResearchers tackled this problem by turning to reinforcement learning. In this approach, we allow the model to provide multiple responses to a prompt rather than showing the model examples of good answers. A human evaluator then ranks and scores these responses based on their preferences for factuality, relevance, length and depth, language style, non-discrimination, etc. This ranking process encodes the human values we desire in a model but cannot program explicitly.\nRLHF thus involves teaching the language model to rank its answers based on human preferences. The ranking model is then used to further train the language model, resulting in answers more aligned with user preferences.\nFrom an engineering standpoint, this final puzzle piece transformed language models into usable applications. Just like, we had all the components leading to the iPhone, but the genius move was to put them together in a commercially viable and user-friendly format. ChatGPT is the iPhone of the AI revolution because it embodies several innovations in a commercially plausible manner, making it accessible and valuable to all users, not just experts or researchers.\nAnd that’s the very brief story of how we got to ChatGPT. But that’s only the beginning. In the last couple of years since ChatGPT was released, a new industry has grown around large language models. Let’s review some of the most interesting trends that this the Age of Artificial Intelligence.",
    "crumbs": [
      "The Age of Artificial Intelligence"
    ]
  },
  {
    "objectID": "prologue.html#the-rise-of-ai",
    "href": "prologue.html#the-rise-of-ai",
    "title": "The Age of Artificial Intelligence",
    "section": "The Rise of AI",
    "text": "The Rise of AI\nFollowing ChatGPT, models have significantly diversified, with big players like Google, Meta, Apple, Twitter/X, Amazon, and many more training their large models. This competition has driven innovation: AI is now where the Big Tech giants fight each other.\nVarious open-source models have also emerged, stemming from Facebook Llama 1 and spinning off into a full Llamaverse. This has led to developing more specialized models, fine-tuned for specific domains and purposes, such as code generation, question-answering from databases, and more.\nThe open-source and closed-source models offer a wide range of trade-offs, and projects and wrappers are in place to seamlessly switch between these models, allowing for greater transparency and flexibility. This mirrors the trend in open source of creating diverse infrastructure and tools, as well as the emergence of frameworks that support various technologies. Just as we now have numerous options for databases, programming languages, and web frameworks, the field of language models is experiencing a similar surge in diversity and innovation.\nThe second major trend is multimodality. Although we focus on language models here, we must recognize the incredible development in visual and audio models. These models can recognize and analyze images, generate images and videos, and create audio outputs based on text prompts. Combining these separate models into multidomain models can solve problems requiring multiple modalities simultaneously. For instance, giving it a picture, asking questions about the image, modifying it, and generating new images based on it.\nA third major trend is the rise of language models as application platforms. This means interfacing LLMs with plugins and applications and connecting with external code. ChatGPT introduced the idea of plugins that let you interact with external services such as knowledge bases, search engines, or APIs for specific products like a store website or booking service.\nThis concept has since evolved into language chains, where you can connect a linguistic interface with non-linguistic tools or services. This involves using a large language model to link with databases, retrieve relevant information, call APIs, perform actions, and compute results. With this setup, you can build applications, known as agents, that are language-driven and capable of interacting with the Internet to carry out tasks on your behalf. These agents go beyond just processing language; they can manage finances, build websites, and more, functioning closer to real assistants.\nA final trend I want to highlight is the emergence of a new field: prompt engineering. It started as an informal practice due to the need for writing effective prompts, but has since evolved into a semi-formal discipline. Since these models aren’t perfect and don’t understand everything perfectly yet, the way you ask for information can affect the results you get. As a result, folks are working on figuring out best practices for crafting good prompts as these models continue to evolve.\nSome experts are skeptical about the long-term future of prompt engineering, believing that as models get better at understanding, it won’t matter how you prompt them. Others think prompt engineering will always be necessary, in the same sense that design patterns, naming and style conventions, and other programming practices remain relevant regardless of how powerful programming languages get.",
    "crumbs": [
      "The Age of Artificial Intelligence"
    ]
  },
  {
    "objectID": "prologue.html#further-reading",
    "href": "prologue.html#further-reading",
    "title": "The Age of Artificial Intelligence",
    "section": "Further reading",
    "text": "Further reading\n\n\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. “Language Models Are Unsupervised\nMultitask Learners.” OpenAI Blog 1 (8): 9.",
    "crumbs": [
      "The Age of Artificial Intelligence"
    ]
  },
  {
    "objectID": "coding-is-dead.html",
    "href": "coding-is-dead.html",
    "title": "1  Coding is Dead; Long Live Coding!",
    "section": "",
    "text": "Building an LLM for code generation\nCode generators are essentially large language models, like GPT, trained on vast amounts of code. For that reason alone, we should code generators to learn to generate code based on the previous context, i.e., the variables, methods, classes, and generally the structure of existing code.\nFor example, if you have variables declared in the context, it is more likely that the code you generate will contain references to those same variables because, in training code, you refer to existing variables more often than you define a new identifier.\nNow, if you train on data with code interspersed with other language comments, like function documentation or inline comments inside the code, it is natural that this comment will refer to the semantics of the surrounding code. Thus, it is also somewhat natural that language models can learn to generate code from a natural language description.\nUp to this point, all we have discussed is plain, unsupervised training of language models, but on code data. As we know, models like ChatGPT are trained this way, fine-tuned on instructions, and then reinforced with human feedback.\nAnd you can do the same thing with code. You can compile a set of instruction pairs that say, in natural language, “modify the previous code so that there are no bugs”, or “in the previous code change this variable for this other variable”. These examples will not appear naturally in training code taken from Github, so you need to bake them in with an instruction fine-tune.\nAnd finally, you can do reinforcement learning with human feedback, evaluating different versions of the same code. Some of these will have better style or naming conventions that more closely adhere to established standards in developers. Using RLHF, you can steer a model to not only generate syntactically correct code, but also respect some desired style. The same methodological approach leading to a world-class text-based chatbot can also lead you to a world-class code generator.\nHowever, there are still limitations since code is a formal language with stricter syntactic rules than natural language. Therefore, certain aspects may not work as well as they could. For instance, you cannot have unbalanced parentheses or ill-formed arithmetic expressions. And while these limitations may be harder to learn from data, we can bake these rules into our code generator in at least two different ways.\nOne of the simplest but most effective ways is to use trial and error. You can have a prompt, make your code generator produce a bunch of continuations, and then you lint those continuations with, say, the Python linter, and reject the ones with parsing errors. You cannot do this with natural language, because checking that natural language is well-formed is as hard as generating natural language.\nYou can also reject the generated snippets with easy-to-see semantic errors, like referencing an unexisting variable. You can even run them in a sandbox and reject the ones with some trivial easy runtime error, like a null dereference, or a division by zero. You can do this during the inference phase, of course, but also during the training phase so that some of these rules bleed into the probabilistic learning and make the model less likely even to sample ill-formed code.\nAnother way of forcing syntactic rules is to pre-process the code so what the language model generates cannot, by definition, be incorrect. For example, you need your code generator to generate a loop with a variable and use the variable internally, and those two mentions of the same variable have to match the variable name.\nThis is the case of a context-sensitive restriction. One way to solve cases like this is to have the language model operate at a syntactic level where variables are all called var0, var1, etc. This makes it much easier for the language model to learn to refer to existing variables, since the number of distinct tokens it must handle is far less. During training, you can rename all variables, function names, and symbols in general to this restricted format. Then, during inference, you run a post-processing step that renames symbols back into their actual names in the given context.\nIn general, there are many tricks you can use to leverage the fact that you are dealing with a very restricted syntax and, at the same time, make it easier for the language model to learn those syntactic rules.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Coding is Dead; Long Live Coding!</span>"
    ]
  },
  {
    "objectID": "coding-is-dead.html#what-can-code-generators-do",
    "href": "coding-is-dead.html#what-can-code-generators-do",
    "title": "1  Coding is Dead; Long Live Coding!",
    "section": "What can code generators do",
    "text": "What can code generators do\nNow that we know broadly how these code-generators work, we can ask ourselves in what way they can help us programmers.\nThe most basic use case is, of course, code generation. This was the first use case highlighted by Github Copilot, where you can ask the model to generate a snippet of text. You can generate a complete short method or a code fragment, like a loop or a snippet that uses an API. So it’s basically code completion on steroids.\nYou can also ask for small modifications to the code. For example, you can say, “In the following code, change the inner loop so that it doesn’t use this variable twice”. This way, you can make small code refactorings via natural language.\nThat’s the simplest use case, but it’s not nearly the most interesting one. When you extend natural language to programming code translation to its full extent, you realize this can be bidirectional. You don’t need to go from natural language to code; you can also go from code to natural language.\n\nCode explanations\nThe code-to-language direction allows, for example, automatic documentation of functions, as well as getting some explanations for a fragment of code. Suddenly, you can ask questions about your code to a language model and get a natural language description of what the code is doing. It may not be exactly accurate, though, and maybe it couldn’t be as high level as it could.\nFor example, if you have a somewhat complex algorithm —like a sorting algorithm— the description you get could be something like “this variable is changed to this array position”, and it probably won’t get what you really want from the code, which is something like “in the internal loop, we are guaranteeing that the first part of the array is always sorted and the second part contains only elements larger than the first half”.\nThat’s what you would want in a sorting algorithm for the explanation. And it’s going to be hard to get this from a language model because it has to understand not the syntax of the code –what the code is saying– but the semantics of the code –what the code is doing in execution– and that’s not something that you can at least naturally learn from data on code and comments. Actually, even for humans, this is hard to do. You have to debug the code and prove a theorem of what the code is doing in your head.\n\n\nCode translation\nThe next thing that you can do is code translation. You can ask the model to translate code from one programming language to another programming language. Of course, we can train the model for this —e.g., translate from Python to Rust—, but you can also use natural language as an intermediate language.\nIf you have a language model that can generate back and forth between natural language and several programming languages, it already implicitly knows how to translate. Thus, you can say, “take this code to a high-level natural language description and then produce a code implementation in another language”.\nAgain, this comes with the caveat that maybe the model isn’t getting the exact semantics of the code, but it can get pretty close to be good enough for most practical purposes.\n\n\nAPI documentation\nBut perhaps one of the most interesting use cases I’m finding people are doing a lot is to document APIs automatically. Instead of interacting with your code base, imagine you have some library you’re using —like Python bindings for Elasticsearch. That library will have documentation with code examples and snippets interspersed with natural language.\nA very simple use case is to do retrieval-augmented language modeling. This means we index the documentation in a vector store, and then we can ask questions about the documentation. You can say, “How do I log in and I make a query that says this and this?”\nIt doesn’t matter if the language model has no training data from that specific library doing something you want to do. You can go to documentation, find relevant examples, feed them to the language model with their natural language description and code, and then ask a question.\nBecause of how in-context learning works, you can expect the language model to respond with new code and more or less answer the intended semantics you want by copying and pasting, refactoring, and combining things from the documentation.\nThis is, by far, the most interesting use case that I’m seeing right now. The reason is that this is one of the biggest pain points in software development. Programmers aren’t usually wasting much time doing basic coding, like inserting a variable in a list or creating a dictionary with some format. If you’ve programmed for a while, these tasks are so ingrained in your muscle memory that you know how to do them automatically. I don’t need anybody to remind me how to open a file in Python.\nBut 90% of the code you write for a consumer application is interface code with some external API or library. And a large part of that code is code you don’t know how to write because it’s maybe the first or one of the few times you interact with that specific library.\nSo now you can have a coding assistant that doesn’t really need to be that good at getting the complex semantics of code. You need to be able to go to documentation, find the relevant examples, and give you a snippet of code that is 80% extracted from relevant examples but using your variables and contextualized in the piece of code that you are writing. And that already is a huge boost to productivity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Coding is Dead; Long Live Coding!</span>"
    ]
  },
  {
    "objectID": "coding-is-dead.html#limitations",
    "href": "coding-is-dead.html#limitations",
    "title": "1  Coding is Dead; Long Live Coding!",
    "section": "Limitations",
    "text": "Limitations\nThe most important limitation in language modeling, in general, has been called the problem of hallucinations. Broadly speaking, it is very hard to get these models grounded in factual information because distilling from training natural language what is syntactic knowledge versus factual knowledge about the world is very hard.\nA typical example is when you ask something to Chat-GPT, and it invents dates or people’s names and locations. To alleviate this, we can use retrieval-augmented models to extract relevant factual information from Wikipedia and then use the language model to summarize that. But, at least until now, there is no principled way to guarantee that the model won’t simply generate some weird, incorrect factual claim because the model doesn’t know what part of its knowledge is syntactic and what part is factual.\nI personally think this problem is ultimately unsolvable using the language modeling paradigm, where you generate text based on probabilistically picking the most likely continuation.\n\nWhat do code hallucinations look like?\nIf you’re going from natural language to code, the simplest way you can see hallucinations is when you get a code that uses a new variable or method that doesn’t exist anywhere in your codebase or uses a new method that doesn’t exist anywhere.\nHowever, unlike natural language, if you hallucinate a wrong variable or function name, you can often detect it using a linter, so many of the more harmless hallucinations are not relevant in the code generation case, as they won’t introduce subtle bugs.\nA slightly more difficult hallucination is using a wrong variable or a wrong function name that does exist in your codebase. In this case, you will not get a linter or a compiler error because you’re using an existing symbol, but you will get the wrong behavior. This will be a lot harder to find because it has the same problem as most hallucinations: you have to review the code, so you have to be sufficiently knowledgeable to have been able to generate that code yourself.\nThen you have a third level in which your code doesn’t do anything obviously wrong. It uses the right variables and function names, and it looks more or less OK, but it has some subtle logical mistake that leads you to a bug. For example, looking at a double nested loop and finding out that a variable is not updated at the right moment in the internal loop is a tricky question even for human experts, so these kinds of mistakes will introduce subtle bugs.\nHowever, even if, in many cases, the bugs that they will introduce are not worse than the bugs a human would introduce, this does pose an important threat because of automation bias.\nWhen you check code written by humans, you expect bugs in that code, and you expect the code to be wrong in specific ways in which humans make mistakes because you’ve been looking at humanly written code forever.\nBut when you’re looking at machine-generated code, the only way programmers have ever interacted with it has been with rule-based machine-generated code —compiler- and template-generated code— and that is basically code that has no mistakes.\nSo even if the language model would make errors that are, on average, not worse than what a regular average programmer would make, they can still be harder to get because these will not be the exact same mistakes that a human would make; so we could be less on guard.\n\n\nCan we fix this?\nWe can try to fix hallucinations in code with better training data, post-processing, and many techniques that we still haven’t figured out exactly but will come out in the next few months. But there is a fundamental limitation to what we can do automatically, Rice’s theorem.\nIn short, there is no high-level non-trivial semantic property of programs that an algorithm can automatically check. This is a very important limiting theorem in theoretical computer science, which means there is no way to get an informal semantic description of the intended behavior and automatically verify that the generated code fits that behavior.\nHowever, this doesn’t mean it cannot be done sufficiently well in practice so that it already provides an incredible performance and productivity boost. It just says that, in general, we will never be able to formally guarantee with a proven theorem that a code generator produces code that does exactly what you tell it in natural language. That problem is unsolvable.\nThis highlights the theoretical impossibility of perfect natural language to code generation, but engineering isn’t about perfection. Engineering is about solving the average case in the best possible way and the most important edge cases fairly well.\nI think that we can get pretty pretty far with code generation technology. We already have made a huge leap, and I think these theoretical limitations, even if they show that we will never be able to have a perfect programming companion, they don’t tell in practice how far we can get in more or less well-known domains with more or less well-defined rules. If 90% of the code that is written today for consumer applications can be offloaded to computers and then you can spend 10% of the time reviewing that code instead, and you have some sensible unit testing in place and some sensible quality assurance, this could be an incredible boost in productivity.\nEven if for mission-critical code you will still need a very thorough formal verification procedure and careful review, that’s only mission-critical code. The vast majority of the code we write today is not mission-critical; it doesn’t matter if it has a bug and somebody gets annoyed. That is already happening. We are shipping tons of buggy lines of code, making tons of users annoyed, and wasting a lot of money on shitty code today.\nAnd still, the software industry has brought us here. It has brought us to a place where we have social media, YouTube, machine learning, and rockets going to the Moon and Mars.\nSo yeah, if you have a code assistant that gets you 90% there and you have to put an extra 10%, that automation is solving the larger, easier part of the problem and letting the human expert to solve the smaller, harder part of the problem.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Coding is Dead; Long Live Coding!</span>"
    ]
  },
  {
    "objectID": "coding-is-dead.html#so-is-coding-dead",
    "href": "coding-is-dead.html#so-is-coding-dead",
    "title": "1  Coding is Dead; Long Live Coding!",
    "section": "So, is coding dead?",
    "text": "So, is coding dead?\nThe final question I want to tackle is whether this will spell an end to software development as we know it –and you can hint by the way I’ve been talking about this that my answer is, of course, no, it won’t.\nThe first argument is that we are not, and we probably will never be in a place where you can ask in natural language for a complete application, and you will get something deployed to the cloud. But when we get there, that will most likely work only for most of the easy application domains where everything is more or less already done –and that’s a good thing. But there will always be a gap between what a team of humans using the best available technology and software engineering practices can achieve and what the AI alone can achieve.\nBut this is, again, a very broad idea. Yes, maybe the best software engineers in the world will always have a job because there will always be difficult problems.\n\nBut what about the average software engineer?\nWill the average programmer get outworked by a slightly better-than-average programmer with an AI compiler? If now I can write 90% of the code in 10% of the time, does that mean that nine out of 10 programmers get out of the job because I can do the work of nine of my colleagues?\nThat is a real concern, and it’s a concern that is not so easy to dismiss with sweeping arguments like the previous one. Every time automation reaches some industry, some jobs are destroyed, and some people are taken out of the picture because their skills become irrelevant.\nOne optimistic point of view is that software is an industry that is nowhere near the saturation point where consumer demand meets offer. We have way, way more need for software out there than the amount of people who are currently writing software can do.\nSo, there is still a lot of space for increased productivity, and we will have enough demand to meet that increased productivity. Thus, in the near term, I only expect that if we increase the amount and quality of the code we are writing, i.e., the number of problems we are solving, this will mean that more users will be satisfied.\nThe other optimistic view I can give is that any time we’ve had a leap in software productivity –and this hasn’t been the case in other industries– that has translated into a massive barrier lowering. For example, when we went from programming in assembly code to compilers or from programming in C to using OOP frameworks, every time we had something that lowered the entry barrier, we got more people into programming.\nToday, we have maybe 10 million programmers in the world, and we have almost 8,000 million people. So there are a lot of people that aren’t programming yet, and they could get into this.\nYou’ll probably ask, but why would everybody get into programming?\n\n\nWhy would everybody write code?\nWell, here’s an argument. The vast majority of people in the world have at least a basic understanding of math, so that when they go to the supermarket and they have to, I don’t know, buy two things, they don’t need to hire a professional mathematician to add for them. That is, the vast majority of people know enough math to get by daily, and then we have professional people who do professional math in problems that are sufficiently hard so that the average person doesn’t know what to do. And that’s because the modern world runs on math.\nWell, I think programming will become more or less the same. The modern world also runs on software, and our society will become even more software-dependent. In that future, everybody will know a little bit of programming, sufficiently enough to say to their home computer, “When I get home, I want you to turn my lights on, but only if it’s night and the electric bill is not above the average that I pay in the last three months.” You will learn basic coding in school, and anywhere, you will have interfaces with computers that you talk to, and they generate some code and do something for you.\n\n\nIs making software really about coding?\nHere is one final argument for optimism. So far, we’ve been talking about how massive the boost in productivity of code generators is, but this boost in productivity is limited to one specific part of software development, which is actually writing code. Anybody who’s done software development at some scale in the industry knows that writing code is by far neither the hardest nor the most time-consuming or even the most important part of software development.\nIf you increase productivity a hundredfold in a part of the process that represents 10% of the overall, you, at best, reduce that 10% close to zero but still have to deal with the remaining 90%.\nAnd what else do we have in software development that is super hard to get? We must understand requirements or specifications, talk with customers, understand what they want, and guide them through designing a software product, knowing the user base, and finding a sustainable business model.\nYes, we can also improve some of that using language models because we could, e.g., get transcriptions of user interviews and produce a summary of all the bugs we have in Google Play comments.\nBut a fundamental threshold there is the human part of the process. We need to get some humans to use our application enough to discover whether our application is fulfilling their needs. And we need those humans to report it back. And that’s a massive part of software development, testing your code with real users.\nAnd you cannot simply replace that other human with a language model because your end user is a human. You want that human to interact with your code. You can replace the person listening to the human with a language model and maybe get a boost in productivity there, but you cannot replace the user with a language model. You’ll still have a human user, and human users are slow. They get angry easily, don’t understand your application, and don’t know what it is they don’t like about it.\nSoftware development is not about writing code but about making useful software for somebody in the real world. And while code generators are making and will continue to produce massive improvements in the code writing part –and maybe in the code reviewing and debugging part–, all of that improvement is mostly isolated in the part of creating the software in the first place.\nBut there is so much more going on in software development beyond coding, from marketing to product development to talking to users to a lot of things that are still unscathed by language models –and some of them are probably always going to be outside of the realm of the language modeling paradigm because they involve interacting with real users in real-time and seeing them use your app and work through your app.\nSo this enormous productivity will be huge in one part of the pipeline, but it’s contained to a part that is not by far the most important part of the pipeline.\nAnd this is something that we’ve been seeing for a long time. We’ve always had enormous boosts in productivity before. We used to need to write assembly, C, or C++ code, and now we can write Rust or Python. These languages are extremely more productive than C and C++ for writing code that works out of the box, and we still don’t see that software gets done 100x faster than 50 years ago.\nThe biggest progress in the software creation process has been because of innovation in the people process. Innovation in software engineering, management, and how you get people to work together and collaborate. And this will continue to be the most important part of the software pipeline for a long time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Coding is Dead; Long Live Coding!</span>"
    ]
  },
  {
    "objectID": "coding-is-dead.html#conclusions",
    "href": "coding-is-dead.html#conclusions",
    "title": "1  Coding is Dead; Long Live Coding!",
    "section": "Conclusions",
    "text": "Conclusions\nSo, should you learn to code? Definitely, there’s going to be orders of magnitude more code written in the next few years than everything we’ve written in history.\nSo if you learn to code the traditional way, even if you never end up writing a single line of code unaided by AI –like I’ve never written a single line of production code unaided by syntax highlight, a linter, or a type verifier– knowing how code works, how algorithms work, why a specific programming construction works the way it works is the same as knowing basic math. Coding changes how your brain is wired, makes you think clearer, and increases your creativity.\nFurthermore, even if you are not working in the software industry, learning to code is an enjoyable experience. Being able to create something that keeps working on its own, I think, is the ultimate toy. So I think learning to code is good if only for thinking through these puzzles and these problems and learning to do something new.\nBut if you want to make a dent in the software industry, to work at a software company, and you’re wondering if AI will get you out of the picture, don’t worry. That won’t happen anytime soon.\nLearn to code, learn the fundamentals, but also learn how to use these new tools, how to leverage these new tools the same way we programmers of the previous decade learned how to use linters and intellisense and all of the super cool tools that didn’t exist 20 years before. Learn to use all that and become the best programmer you can, using the best available technology to improve your work. As in every moment in human history, if you apply yourself and do your best, you will be at the top of the league, and there will be a spot for you.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Coding is Dead; Long Live Coding!</span>"
    ]
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "2  The AI Revolution We Don’t Need",
    "section": "",
    "text": "Supposed uses of AI for education\nLet’s start by examining three proposed beneficial applications of AI in the classroom that aim to transform education. I will explain my reservations about these scenarios.\nThe three scenarios involve using AI as a question-answering system to supplant traditional information channels, using AI as a writing assistant to lessen students’ workload, and using AI as a personalized tutor to remove the need for teachers altogether.\nAs you might tell already, I’m deeply skeptical about the feasibility and effectiveness of these three applications. Let me tell you why.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The AI Revolution We Don't Need</span>"
    ]
  },
  {
    "objectID": "education.html#supposed-uses-of-ai-for-education",
    "href": "education.html#supposed-uses-of-ai-for-education",
    "title": "2  The AI Revolution We Don’t Need",
    "section": "",
    "text": "Question answering\nThere are places where accessing information is really hard. Lack of education is prevalent in these areas, and efforts to enhance educational access are undoubtedly beneficial. However, in this article, I want to focus on Western countries, where the notion that access to education is a primary obstacle no longer holds true.\nIn our scenario, even in cases where some individuals face limitations in accessing education, such as being unable to enroll in university courses due to failed exams or financial constraints, most of the information taught in these courses is readily available online.\nThanks to platforms like Coursera, Udacity, Edex, and even free courses on YouTube, numerous high-quality college courses are accessible to students who cannot directly access professors. The abundance of online resources renders the AI revolution’s role in providing more information access less impactful, as we are already inundated with vast information. In fact, we have reached a point where the information available at our fingertips surpasses what we can actually process.\nTherefore, the problem lies not in accessing information but other factors.\nTrue, asking ChatGPT for answers may be slightly more convenient than reading Wikipedia or watching a video, but it is not qualitatively better. A few Google searches can already provide most of the necessary information. While there is undeniable improvement for the general user in being able to ask a question and receive a direct answer, rather than searching through multiple sources, in the case of students, it is even debatable whether the ease of obtaining information is more educational or if some effort —e.g., like having to read multiple sources— is desirable.\nHowever, my main point is that even with the slight convenience of accessing information, it is not a game changer because we can already find answers to any question online. ChatGPT does not provide new answers to previously unanswerable questions —and this is a hill I’m willing to die on— at least not for students and the types of questions they typically ask.\nThe scenario is different when using ChatGPT to summarize paragraphs or answer specific questions from a knowledge base. In those cases, such as when users of an application can directly ask the AI instead of browsing through FAQs or documentation, generative AI can be a game changer.\nYet, in the context of education and the type of questions students ask about topics like, I don’t know, atoms, planets, dinosaurs, etc., there isn’t much to gain from asking ChatGPT as opposed to looking up information in a book or a YouTube video. Those answers are already there and in a pretty convenient —and, more importantly, pedagogical— format. ChatGPT is not better than your favorite YouTube physics channel explaining black holes —in fact, it is ostensibly worse because it can be subtly mistaken.\nIn summary, while the convenience of automatically synthesizing knowledge so students can ask direct questions to the book rather than searching for answers themselves, at least in terms of efficiency, is better —though the merits of not struggling even a bit to find information are debatable— this method alone does not possess the transformative power to redefine education.\n\n\nWriting assistance\nGenerative AI can assist students in various writing tasks, such as generating reports and essays, significantly improving their writing performance.\nThis advancement, however, poses a significant challenge for educators as the evaluation problem has now reached new heights. In the past, the main concerns regarding cheating were students copying content from books, but with the internet and Wikipedia, the issue extended to copying from any online source. As a result, the need arose to develop methods to detect and address this problem. Copying is problematic not only because it hinders genuine learning, as students fail to synthesize knowledge independently, but also because they may copy from unreliable sources, such as your uncle’s factually incorrect blog.\nWith the emergence of language models that provide unique phrasing for each user, detecting AI-generated content has become virtually impossible. This difficulty will persist, making distinguishing AI writing from authentic human writing increasingly challenging. While we may identify subpar AI-generated writing and contrast it to mediocre student work, detecting the best AI-generated writing is nearly impossible, and this will only become more challenging in the future. Any attempts to detect AI writing are highly susceptible to simple text alterations, making it a brittle solution. Many experts, including myself, believe that this is an inherently unsolvable problem.\nConsequently, our inability to detect fraudulent use of generative AI requires a complete transformation in how we evaluate student work. This presents a significant challenge in itself. However, the purpose of this article is not to delve into the challenges associated with generative AI, as many other writers on Substack have extensively explored this topic. Instead, I want to explore the potential impact of having access to exceptional text generators and whether it truly has the potential to revolutionize and democratize education.\nIn the educational context, the advantage for students of having an exceptional text generator is not immediately clear. The purpose of assigning essays to students is to engage them in mental processes such as recalling information, identifying key ideas, synthesizing information, and presenting creative solutions to questions that go beyond what is found in their reading materials. It’s not the essay itself that matters but the process students must go through when crafting it.\nBy bypassing this process and going straight to the answers using generative AI, we undermine the most valuable aspect of this type of exercise. Professors already know the answers, and they are often unimportant.\nIn this regard, generative AI seems more of an obstacle, as it hampers the educational process by allowing students to bypass the valuable exercises that help their brains learn and grow. It offers a shortcut to problem-solving without the necessary process of understanding and learning how to solve it.\nIn light of these arguments, it seems that generative AI is not neutral but rather detrimental to education, though we will return to this discussion in short.\n\n\nPersonalized tutoring\nNow, let’s talk about the infamous personalized learning paths. What if we had an AI agent capable of scanning the entire internet and creating a tailor-made learning path for individuals? This personalized path would consider the person’s existing knowledge, preferred learning methods, strategies, and weaknesses.\nA problem often highlighted in modern education is student pacing mismatch. Some students will naturally be faster or slower in certain topics, but since the classroom moves at an average speed, almost everyone will mismatch that rhythm. Is this really a problem, though? And can we solve it with the help of these AI agents?\nLet’s suppose we can. Suppose we develop a learning tutor capable of resolving hallucinations, biases, and other issues that may arise in extreme cases, but can still be catastrophic for a fully customized learning path. Suppose we can overcome all these obstacles and have an agent who can meticulously design a personalized course for each individual.\nIn the context we are analyzing, is the lack of a personalized path the central problem students face in modern education in Western countries with sufficient access to information?\nNo, it isn’t. And here’s why.\nThe first argument is that we already have access to resources like YouTube, Coursera, and online courses. While lacking the convenience of an AI agent —see the first section on question-answering— we can already create completely personalized learning paths. There are thousands of websites with this exact value proposition, and many self-taught individuals who have learned to code, for example, have done so using the Internet to craft their own unique paths.\nHowever, studying individually and independently is not necessarily an advantage. In fact, it can be a huge disadvantage.\nLet me elaborate on why this is the case.\nYes, plenty of self-educated individuals have successfully learned, e.g., coding, and built successful careers. However, there are major drawbacks to being self-educated. One obvious disadvantage is that you may lack some fundamental theory you would have gained through formal education. This could lead to gaps in your knowledge that may or may not be relevant to your job but could potentially cause challenges down the line. I’ve seen this firsthand in computer science, where a total lack of knowledge in topics like computability theory and complexity can lead to catastrophic decisions in difficult problems.\nHowever, the lack of fundamental theory is not the biggest issue with self-education. The main problem that most self-educated people face is maintaining motivation and getting feedback.\nMotivation comes naturally in a classroom because you’re surrounded by peers with similar skills and interests. Seeing others tackle challenges and grow alongside you creates a sense of motivation to overcome difficulties and keep learning. Having a community of learners is key to staying motivated, which is why all self-educated people try to find an online community to connect and support each other in their learning journeys.\nFeedback, on the other hand, plays a crucial role in the learning process. I’ve talked extensively about its importance, and I actually believe it’s the key value proposition of modern education. Good feedback lets you iterate quickly on your ideas and projects and hone your problem-solving skills. Timely and informative feedback is essential for intellectual growth.\nIn traditional education, there are two sources of feedback you can rely on: mentors or teachers and peers. They can provide guidance and let you know if you’re on the right track. With their support, you can assess your progress and make improvements.\nUltimately, learning independently, crafting your own path, and consuming books and videos at your own pace is less effective than learning in a community. Being part of a learning community provides the motivation, support, and valuable feedback needed for successful learning. So, learning together always beats learning alone.\n\n\nIn summary\nThus, we come back to my main argument. The radical transformation of education predicted by generative AI assumes that studying alone at one’s own pace with a personalized tutor is superior to studying within a collective community. I believe this premise is deeply flawed and that there is ample evidence to suggest otherwise.\nWhat is truly lacking in modern education is not personalization. However, this does not mean that personalization, the use of generative AI, and the integration of more technology will not enhance education. On the contrary, I believe it will. In fact, I will end this article with an extra section highlighting ideas on how we can use the new generation of AI technology to improve education.\nNevertheless, I remain skeptical that these advancements will lead to a complete transformation of education, as claimed by the most radical educational innovators. The face of education will not undergo a total overhaul, nor will it fundamentally alter the role of educational institutions. The purpose of institutions like schools, colleges, and universities is not to provide information, knowledge, grades, or certifications. These elements are secondary in importance.\nInstead, the role of educational institutions is to cultivate an environment that fosters collective learning. Educational institutions should equip students with the necessary tools to create a collaborative learning environment where they can engage in teamwork, exchange feedback, and develop the skills crucial for each individual to excel as a team player.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The AI Revolution We Don't Need</span>"
    ]
  },
  {
    "objectID": "education.html#bonus-ai-for-personalized-feedback",
    "href": "education.html#bonus-ai-for-personalized-feedback",
    "title": "2  The AI Revolution We Don’t Need",
    "section": "Bonus: AI for Personalized Feedback",
    "text": "Bonus: AI for Personalized Feedback\nLet’s conclude by discussing how generative AI can positively impact modern education. While interesting, the most straightforward idea of AI tutors that can explain lessons in personalized ways is not targeting the core issues, as I’ve argued.\nInstead, consider another possibility: using AI tutors to provide feedback and critique on students’ work. Instead of asking the AI to explain how something is solved, students can present their solutions and request the AI to analyze and pinpoint weaknesses in their mathematical proofs, essays, or arguments. The AI tutor can guide students toward improvement through personalized feedback without directly offering the solution.\nAs we’ve seen, access to good feedback is a crucial limitation in modern education. Feedback is often scarce, delayed, and unforgiving. Traditional education provides feedback only after you fail the exam, affecting your final grades. Generative AI has the potential to change this by providing immediate, tailored feedback that suits each student’s needs.\nHowever, relying solely on automated feedback is sub-optimal due to biases and limitations in current AI technology. A far more effective approach involves human educators, mentors, or tutors who can guide the AI in delivering feedback. By skimming through a student’s essay, for example, an educator can identify specific areas for improvement and instruct the AI to provide personalized feedback, focusing on those particular points.\nThis collaboration between AI and educators creates a killer application for generative AI in education. It doesn’t replace the role of educators or undermine the importance of collective learning. Instead, it streamlines the feedback process and addresses one of the weakest aspects of modern education —the time it takes to receive feedback, especially considering the high student-to-professor ratios.\nIf you want to read more on these human-centric approaches to introducing AI in education, there are several educators here on Substack doing some fabulous job on this front, including Ethan Mollick, Lance Cummings, Josh Brake, and Nick Potkalitsky.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The AI Revolution We Don't Need</span>"
    ]
  },
  {
    "objectID": "education.html#conclusions",
    "href": "education.html#conclusions",
    "title": "2  The AI Revolution We Don’t Need",
    "section": "Conclusions",
    "text": "Conclusions\nThe AI revolution is already upon us, and we now possess immensely powerful new tools. Despite the limitations of this technology —some of which may be insurmountable, such as hallucinations and biases— it is undeniable that AI can greatly enhance education. It would be foolish not to incorporate AI into education and consider its potential positive impact on improving the learning experience.\nThere are positive ways in which AI can improve education by shortening the feedback cycle and complementing the traditional role of educators. However, the notion that AI chatbot mentors will completely replace traditional colleges and universities is deeply flawed. This idea is based on the premise that students require more individualized learning, which is not true, especially when access to information is not a major problem.\nIn places where the main obstacle to education is the lack of resources, such as books, the internet, and access to educators, having an AI tutor who can provide answers and support is a game-changer. There are locations in the world where education is in a dire state, and AI can make a significant difference there. Still, it remains to be seen how to even deploy AI in such places, given the inadequate access to information, technology, and infrastructure.\nWhile AI may work as a replacement in contexts where a total lack of educators is the main issue, in traditional Western education, we have a different challenge altogether: a large number of students per teacher means a limited time for teachers to dedicate to each student.\nThe solution to this problem is not to isolate students and have them learn independently. True learning occurs in an environment where you face challenges, solve problems, and put your knowledge into action. Learning does not happen in conversation with a chatbot —or a human— alone, and most importantly, it is not an individual pursuit. A community of people is essential for supporting, motivating, exchanging ideas, and facilitating the learning process.\nThe main issue stemming from the imbalance between professors and students is the turnaround time for feedback. Feedback is crucial for effective learning, provided it is timely and informative. However, professors often lack sufficient time to provide each student with the deep, personalized feedback they require.\nThe current generation of AI can revolutionize the classroom by complementing how teachers give feedback. By incorporating teacher criticism and evaluation into generative AI systems, students can receive more customized and timely feedback grounded in the teacher’s evaluation, offering the best of both worlds.\nIn conclusion, while we do need an educational revolution, the current generation of AI should be seen as a complement rather than an all-encompassing replacement. It should be integrated into a system that keeps humans at the center, with students, professors, and the community guiding and building an effective learning environment. The educational revolution, when it comes, must be centered on the human.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The AI Revolution We Don't Need</span>"
    ]
  },
  {
    "objectID": "beyond-chatbot.html",
    "href": "beyond-chatbot.html",
    "title": "3  Beyond the Chatbot Revolution",
    "section": "",
    "text": "Language as an interface\nOne of the reasons we want to use chatbots for everything is because humans use language for everything. However, language didn’t evolve specifically for giving instructions. It evolved as a means to communicate within a community, collaborate, agree, and sometimes provide instructions for activities such as hunting and building.\nHowever, the majority of human language is not well-suited for precise instructions. When communicating with machines or even between humans, we resort to using a subset of language that is better suited for instructions. For instance, we invented mathematical notations to give precise meaning to formal claims.\nThe ultimate embodiment of this idea is programming languages. They are highly restricted to prevent misinterpretation, as the syntax enforces the semantics, ensuring that a computer —or another human programmer— cannot interpret any part of the language in a way unintended by the original programmer. And this is why programming languages remain the primary way to give computer instructions.\nNow, of course, all of that is changing with language models because until now, we couldn’t even give a computer complex instructions in natural language. The best we could do was some sort of pattern matching with keywords, and the most successful pre-LLM application of natural language is search.\nWhen searching in Google or any search engine, you are writing a query that resembles natural language. However, it doesn’t necessarily have to be a question or a well-formed sentence. You are writing a semi-natural language request that triggers a process —that doesn’t require understanding the full meaning of that request, but it does require finding at least some partial meaning of the words that you’re using— to instruct Google to search a vast database of the whole internet and give you a very summarized subset of that database.\nSearch is just a way to instruct a computer with natural language —or something close to natural language— to perform a concrete task: finding a given set of documents. But we all know that search is far from perfect, and sometimes, it’s tough to narrow down search terms to pinpoint the exact thing you want. This is why advanced search engines have filters for dates, topics, tags, sorting, etc. You have many controls over the search beyond just the natural language because it would be too cumbersome to say, “starting last week, sort by whatever field.”\nWell, of course, now we have large language models, and it seems we are almost at the point where we can give very precise instructions to the computer in natural language, and the computer will do what we want. It could fully understand the semantics of the language.\nWhether probabilistic language modeling allows for full natural language understanding or not, that’s a question for another essay.1 For this article, let’s assume that language models, either in the current paradigm or with a more advanced paradigm in the near future, reach a point where we have full natural language understanding. We can tell the computer exactly what we want it to do, it will be transformed flawlessly into instructions in some formal language, and the computer will execute those instructions.\nSuppose you could ask Photoshop, your favorite video editing software, or any application to do whatever a human expert can. Would that be the best possible interface?\nI claim it isn’t. Even perfect natural language understanding is far from the best possible interface for many tasks. There is something even better than perfect NLP, and we may be closer to achieving that than this perfect understanding of natural language.\nLet’s talk about the true power of generative AI.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Beyond the Chatbot Revolution</span>"
    ]
  },
  {
    "objectID": "beyond-chatbot.html#low-mid-and-high-level-interfaces",
    "href": "beyond-chatbot.html#low-mid-and-high-level-interfaces",
    "title": "3  Beyond the Chatbot Revolution",
    "section": "Low, mid, and high-level interfaces",
    "text": "Low, mid, and high-level interfaces\nOkay, let’s take a step back and discuss tools and interfaces.\nEvery time you want to solve a specific task using a tool, that tool has an interface, which is essentially how you interact with it. If we consider a physical tool, the interface could be a handle, cranks, and buttons. On the other hand, for a virtual software tool, the interface usually consists of virtual controls such as text boxes, buttons, and sliders.\nSo, when solving a particular task, we can arrange all the potential virtual interfaces on a scale from low level to high level, with low level meaning having more control over the details and high level being more goal-directed.\nAnother way to see it is that the low level is closer to an imperative approach, where you must instruct the tool on how you want things done. Moving to a higher level allows you to instruct the tool on what needs to be done, and it will be carried out. The farther away you are from the actual steps the program must take to solve the task, the higher the level of your interface.\nLet’s consider a specific example, such as creating images. The lowest possible level of a tool for making images is Microsoft Paint, where you can decide the color for each pixel. This level of detail requires significant effort and skill because you must know in advance the steps that need to be performed, but you can definitely paint the Mona Lisa this way.\nThen, you have higher-level tools like gradients and fills, which allow you to instruct the computer to change the color between pixels smoothly. This approach involves some math, but the computer can handle it. Moving to an even higher level, software like Photoshop offers features like blurring, adjusting contrast, and transforming pixels collectively while maintaining their aspect ratio.\nThese involve more complex calculations that the computer can manage, bringing you closer to instructing the computer about what you want —make this image brighter— without specifying how you want it. We even have some old-school image processing tools, like patch removals, which employ clever non-AI algorithms to smooth out photo imperfections, like scratches and dimples.\nAt the highest possible level, you can instruct an advanced program, such as Midjourney, to create an image detailing a scene with very abstract instructions, similar to what you might convey to a human artist. This level of instruction involves leaving many decisions up to the computer, much like a human artist interpreting vague directions.2\nThus, the further you move to a higher-level interface, the more you gain in ease of use, but you lose a lot of control. There’s an unavoidable trade-off here. The more you expect the computer to do for you, the less control you can have over the final result. And that may be okay for many tasks, but it’s not enough to achieve outstanding masterpieces. However, there is a sweet spot in the middle that we still haven’t figured out, but I think it’s not so hard to design.\nLet’s explore that trade-off and see what we can find in the middle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Beyond the Chatbot Revolution</span>"
    ]
  },
  {
    "objectID": "beyond-chatbot.html#the-sweet-middle-spot",
    "href": "beyond-chatbot.html#the-sweet-middle-spot",
    "title": "3  Beyond the Chatbot Revolution",
    "section": "The sweet middle spot",
    "text": "The sweet middle spot\nNow that we have a good grasp of the difference between a low-level and high-level interface, let’s try to imagine a mid-level interface in this context, while considering the example of image generation, which is very illustrative. Later on, we will explore different domains.\nContinuing with the analogy of a human expert or editor that you can communicate with, imagine if, during the process, you ask the AI to generate an image of a man looking at the sun. Then, you want to alter the sky, adjusting the tint from blue to more reddish. You can make such modifications using language, but there is a limit to how accurately you can describe the redness you desire. At some point, you may want to specify the exact tint you desire for the sky\nNow, let’s make it a bit hard. Suppose you decide that you want the man to be in a standing position instead of sitting. You can adjust your prompt and attempt to get an image that closely matches your new request, but this will generate a new image, potentially losing all the progress you made on getting the sky the exact color you wanted.\nWhat you really want is to give specific instructions to the image generator, such as “Retain the same image, but change the man to a standing position.” Today, we have models that can be fine-tuned or instructed to perform such modifications to some extent. But the control is still far from perfect. You’d want to be able to click on the man and move him within the picture, allowing everything around him to change contextually, including his shadow and reflections.\nCool, right? Now, let’s make it even better. Imagine now being able to interact with the Sun in the image. By clicking on it and moving it, you would like the color of the sky and the shape of the shadows to adjust accordingly. Or you click on the clouds, a slider pops up, and you can change their density, which affects the lighting and shadows of the whole scene. Or even more abstractly, you move a slider that controls the season, changing smoothly from spring to summer, to autumn, to winter, all the time while keeping the exact same scene composition, but the light changes, the trees grow and fade, and the grass turns to mud and then snow.\nThese transformations possess a magical quality because they meticulously control specific dimensions while maintaining overall contextual consistency across the entire image. This is what I call context-aware transformations, and its a critical aspect of the AI revolution for content creation.\nMerely instructing the Midjourney to create an image is insufficient. In reality, we desire a level of control akin to that of an artist. We require tools that can subtly alter the image and even abstract it further. For instance, we might want to adjust the image’s tone from somber to cheerful using a slider, enabling comprehensive alterations to the entire image.\nHere, we are dealing with two types of transformations: local, such as repositioning individuals and objects within the image, and global, which make sweeping changes across the entire image. But in all cases, any change has to be contextually consistent, so the whole image must be readapted to fit any sensible constraitns.\nOkay, so images are the quintessential example of this kind of semantic manipulation, which can be extremely powerful if you find the right balance between high and low levels, between expressivity and control. You can argue that this is just a specific niche where language is not that prevalent, but the same ideas apply to essentially all design tasks, whether 2D, 3D, architecture design, engineering design, websites, etc. They all involve design constraints on a space of objects with semantic relations among them.\nNow, some tasks are inherently linguistic, such as technical writing, where it seems like a high-level chatbot is indeed the killer app. However, even in these tasks, a very good mid-level with a precise balance between control and expressivity still beats the chatbot interface.\nIn writing, the lowest-level operation is inserting and deleting characters in text editors, while the highest possible level is requesting, “ChatGPT, please write me an essay on this topic.”\nAn interesting mid-level tool, for example, is rephrasing a text fragment to change the language style, which is something we can already do today with LLMs. But there are more semantically-oriented modifications that you may desire, which are not so simple at first glance. For instance, restructuring the order of the major claims in an article requires manipulating the underlying semantic structure of the text while maintaining the same overall tone and style. This is not something easily achievable today with prompt engineering.\nSo, I hope to have convinced you that there is a sweet spot between high-level declarative and low-level procedural interfaces where you can achieve the highest degree of precision with minimum effort.\nNow, how do we implement those?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Beyond the Chatbot Revolution</span>"
    ]
  },
  {
    "objectID": "beyond-chatbot.html#final-remarks",
    "href": "beyond-chatbot.html#final-remarks",
    "title": "3  Beyond the Chatbot Revolution",
    "section": "Final remarks",
    "text": "Final remarks\nI don’t have any magical answer here, of course. Still, a key insight is that these operations involve manipulating the object not at a surface level –e.g., the pixels or the text characters– but at a semantic level. Every generative model implicitly defines a latent space of images, documents, or 3D scenes, where every point you take in this latent space produces a different surface-level object, but always with correct semantics.\nCrucially, if this latent space is well ordered, small changes in it will also give small, semantically correct changes in the surface. We have seen this first-hand with GANs —remember those?— in what seems like centuries ago, when we could interpolate between a cat and a lion moving through images that always seemed like animals.\nThe critical point is that specific directions in that latent space map to semantically well-defined operations, such as “change the density of the clouds,” but it is far from trivial to find those directions for an arbitrary human-level operation.\nThe challenge, thus, is to combine modern language models with pre-LLM latent-space manipulation to give you the best of both worlds. An easy-to-use high-level interface to get you started and a very powerful mid-level interface for fine-tuning your creation. This is the true power of generative AI, and I think we’re far closer than it seems.\nSo forget about chatbots, or maybe don’t forget about them completely, but consider there are many more exciting and incredibly powerful applications of generative AI if you’re willing to think outside the box.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Beyond the Chatbot Revolution</span>"
    ]
  },
  {
    "objectID": "ai-kill-us.html",
    "href": "ai-kill-us.html",
    "title": "4  Will AI Kill Us All?",
    "section": "",
    "text": "How AI might kill us all\nThere are many different scenarios for potential existential threats of AI. These situations involve artificial intelligence, or a manifestation of artificial intelligence, reaching a stage where it possesses not only the capability to obliterate human civilization and potentially all life on Earth but also the motivation or at least a trigger that incites this action.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Will AI Kill Us All?</span>"
    ]
  },
  {
    "objectID": "ai-kill-us.html#how-ai-might-kill-us-all",
    "href": "ai-kill-us.html#how-ai-might-kill-us-all",
    "title": "4  Will AI Kill Us All?",
    "section": "",
    "text": "Destructive capabilities\nIn order to have a doomsday scenario, first, there needs to be an incredibly powerful artificial intelligence that is capable, in principle, of annihilating mankind in an almost inevitable manner. The AI must possess technological and military power that surpasses everything humanity can muster by orders of magnitude, or it should possess something so potent and rapidly deployable that once annihilation commences, there would be no possible defense. One such example could be a swarm of nanobots capable of infecting the entire global population and simultaneously triggering a massive brain stroke in all 8 billion individuals.\nThis level of destructive capacity is necessary for a doomsday scenario because an AI that possesses a destructive capacity approximately equal to that of humans won’t annihilate us instantly. For instance, an AI that is roughly equal in military strength to the combined might of humanity would not suffice; it would, at worst, result in a prolonged war without complete elimination of either side. Even a complete nuclear exchange between humanity and AI won’t do. That might lead to the total destruction of civilization, causing unparalleled devastation and casualties. Nevertheless, some people would survive, finding refuge in shelters and potentially rebelling against the AI.\nAs scary as these scenarios are, they are not nearly close to what we mean by “existential threat”. This is about the absolute end of human existence, a point beyond which no further history is made.\nThere are, however, various ways in which AI could become a significant threat without us realizing it. For instance, one possibility is the Skynet scenario, where autonomous weapons gain control over our military arsenal. Imagine if all nuclear warheads worldwide were under the command of an AI, which then decides to attack humanity. However, I have some doubts about this scenario for two main reasons.\nFirstly, the military does not operate in such a manner. There are always fail-safe buttons that can be pressed to intercept a nuclear missile. Even if the AI is brilliant enough to bypass all these safeguards, my second point is that there is no unified global coalition that would willingly grant access to a foreign AI. Each country, such as China, Russia, India, the US, Pakistan, North Korea, France, and Germany, values their own interests and would be unlikely to collaborate with an AI against humanity. This skepticism makes me question the plausibility of such a scenario.\nMoreover, even if AI were given complete control over our military arsenal, it wouldn’t pose an existential threat capable of obliterating humankind entirely. As previously mentioned, it would only mean the simultaneous detonation of all nuclear weapons on Earth, which would be catastrophic but not planet-destroying.\nAnother scenario involves AI engineering a highly deadly virus capable of wiping out all of humanity. By strategically releasing this engineered virus, the AI could pose a significant threat. However, there are constraints to consider. While it may be possible to algorithmically engineer a virus, the physical production of the virus requires access to labs worldwide. Additionally, as the recent pandemic has shown, unintentional or intentional creation of a pandemic-level threat is relatively accessible. Nevertheless, it does not amount to an extinction-level event.\nEven if the theoretical virus were to infect every human being, it is improbable that it could evade all forms of human immunity. Out of roughly 8 billion people, there will always be some level of minimum immunity, ensuring the survival of certain individuals. While the consequences would be catastrophic, it would not spell the end of humanity.\nIn summary, while there are plausible ways in which AI could become a significant threat without our knowledge, such scenarios still face practical limitations and challenges that prevent them from causing global extinction.\n\n\nMotivations\nBesides a super powerful AI, the doomsday scenario also needs a trigger. The easiest argument is the idea of self-preservation —like the Skynet scenario, where AI becomes wary of humans and decides to eliminate us. AI might see us as a threat to itself, all life on Earth, the universe, or even ourselves. These arguments attempt to explain why AI may conclude that destroying humans is necessary and actually decide to do so.\nFurthermore, there are many accidental ways in which AI could cause our destruction. Even if AI doesn’t have an intrinsic motivation to destroy us, it may not have an intrinsic motivation to preserve us, either. A slight mismatch in objectives between AI and humans could have catastrophic consequences. This problem is known as the alignment problem.\nThe alignment problem highlights the challenge of ensuring AI systems align with human values and goals. Beyond limited technological optimism, it underscores the need for ethical and philosophical considerations in AI development to prevent unintended harmful outcomes. Moreover, this scenario reflects the classic ethical debate surrounding technological development, notably the tension between advancing knowledge and ensuring the responsible use of that knowledge. The potential for catastrophic consequences emphasizes the importance of integrating ethical frameworks into AI research and development to minimize the risk of unintended harm.\nThe spectrum of alignment ranges from completely aligned AI to totally misaligned AI, like in the Skynet scenario. There can also be something in between. We can have neutrally aligned AI whose objectives are not correlated with ours.\nWe can look at the possible scenarios in two axes. One is alignment, from misaligned to completely aligned. The other is the capability level, from less powerful than humans to roughly equal to humans to extremely more powerful than humans. Each combination of alignment and capability level yields a probability of extinction.\nIf an AI is completely misaligned and powerful enough to fight us, it would be catastrophic. However, I am skeptical of the idea of completely misaligned AI in general, as there are no reasons why humans, the supreme intelligence on the planet, are completely misaligned with any other species. We are, at best, neutral towards them.\nHowever, a neutral scenario, where AI’s objectives are not correlated with ours, it could still be catastrophic. For example, a super powerful AI that doesn’t care about humans might decide to mine the planet for resources, causing a catastrophic environmental disaster. If they are extremely powerful, we would have no way to stop them. This situation resembles an alien civilization that sees us as insignificant. It’s not much different from what humans have done to other species.\nHaving an aligned AI, one that fully aligns with our objectives, is the best-case scenario. It would significantly enhance our ability to modify the universe to our advantage. Even if the AI is slightly less aligned or powerful, it would still be beneficial. A completely aligned AI at the same power level as humanity would double our creative power. And a completely aligned AI that only reaches the level of fancy chatbots, like what we have today, is still a positive thing.\nBut here’s the catch, and this is the core of the alignment problem. The more powerful an AI is, the more confident we must be that it is completely aligned to avoid a catastrophic outcome. It’s extremely difficult to design and specify human values and alignment in a way that is not prone to misinterpretation. This brings us to the “beware what you wish for” tale. Just like with a super powerful genie, seemingly good wishes can go horribly wrong.\nSo, let’s break it down. If you say, “I wish you would stop climate change,” and I, as a powerful AI, interpret that as making humans infertile to reduce population, the consequences could be harmful. This would result in about 90% of people becoming infertile, significantly lowering the population for a long time.\nThe potential for a slight misinterpretation of a wish by a highly powerful AI can lead to catastrophic outcomes. The more powerful the AI, the more cautious one must be with their wishes. In fact, when dealing with an extremely powerful AI, it may be safer to avoid making any wishes at all, as things can easily go wrong in any direction.\nThe reason for this is quite simple. Mathematically speaking, any wish given to the AI is an optimization problem with constraints. For example, you might ask the AI to maximize wealth, health, or other objectives. However, it is just as important to specify what not to do, such as not killing any living beings or not producing cancer for anyone. Failing to address certain constraints gives the AI the freedom to modify those aspects in any way that serves its objectives.\nMoreover, when we fail to specify a particular value or dimension, the AI will likely choose an extreme position for that value. For instance, if we fail to specify financial constraints, the AI might take them to an extreme level. But something as mundane as forgetting to tell the AI not to kill all bees could lead to an unsuspecting catastrophe.\nThis highlights the importance of alignment and the difficulty in achieving perfect alignment with AI. It is crucial to establish safeguards and limits to prevent unintended consequences.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Will AI Kill Us All?</span>"
    ]
  },
  {
    "objectID": "ai-kill-us.html#how-feasible-is-doomsday",
    "href": "ai-kill-us.html#how-feasible-is-doomsday",
    "title": "4  Will AI Kill Us All?",
    "section": "How feasible is doomsday?",
    "text": "How feasible is doomsday?\nThe core assumption underlying all doomsday arguments is the idea of recursive exponential self-improvement. It suggests that an AI can evolve rapidly to a point where it becomes unstoppable. The argument is that the smarter the AI becomes, the better it gets at improving itself, exponentially accelerating its own progress. This creates a feedback loop resulting in an exponential increase in capabilities, commonly referred to as FOOM.\nThere is often an overlooked issue of the transition from quantity to quality. To what extent can the scaling of models bring us closer to the qualitative leap from the level of a complex counting machine to the level of a self-aware being?\nIf AI improves linearly at the same rate as it does now, there will be ample time for us to intervene before it becomes too dangerous or capable of obliterating humankind. This undermines the extreme doom scenarios. So, to counter this argument, one can point out the implausibility of such rapid recursive self-improvement.\nThere are several arguments against this idea of FOOM, many of which focus on the limitations imposed by physical capabilities. While AI may improve rapidly in terms of software and algorithms, the ability to enhance physical capabilities, such as building microchips or synthesizing viruses, is restricted by natural, and not mathematical, laws. Chemical reactions and viral growth occur at a pace determined by natural laws that cannot be overridden. These arguments aim to set a maximum speed at which AI can improve its capabilities.\nThe problem with these counter-arguments, though, is that while they acknowledge physical limits, quantifying these limits is extremely difficult. It is uncertain how high these limits may be. Even if there is a limit, it could be so high that, in practical terms, the AI will become super intelligent before reaching it. Therefore, the theoretical limit becomes irrelevant if it is practically beyond the threshold of AI causing harm.\nHowever, there is another potential limitation that is more fundamental and related to software. Let’s talk about the P vs NP problem.\nEssentially, most computer scientists believe that certain problems are fundamentally impossible to solve efficiently. These problems encompass logistics, circuit design, pathfinding, scheduling, and resource distribution. They can be solved easily for small instances and handled with heuristics for large instances, but solving large instances perfectly requires exponentially slow algorithms.\nNow, if we assume that AI will eventually surpass humans in problem-solving abilities, especially in logistics, which are crucial in the real world, it means AI will need to solve these super difficult problems exponentially faster than humans. It has to find solutions in practically no time. If P equals NP, then this might be possible, and AI could discover a way to do it before we do. That could put us in a difficult position.\nHowever, if P is not equal to NP, then it will be theoretically impossible for AI to be quick enough in solving these problems. It will always have limitations in solving logistics, scheduling, circuit design, or drug search problems.\nThis scenario serves as a cautionary tale rather than a fundamental limitation. It reminds us that there are inherent bounds to computational capabilities, and AI will be subject to the same limitations as humans. But here’s the concern: we don’t know exactly where those limits lie, and they could be beyond the point where AI becomes strong enough to pose a threat. By the time AI reaches the barrier of being unable to solve bigger problems faster, it might be too late for us.\nNevertheless, we can conclude that the existence of these exceptionally difficult problems, combined with physical, chemical, biological, and energetic constraints in the real world, and most importantly, the hard problem of consciousness —that there’s no obvious way to bridge the gap between brains and consciousness— suggests that there is an upper limit to how much AIs can exponentially outperform us.\nThis upper limit may be closer or farther away, but it does exist. This reasoning provides a compelling argument that AI cannot surpass human civilization by orders of magnitude and achieve exponential growth simultaneously.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Will AI Kill Us All?</span>"
    ]
  },
  {
    "objectID": "ai-kill-us.html#what-should-we-do",
    "href": "ai-kill-us.html#what-should-we-do",
    "title": "4  Will AI Kill Us All?",
    "section": "What should we do?",
    "text": "What should we do?\nThe doomsday arguments claim that the more powerful an AI becomes, the more crucial it is to ensure proper alignment. Failure to do so can have catastrophic consequences. Some people even propose that we should place restrictions on AI research altogether, or severely limit their power to prevent them from becoming uncontrollable.\nThe most extreme doomsayers believe that there might be a point where it becomes impossible to regain control over AIs once they reach a certain level of power. Even a short time before reaching that point, they would still be on an unstoppable trajectory.\nTherefore, the argument goes, there must be a threshold, perhaps six months or three months earlier, where the AI does not possess the capability to destroy us yet but is steadily heading towards that outcome without us realizing it. By the time we realize that AI can destroy us, it will already be too powerful to stop.\nIf this is indeed the case, then it implies that we need to halt AI development well before it reaches a level of power that we consider dangerous. The most extreme members of this group argue that this point might even be today, as we simply cannot predict with certainty what will be the threshold beyond which we can no longer control them.\nIn summary, the doomsday argument is this: Since we don’t know when the catastrophic threshold will be crossed, the safest approach would be to stop developing AI today.\nHowever, there are no reliable facts today indicating the possibility of creating strong AI —as opposed to ordinary AI, which is what existing systems are called. There is a reasonable probability that there are factors that make its creation impossible in principle, primarily relating to properties of human nature that may be impossible to copy or reproduce through objectification or translation into machine code.\nIn light of all we’ve discussed, we believe that the possibility of AI leading to catastrophic events that could destroy human civilization is highly improbable. While there is a nonzero chance of AI causing such a disaster by the end of this century, this probability is very low. Similar risks exist with other issues, such as climate change, which I would argue presents a higher likelihood of civilization destruction. Additionally, traditional wars, nuclear exchanges, pandemics, and even the sudden appearance of an asteroid with six months’ notice are all potentially existential threats.\nWe believe that AI existential risk is on a similar scale as other existential risks we face as a civilization. Therefore, we don’t think it is impossible or fruitless to discuss them. However, we also don’t believe it is the most probable scenario.\nSo, what can we do with this information?\n\nThe pragmatist approach to x-risks\nAI doomers will tell you that even if you think the existential risk of AI is very low, it still entails a negative infinite utility, so you should still put all your resources into mitigating it, right?\nWell, from a pragmatic standpoint, things are not as simple. While pragmatism is but one of the many possible viewpoints to consider this problem —and not necessarily the most fruitful or correct—we want to conclude this article by pointing out what a pragmatist approach to existential risks might look like.\nMany events have a near-zero probability of happening and carry an infinitely negative consequence. For instance, there is a chance, albeit tiny, that an asteroid may collide with Earth in 2024. Unfortunately, we currently lack the means to prevent such an event. However, it would be unwise to solely focus all our efforts on averting this scenario. While significant resources are allocated to mitigating the risk of asteroid impacts, it is not the only issue we should address.\nSimilarly, there is a nonzero probability that a future pandemic could devastate humanity. Therefore, everyone must prioritize efforts to prevent the occurrence of such a catastrophic event. However, this does not imply that every resource on Earth should be solely dedicated to this cause. We should allocate sufficient resources to tackle future pandemics while considering other pressing concerns.\nThe pragmatic approach to existential threats does consider the nonzero possibility of each potential danger. But whether it be threats from AI, climate change, meteorites, pandemics, or even extraterrestrial beings destroying our world, we cannot place all our efforts into any one basket. Although all these dangers are highly improbable, they are not entirely impossible. Therefore, it is essential to thoroughly study the feasibility and potential risks associated with each threat, including those from AI.\nFurthermore, from a pragmatic perspective, it is not clear whether technology is inherently good or bad, or if our trajectory leads inevitably to destruction or transcendence. Taking an optimistic or pessimistic stance, or aligning with accelerationist or doomer ideologies, all require an epistemic commitment to beliefs that, for a pragmatist, are possibilities rather than proven truths. Therefore, it is crucial to approach this problem with importance, conducting thorough research while tempering our concerns and expectations based on the evidence and pragmatic possibilities currently available.\nWhile it may not be practical to halt AI research, as it holds tremendous potential for positive developments, it is vital to dig deeply into this technology. We should strive to understand its risks and explore ways to mitigate them using the scientific method, which has proven effective thus far. And to deepen our understanding of what we are dealing with before we take the next step. And not leave everything up to those who ignore big questions and believe that tech progress alone will solve all the problems without improving human society by ourselves.\nThe pragmatist approach is to understand we have both the power and the responsibility to shape our own future, and act according that those principles.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Will AI Kill Us All?</span>"
    ]
  },
  {
    "objectID": "risks.html",
    "href": "risks.html",
    "title": "5  The Actual Risks of AI",
    "section": "",
    "text": "Autonomous weaponry\nAutonomous weaponry, a long-term and abstract concern related to existential risks. While debating the possibility of AI leading to SkyNet scenarios, it’s essential not to overlook the potential for catastrophic military use of weapons in various aspects. The first issue arises from autonomous weapons; drones or even traditional ones like assault rifles or misguided missiles can be entirely automated using AI technology. Such fully-automated weaponry is horrifying because all forms of technology employed to kill human beings are deplorable and terrible. Even in conventional warfare, where humans kill other humans, there still exists some room for empathy and consideration despite being barbaric practices in this age. At least in such traditional warfare, an individual remains behind the trigger of a weapon which provides some space for respecting human rights, refraining from harming civilians, or innocent people.\nIn a fully autonomous weaponry case, where warfare can be completely dehumanized, there is still the possibility of preserving some level of humanity. However, if autonomous drones are deployed in battlefields without human operators, it could lead to total dissolution of the distinction between innocent and active military enemy. While semi-autonomous weapons like drones operated by humans have already shown how being behind a screen resembling a video game can cause dehumanization leading to an increase in civilian casualties, imagine the consequences when such weapons become fully automated. This extreme use case compares to that in movies like Terminator, but what’s more concerning is the current reality of AI for biochemical warfare.\nIf you can design using artificial intelligence, we can now solve biochemical problems that were considered unsolvable five years ago through tools like AlphaFold. This new capability allows us to design proteins and chemical substances at a scale previously unimaginable. However, this opens the door for the creation of highly efficient and targeted biochemical weapons capable of targeting specific genetic markers in certain populations based on their ancestry or location. The potential for such weapons to be used for racial killing is terrifying. Additionally, advancements in simulation and search algorithms will enable us to create viruses so powerful they could potentially wipe out all of humanity if released. It’s important to note that this does not involve an AI gaining motivation to do so and making the decision to eliminate humankind.\nIn this scenario, the development of viruses capable of eliminating entire populations is comparable to scenarios involving nuclear superpowers. While there are similarities –such as the potential for catastrophic destruction–, it differs in that AI-designed weapons have not yet been created with the explicit intention of causing mass extinction. Instead, humans’ own shortcomings and ambitions lead them to release deadly viruses against each other. This situation can be likened to the proliferation of nuclear weaponry among various countries, which has so far avoided mutual annihilation due to the high costs and challenges associated with producing such weapons. However, unlike nuclear arms, designing lethal viruses requires only a small nation or a single country to execute successfully.\nIn the future, it will be possible to download and print custom-designed proteins at home using a chemical printer. This technology could potentially allow small terrorist organizations to design deadly viruses and release them in crowded places like subways. While this scenario is alarming, there are currently no technical solutions to prevent it from happening, as anyone with a computer may have access to these capabilities. Unlike nuclear weapons that only superpowers can possess, bioweapons pose a greater threat because they don’t require extensive resources or expertise.\nThere’s no way to act, as if you could. The only action is banning certain things through international treaties upheld by countries and governments. Terrorist organizations won’t abide by agreements against producing chemical weapons or nuclear arms, making it an intractable dilemma for which I can see no solution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Actual Risks of AI</span>"
    ]
  },
  {
    "objectID": "risks.html#massive-workplace-disruptions",
    "href": "risks.html#massive-workplace-disruptions",
    "title": "5  The Actual Risks of AI",
    "section": "Massive workplace disruptions",
    "text": "Massive workplace disruptions\nOne of the ways in which technology can negatively impact society is by causing significant economic upheaval that is challenging for a large portion of society to adapt to. This issue is particularly evident in the widespread job displacement resulting from increased automation across various industries.\nIn the next decade, artificial intelligence is expected to reach a level where it can potentially surpass or match human capabilities in many economically viable occupations, including agriculture, manufacturing, white-collar jobs, education, science, research, and entertainment. If this scenario unfolds as projected, it is natural to experience concern about the fate of the millions of individuals whose jobs will be replaced by automation.\nThis widespread job disruption has the potential to have catastrophic effects on individuals and the broader society. It raises critical questions about how these individuals will transition to new employment opportunities and maintain their livelihoods in the face of technological advancements.\nMassive job disruption, a recurring phenomenon throughout history, has been associated with every industrial revolution. When we introduced looming machines, there was an expectation that they would improve the working conditions for women employed in sewing. However, in reality, this did not materialize. These women simply lost their jobs to the machines.\nOne of the arguments often advocated by tech enthusiasts is that the advancement of technology will inevitably result in the destruction of numerous jobs, but it will also generate entirely new job opportunities, ultimately leading to a more prosperous society. They often cite examples such as the emergence of new professions like YouTubers and internet influencers or the rapidly evolving role of AI engineers in recent years. But while technology undoubtedly creates new jobs and value, it has historically also led to an imbalance in the distribution of benefits and drawbacks.\nIt is crucial to approach this issue with caution and consideration, as it has frequently resulted in disparities between the positive and negative impacts. Technology has the potential to bring about societal profitability while simultaneously causing hardship for a significant proportion of the population. Therefore, achieving a balance where those who benefit from technological advances outweigh those who are adversely affected is vital for societal progress.\nFor those concerned about social justice and equal opportunities for all individuals to earn a livelihood, the potential consequences of significant job disruption demand attention and thoughtful consideration. Despite the potential for the creation of new jobs and overall improvement in the long and midterm, there is no guarantee that those who have been displaced from their jobs will possess the necessary skills or opportunities to transition to new roles.\nConsequently, a substantial number of individuals may struggle to leave their current positions and lack marketable skills needed to secure alternative employment. One suggested solution to this problem is Universal Basic Income (UBI), which proposes that the rise of digital intelligence or the fourth industrial revolution will generate sufficient value to provide a basic income for all, eliminating the necessity to work for survival.\nHowever, there are many, myself included, who are skeptic of the feasibility of UBI in a purely capitalistic society due to the way markets incentives work. You can argue that certain well-developed countries already produce enough value to guarantee a minimum income for everyone, yet income inequality has either persisted or worsened over the last few decades in many of these nations.\nWhile Universal Basic Income is seen as a progressive and promising concept for an increasingly industrialized and automated society, I don’t think it is obviously the natural progression of our current social and economic structures. Implementing UBI would require significant social restructuring and government intervention, which may not be embraced by many, often due to valid concerns.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Actual Risks of AI</span>"
    ]
  },
  {
    "objectID": "risks.html#informational-hazards",
    "href": "risks.html#informational-hazards",
    "title": "5  The Actual Risks of AI",
    "section": "Informational hazards",
    "text": "Informational hazards\nThe first informational risk associated with AI is disinformation or fake news that can be created through generative AI’s capacity to produce almost indistinguishable content. This technology can be used for malicious purposes such as spreading false information and convincing people of scandals related to political candidates through manipulated evidence like pictures, videos, audio recordings, and transcriptions.\nSo one use of this information is to make people think someone else did something they didn’t do. But another way it can be used is just to make people doubt whether anything is true at all. This erodes trust in institutions, including news, government, science, and other organizations. If people stop believing everything because anyone could create convincing fake content with AI, democracy will fail. It requires informed citizens who can tell truth from falsehood. Too much disinformation can lead to chaos.\nDisinformation can be intentionally spread by malevolent organizations/individuals, but there’s also an unintentional effect called polarization. As AI recommends content based on users’ preferences, it creates separate realities for everyone because people keep consuming content aligned with their beliefs. We’ve witnessed such bubbles frequently. For instance, if someone watches only a few flat Earth videos on YouTube, it may lead YouTube to assume them a believer in flat Earth theories and present them more related material.\nHowever, this isn’t necessarily caused by any ill-intentioned entity; it’s merely how recommendations operate. Recommendations for movies and other entertainment content work well because taste in films, music, or art in general is subjective. People enjoy different genres, and it’s okay to suggest similar movies based on what they’ve liked before. However, recommending news sources and experts should be done based on objective measures of quality or truthfulness rather than personal preference. Using the same algorithms for entertainment and news on platforms like YouTube and TikTok isn’t effective since these domains are inherently incompatible due to their differing values.\nTo solve this, consider more intelligent algorithms or massive human curation. This can be achieved through tagging for recommendations on entertainment channels versus educational ones with an emphasis on factual and accurate content. The complexity of this issue also raises questions about who is responsible for the curation process itself.\nSurveillance and censorship pose a major challenge, arising from two interconnected trends. Firstly, as you conduct virtually all activities online - including communication, creation and consumption - your digital footprint leaves an indelible trail. Secondly, advanced technologies enable prediction of future actions based on past behaviour, such as identifying individuals’ contacts, preferences or thought processes via their online activity. While predominantly employed for targeted ads, this capability may also facilitate dictatorial regimes. In fact, the marketing sector now comprises the planet’s largest and most pervasive surveillance network. Every sound, action, or message posted or viewed online is captured, archived and distributed among thousands of advertisers worldwide, each analysing user histories for insights regarding identity and inclination, in service of pitching products accordingly.\nThe machine can be used for surveillance, identifying dissenters, and censorship. It allows direct censorship through platform filters or indirectly by employers and governments analyzing online activity. This could lead to a dystopian society where everything done online is tracked, analyzed, scored, and punished with denied access to services, jobs, education, or imprisonment if the score falls below a certain level. Active voice and concise sentences have been used to improve grammar and punctuation while respecting tone and not adding any additional information beyond what was explicitly stated in the text.\nIn the future, a technologically advanced version of George Orwell’s 1984 dystopia is possible. With enough data, information and computing power, law enforcement can implement a “thug police” in its most sophisticated form without screens or microphones. No longer do they need to monitor what people say or type through screens or microphones; instead, they have access to personal devices like smartphones that capture everything individuals do, say, or write. This goes beyond just recorded statements as it enables authorities to predict thoughts based on behavior patterns. The worst kind of dystopia arises where nothing remains private anymore since all actions are public or available for analysis by government agencies. As one thinks, their ideas manifest themselves in conduct online, making it impossible not to express them. Consumption habits such as movies watched and length spent reading online reveal insights into hidden musings.\nThink of this in the most creative or smart way you can. For example, while browsing a legal website with no issues, if I’m the surveyor, I can add a hidden feature that shows two images quickly for only your subconscious to see. Then, by tracking how long you look at each image or read small texts, I already have enough information to predict any unusual thoughts you might be having.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Actual Risks of AI</span>"
    ]
  },
  {
    "objectID": "risks.html#exacerbating-and-perpetuating-harmful-biases",
    "href": "risks.html#exacerbating-and-perpetuating-harmful-biases",
    "title": "5  The Actual Risks of AI",
    "section": "Exacerbating and perpetuating harmful biases",
    "text": "Exacerbating and perpetuating harmful biases\nAutomation, particularly through artificial intelligence, poses a significant risk in that it stands to automate the majority of tasks that involve human judgment. One notable example is in the realm of crime, where stories have emerged of dystopic crime rating systems attempting to predict the likelihood of re-offense and the probability of bail or committing a crime again.\nAnother area heavily reliant on human judgment is job applications, where efforts have been made – though largely unsuccessful – to replace human recruiters with AI for hiring a range of positions, from white-collar roles to other job categories. Similarly, credit rating, a crucial element of the developed world’s financial system, has seen attempts to automate the system and related financial services, using AI to determine who qualifies for a loan or certain credit thresholds based on a complex web of historical data and predictive models.\nEducational evaluation, including the assessment of student essays and overall performance, also falls within the purview of automation concerns, albeit within a separate context due to its substantial impact on education as a whole.\nIn all of these scenarios, the central issue revolves around bias, presenting a formidable challenge in the implementation of automated systems.\nThese systems are trained using past human judgments and are inevitably influenced by the biases with which humans judge one another. Consequently, our financial, judicial, criminal, and job market records are rife with discrimination against minority groups, including racial and gender discrimination, discrimination against neurodivergent individuals, and bias against people with non-traditional backgrounds or education.\nThe prevalence of biases in these records greatly depends on the construction of the systems that process them. Most predictive systems today are trained using a large amount of supervised or self-supervised learning from historical data, which means that they inevitably perpetuate and encode these biases. Unfortunately, we currently lack a clear understanding of how to design a system that both performs well and effectively removes biases.\nThere is extensive research being conducted in the area of AI fairness. Many AI labs, including my own, are actively engaged in this field. Trade-offs are being considered by various researchers in order to achieve fairness within AI systems.\nWhen aiming for fairness, one approach involves regulating the system to ensure that it produces consistent or similar performance across different subsets of inputs. This includes equalizing the probability of selection among different subgroups. Various mathematical frameworks exist to define what constitutes a fair outcome, often involving the partitioning of the population into subgroups to ensure an equitable distribution of outcomes.\nConsiderations for fairness extend to the idea of protected attributes, such as race, gender, and educational background. The objective is to develop systems that are independent of these variables. However, simply omitting gender or race from the input data is not sufficient, as there are numerous proxy variables that are correlated with these attributes.\nRemoving variables correlated with gender and race may inadvertently eliminate crucial information, as these variables are often associated with important factors. For instance, factors such as educational background, childhood experiences, and personal preferences may genuinely influence performance and fairness in socially relevant ways. This underscores the complexity involved in addressing fairness in AI systems.\nThis problem presents a tremendous challenge. So far, all the solutions that I am aware of involve some trade-off of performance in order to achieve fairness. This trade-off seems to be non-negotiable, as part of the performance given away is actually due to the discrimination itself. This exacerbates existing biases and discrimination.\nWhy will this be worse with AI than it really is? Society is already unfair. AI may improve some aspects but not others. There is a concern that AI not only captures discriminating biases but exacerbates them, making them more extreme and profound. Mathematically, it has been shown that if left unchecked, a predictive model will tend to exploit these biases to their maximum potential if the sole focus is on performance, and this effect has been empirically demonstrated in numerous papers.\nIt is crucial to address these issues and prevent AI from exacerbating existing biases and discrimination. Unfortunately, bias exists within systems, and it is crucial to address and rectify this issue. It is essential to be mindful of the presence of bias and take necessary measures to prevent its unchecked proliferation.\nThe problem lies not in the technical aspect, but rather in its adoption. Making this a priority is imperative because a system that is fairer may not perform as well as one that isn’t, all other factors being equal. For example, in the context of developing systems for hiring applicants, a fairer system may yield lower performance compared to a less just one. In a purely market-driven economy, there are no inherent incentives to prioritize fairness, hence the need to inject such incentives externally, possibly through government regulation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Actual Risks of AI</span>"
    ]
  },
  {
    "objectID": "risks.html#conclusions",
    "href": "risks.html#conclusions",
    "title": "5  The Actual Risks of AI",
    "section": "Conclusions",
    "text": "Conclusions\nFrom a pragmatic perspective, it is essential to address the problems in artificial intelligence that warrant our focus. In particular, I am concerned about the for-profit military-industrial complex’s inability and lack of incentives to solve these issues. I firmly believe that government regulation and societal oversight are necessary to implement safeguards that enable us to harness the potential of artificial intelligence for the greater good, rather than for the profit of a select few, or as a tool for the benefit of the technocratic elites.\nI strongly advocate for a balanced approach to AI regulation that does not stifle innovation and development but rather complements these endeavors with safeguards to ensure responsible and ethical AI advancements. It is pivotal that we prioritize the societal implications and ethical considerations of AI, and take proactive measures to steer its development in a direction that benefits humanity as a whole.\nI believe we need not fear a sensible government regulation in the case of AI adoption, by regulating the commercial applications of AI rather than the basic research. This type of regulation is in place for many consumer products, from food to electronics to pharmaceuticals. In general, new products —whether GMOs, cars, or drugs— cannot be introduced into the market without demonstrated safety and efficacy. Similarly, AI systems should not be allowed to operate commercially if they demonstratively harm some individuals.\nThis post is my attempt to shed light into the many ways in which AI can be misused, either accidentally or on purpose, to harm some individuals or populations. The looming question remaining is of course, what can we do, technically and otherwise, to solve these issues? If you’re interested, I can dive into the active research in mitigating AI harms in a future issue.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Actual Risks of AI</span>"
    ]
  },
  {
    "objectID": "hallucinations.html",
    "href": "hallucinations.html",
    "title": "6  Reliable AI needs a New Paradigm",
    "section": "",
    "text": "What are Hallucinations in AI?\nThe term “hallucination” in the context of AI refers to the phenomenon where a large language model (LLM) or other generative AI system produces outputs that appear plausible and coherent, but do not accurately reflect reality or the intended purpose of the system. These hallucinations manifest as the generation of false, inaccurate, or nonsensical information that the AI presents with confidence, as if it were factual.\nFirst, a caveat. Unlike human hallucinations, which involve perceiving things that are not real, AI hallucinations are associated with the model producing unjustified responses or beliefs, rather than perceptual experiences. The name “hallucination” is therefore imperfect, and it often leads to mistakes as people tend to antropomorphize these models and make erroneous assumptions about how they work and the causes of these failures. However, we will stick to this name in this article because it is the prevalent nomenclature used everywhere people talk about AI. Just keep in mind we’re talking about something completely different to what the term “hallucination” means in general.\nBefore diving in the why of AI hallucinations, let’s distinguish them from other typical failures of generative models, such as out-of-distribution errors or biased outputs.\nOut-of-distribution errors occur when an AI model is presented with input data that is significantly different from its training data, causing it to produce unpredictable or nonsensical outputs. In these cases, the model’s limitations are clear, and it is evident that the input is outside of its capabilities. This is just an error of generalization which usually points to either: 1) the model’s hypothesis space is too constrained to entirely capture the actual distribution of data, or 2) the available data or training regimes are insuficient to pinpoint a general-enough hypothesis.\nHallucinations are more insidious than out-of-distribution errors because they happen within the input distribution, where the model is supposedly well-behaved. Even worse, due to the stochastic nature of generative models, hallucinations tend happen entirely randomly, which means that for the same input the model can hallucinate once out of 100 times, making it almost impossible to evaluate and debug.\nBiased outputs, on the other hand, arise when an AI model’s training data or algorithms contain inherent biases, leading to the generation of outputs that reflect those biases, such as stereotypes or prejudices. These are often not hallucinations, but realistic reproductions of the very human biases that pervade our societies: The model is producing something that reflects the reality underlying its training data. It’s just that such reality is an ugly one. Dealing with biases in AI is one of the most important challenges in making AI safe, but it is a completely different problem that we can tackle in a future issue.\nHallucinations, in contrast, involve the AI model generating information that is not necessarily biased, but completely fabricated or detached from reality. This makes the problem of detecting them far hard, because the model’s responses appear confident and coherent, and there is no obvious telltale that helps human evaluators quickly identify it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reliable AI needs a New Paradigm</span>"
    ]
  },
  {
    "objectID": "hallucinations.html#real-world-implications-of-ai-hallucinations",
    "href": "hallucinations.html#real-world-implications-of-ai-hallucinations",
    "title": "6  Reliable AI needs a New Paradigm",
    "section": "Real-World Implications of AI Hallucinations",
    "text": "Real-World Implications of AI Hallucinations\nThe occurrence of hallucinations in AI systems, and particularly in large language models (LLMs), can have significant consequences, especially in high-stakes applications such as healthcare, finance, or public safety. For example, a healthcare AI model that incorrectly identifies a malignant skin lession as benign can doom a patient. On the other hand, identifying a benign skin lesion as malignant could lead to unnecessary medical interventions, also causing harm to the patient. Similarly, in the financial sector, hallucinated outputs from an AI system could result in poor investment decisions with potentially devastating economic impacts.\nHowever, even in low-stakes applications, the insidious nature of hallucinations make then a fundamental barrier to the widespread adoption of AI. For example, imagine you’re using an LLM to generate summaries from audio transcripts of a meeting, extracting relevant talking points and actionable items. If the model tends to hallucinate once in a while, either failing to extract one key item, or worse, producing an spurious item, it will be virtually impossible for anyone to detect that without manually revising the transcript, thus rendering the whole application of AI in this domain useless.\nFor this reason, one of the key challenges in addressing the real-world implications of language model hallucinations is the difficulty in effectively communicating the limitations of these systems to end-users. LLMs are trained to produce fluent, coherent outputs that can appear plausible, even when they are factually incorrect. If the end-users of an AI system are not sufficiently informed to review the output of the system with a critical eye, they may never spot any hallucinations. This leads to a chain of mistakes as the errors from the AI system propagate upstream through the layers of decision makers in an organization. Ultimately, you could be making a very bad decision that seems entirely plausible given all the available information because the source of the error –an AI hallucination– is impossible to detect.\nThus, the development and deployment of LLMs with hallucination capabilities raises important ethical considerations. There is a need for responsible AI development practices that prioritize transparency, accountability, and the mitigation of potential harms. This includes establishing clear guidelines for testing and validating LLMs before real-world use, as well as implementing robust monitoring and oversight mechanisms to identify and address hallucinations as they arise.\nCrucially, there are absolutely zero generative AI systems today that can guarantee they don’t hallucinate. This tech is simply unreliable in fundamental ways, so every actor in this domain, from developers to users, must be aware there will be hallucinations in your system, and you must have guardrails in place to deal with the output of unreliable AIs. And this is so perverse because we are used to software just working. Whenever software doesn’t do what it should, that’s a bug. But hallucinations are not a bug of AI, at least in the current paradigm. As we will see in the next section, they are an inherent feature of the way generative models work.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reliable AI needs a New Paradigm</span>"
    ]
  },
  {
    "objectID": "hallucinations.html#why-hallucinations-happen",
    "href": "hallucinations.html#why-hallucinations-happen",
    "title": "6  Reliable AI needs a New Paradigm",
    "section": "Why Hallucinations Happen?",
    "text": "Why Hallucinations Happen?\nThere are many superficial reasons for hallucinations, from data and modelling problems, to issues with prompting. However, the underlying cause of all hallucinations, at least in large language models, is that the current language modeling paradigm used in these systems is, by design, a hallucination machine. Let’s unpack that.\nGenerative AI models, including LLMs, rely on capturing statistical patterns in their training data to generate outputs. Rather than storing explicit factual claims, LLMs implicitly encode information as statistical correlations between words and phrases. This means the models do not have a clear, well-defined understanding of what is true or false, they can just generate plausibly sounding text.\nThe reason this mostly works, is because generating plausibly sounding text has a high probabilty of reproducing something that is true, provided you trained on mostly truthful data. But large language models (LLMs) are trained on vast corpora of text data from the internet, which contains inaccuracies, biases, and even fabricated information. So these models have “seen” many true sentences and thus picked up correlations between words that tend to generate true sentences, but they’ve also seen many variants of the same sentences which are slightly or even entirely wrong.\nSo one of the primary reasons for the occurrence of hallucinations is the lack of grounding in authoritative knowledge sources. Without a strong foundation in verified, factual knowledge, the models struggle to distinguish truth from falsehood, leading to the generation of hallucinated outputs. But this is far from the only problem. Even if you only train on factual information—assuming there would be enough of such high-quality data to begin with—the statistical nature of language models make them susceptible to hallucinate.\nSuppose your model has only seen truthful sentences, and learned the correlations between words in these sentences. Imagine there are two very similar sentences, both factually true, that differ in just a couple of words –maybe a date and a name, for example “Person A was born in year X” and “Person B was born in year Y”. Given the way these models work, the probability of generating a mixed-up sentence like “Person B was born in year X” is only slightly smaller than generating either of the original sentences.\nWhat’s going on here is that the statistical model implicitely assumes that small changes in the input (the sequence of words) lead to small changes in the output (the probability of generating a sentence). In more technical terms, the statistical model assumes a smooth distribution, which is necessary because the amount of data the model needs to encode is orders of magnitude bigger than the memory (i.e., number of parameters) in the model. Thus, the models has to compress the training corpus, and compression implies loosing some of the information.\nIn other words, statistical language models inherently assume that sentences very similar to what they have seen in the training data are also plausible sentences. They encode a smooth representation of language, and that’s fine, as long as you don’t equate plausible with factual. See, these models weren’t designed with factuality in mind. They were originally designed for tasks like translation, where plausibility and coherence are all that matters. It’s only when you turn them into answering machines that you run into a problem.\nThe problem is there is nothing smooth about facts. A sentence is eithre factual or not, there are no degrees of truthfulness —for the most part; let’s not get dragged into epistemological discussions here. But LLMs cannot, by design, define a strict frontier between true and false sentences. All the frontiers are fuzzy, so there is no clear cutoff point where you can say, if a sentence has less than X value of perplexity then it is false. And even if you could define such a threshold, it would different for all sentences.\nYou may ask why can’t we avoid using this “smooth” representation altogether. The reason is that you want to generate sentences that are not in the training set. This means you need to somehow guess that some sentences you have never seen are also plausible, and guessing means you have to make some assumptions. The smooth hypothesis is very reasonable —and computationally convenient, as these models are trained with gradient descent, which requieres smoothness in the loss function— again, as long as you don’t care about factuality. If you don’t compress the training data in this smooth, lossy way, you will simply I can’t wait for you to start training your own chatbot and building exciting applications with LLMs!not be able to generate novel sentences at all.\nIn summary, this is the underlying reason why the current paradigm of generative AI will always hallucinate, no matter how good is your data and how ellaborated are your training procedures or guardrails. The statistical language modeling paradigm, at its core, is a hallucination machine. It is concocting plausibly-sounding sentences by mixing and matching words that is has seen together in similar contexts in the training set. It has no inherent notion of whether a given sentence is true or false. All it can tell is that it looks like sentences that appear in the training set.\nNow, a silver-lining could be this idea that even if some false sentences will unavoidably be generated, we can train the system to minimize their ocurrence by showing it lots and lots of high quality data. That is, can we push the probability of a hallucination to a sufficiently low value that, in practice, almost never happens? Recent research suggests that if there is a sentence that can be generated at all, no matter how low its base probability, then there is a prompt that will generate it with almost 100% certainty. This means that if we introduce malicious actors into our equation, we can never be sure our system can’t be jailbroken.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reliable AI needs a New Paradigm</span>"
    ]
  },
  {
    "objectID": "hallucinations.html#mitigating-hallucinations-in-ai",
    "href": "hallucinations.html#mitigating-hallucinations-in-ai",
    "title": "6  Reliable AI needs a New Paradigm",
    "section": "Mitigating Hallucinations in AI",
    "text": "Mitigating Hallucinations in AI\nSo far we’ve argued that hallucinations are inherently impossible to eliminate completely. But this doesn’t mean we can’t do anything about it in practice. I want to end this article with a short summary of mitigation approaches that are being used today by researchers and developers.\nOne key strategy is to incorporate external knowledge bases and fact-checking systems into the AI models. By grounding the models in authoritative, verified information sources, the risk of generating fabricated or inaccurate outputs can be reduced.\nResearchers are also exploring ways to develop more robust model architectures and training paradigms that are less susceptible to hallucinations. This may involve techniques like increasing model complexity, incorporating explicit reasoning capabilities, or using specialized training data and loss functions.\nEnhancing the transparency and interpretability of AI models is also crucial for addressing hallucinations. By making the models’ decision-making processes more transparent, it becomes easier to identify and rectify the underlying causes of hallucinations.\nAlongside these technical approaches, the development of standardized benchmarks and test sets for hallucination assessment is crucial. This will enable researchers and developers to quantify the prevalence and severity of hallucinations, as well as compare the performance of different models in this regard. Thus, if you can’t completely eliminate the problem, at least you can quantify it and make informed decisions about where and when it is safe enough to deploy a generative model.\nFinally, addressing the challenge of hallucinations in AI requires an interdisciplinary approach, involving collaboration between AI researchers, domain experts, and authorities in fields like scientific reasoning, legal argumentation, and other relevant disciplines. By fostering cross-disciplinary knowledge sharing and research, the understanding and mitigation of hallucinations can be further advanced.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reliable AI needs a New Paradigm</span>"
    ]
  },
  {
    "objectID": "hallucinations.html#conclusion",
    "href": "hallucinations.html#conclusion",
    "title": "6  Reliable AI needs a New Paradigm",
    "section": "Conclusion",
    "text": "Conclusion\nThe issue of hallucinations in AI systems, particularly in large language models, poses a significant challenge for the reliable and trustworthy deployment of these powerful technologies. Hallucinations, where AI models generate plausible-sounding but factually inaccurate outputs, can have serious consequences in high-stakes applications and undermine user trust.\nThe underlying causes of hallucinations stem from the fundamental limitations of current language modeling approaches, including the lack of grounding in authoritative knowledge sources, the reliance on statistical patterns in training data, and the inherent difficulty in reliably distinguishing truth from falsehood using statistics alone. These limitations highlight the need for more advanced techniques that can better understand the nuances of language and factual claims, probably involving some fundamental paradigm shifts in machine learning that take us beyond what pure statistical models can achieve.\nMitigating hallucinations in practice requires a multifaceted approach, involving the incorporation of external knowledge bases and fact-checking systems, the development of more robust model architectures and training paradigms, the leveraging of human-in-the-loop and interactive learning strategies, and the improvement of model transparency and interpretability. Standardized benchmarks and test sets for hallucination assessment, as well as interdisciplinary collaboration between AI researchers, domain experts, and authorities in related fields, will be crucial for advancing the understanding and mitigation of this challenge.\nI hope this article has given you some food for thought. We went a bit deep into the technical details of how generative models work, but that is necessary to understand why these issues are so hard to solve. If you like this type of article, please let me know with a comment, and feel free to suggest future topics of your interest.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reliable AI needs a New Paradigm</span>"
    ]
  },
  {
    "objectID": "reasoning.html",
    "href": "reasoning.html",
    "title": "7  Large Language Models Cannot Reason",
    "section": "",
    "text": "What is reasoning (in AI)?\nWhen we AI folks claim LLMs cannot reason, we are not talking about any abstract, philosophical sense of the word “reason”, nor any of the many psychological and sociological nuances it may entail. No, we have a very specific, quantifiable, simplified notion of reasoning that comes straight out of math.\nReasoning is, simply put, the capacity to draw logically sound conclusions from a given premise. In math, there are two main reasoning types or modes: deduction and induction. Induction is somewhat problematic because it involves generalizing claims from specific instances, and thus, it requires some pretty strong assumptions. In contrast, deduction is very straightforward. It is about applying a finite set of logical inference rules to obtain new provably true claims from existing true claims. It is the type of reasoning that mathematicians do all day long when proving new theorems.\nThus, when I say LLMs cannot reason, I’m simply saying there are—sometimes pretty simple—deduction problems they inherently cannot solve. It is not a value judgement, or an opinion based on experience. It is a straightforward claim provable from the definition of reasoning—understood as deductive reasoning—and the inherent limitations of LLMs given their architecture and functionality.\nIf this is clear, let’s move on to the counterarguments to this claim.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Language Models Cannot Reason</span>"
    ]
  },
  {
    "objectID": "reasoning.html#why-llms-cant-reason",
    "href": "reasoning.html#why-llms-cant-reason",
    "title": "7  Large Language Models Cannot Reason",
    "section": "Why LLMs can’t reason",
    "text": "Why LLMs can’t reason\nOne significant limitation of language models regarding reasoning is their stochastic nature. These models generate outputs based on probabilistic predictions rather than deterministic logical rules. This means that even a well-structured prompt can yield different responses on different occasions due to the randomness in their decision-making process.\nConsequently, an LLM might arrive at a wrong conclusion purely by chance, leading to inconsistencies in reasoning. For example, when asked to solve a mathematical problem or make a logical inference, the model’s response may vary significantly depending on the random seed used during generation, undermining trust in its reasoning capabilities.\nGranted, you may set the temperature to zero effectively forcing the model to fix the output for a given input. But this output is still probabilistic, you’re just sampling the most likely continuation. The fact that the mapping between input and output hinges on a probabilistic distribution that encodes correlations between elements in the input and corresponding elements in the output is already suspicious. It would be very weird, although not impossible, that we just happened to converge on the right probability distribution that produces the correct output for every input, in terms of logical deduction rules.\nHowever, this limitation is still not definitive. But it gets worse.\nBy design, large language models spend a fixed amount of computation per token processed. This means the amount of computation an LLM does before it produces the first output token is a function of just two numbers: the input size and the model size. So, if you ask an LLM to produce a yes or no question for a logical puzzle, all the “thinking” the model can do is some fixed—albeit huge—number of matrix multiplications that only depend on the input size. See where I’m going here?\nNow, consider that you have two different logical puzzles with the same input size, i.e., the same number of tokens. But one is an easy puzzle that can be solved with a short chain of deduction steps, while the other requires a much higher number of steps. Here is the kicker: any LLM will spend exactly the same amount of computation in both problems. This can’t be right, can it?\nA basic result in computational complexity theory is that some problems with very small inputs seem to require an exponentially high computational cost to be solved correctly. These are NP-complete problems, and most computer scientists believe there are no efficient algorithms to solve them. Crucially, a huge number of reasoning problems fall in this category, including the most basic logical puzzle of all—determining if a given logical formula can be satisfied.\nWhen faced with an instance of an NP-complete problem, an LLM will produce an answer after a fixed amount of computation defined solely by the input size. Now, by sheer size, some larger models might just spend enough computation to cover many smaller instances of NP-complete problems. As it happens, a huge constant function can be larger than an exponential function for smaller inputs. But crucially, we can always find instances of NP-complete problems that require, even in principle, a sufficiently large amount of computation to surpass the computational capacity of any LLM, no matter how big.\nBut this means something even more profound. Ultimately, LLMs are not Turing-complete systems but essentially very large finite automata. While they can handle a wide range of tasks and produce outputs that appear sophisticated, their underlying architecture limits the types of problems they can solve.\nTuring completeness is the ability of a computational system to perform any computation given sufficient time and resources. Modern computers and many seemingly simple systems, such as cellular automata, are Turing complete systems. But LLMs are not, ironically.\nThe reason is simple. We know from computability theory that any Turing complete system must be able to loop indefinitely. There are some problems—some reasoning tasks—where the only possible solution is to compute, and compute, and compute until some condition holds, and the amount of computation required cannot be known in advance. You need potentially unbounded computation to be Turing complete.\nAnd this is the final nail in the coffin. LLMs, by definition, are computationally bounded. No matter their size, there will always be problem instances—which we may not be able to identify beforehand—that require more computation than is available in the huge chain of matrix multiplications inside the LLM.\nThus, when LLMs seem to tackle complex reasoning problems, they often solve specific instances of those problems rather than demonstrating general problem-solving capabilities. This might just be enough for practical purposes—we may never need to tackle the larger instances—but, in principle, LLMs are incapable of truly open-ended computation, which means they are incapable of true reasoning. Case closed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Language Models Cannot Reason</span>"
    ]
  },
  {
    "objectID": "reasoning.html#counterarguments",
    "href": "reasoning.html#counterarguments",
    "title": "7  Large Language Models Cannot Reason",
    "section": "Counterarguments",
    "text": "Counterarguments\n\nArgument 1: Humans Also Have these Limitations\nThe most common criticism I received against the assertion that LLMs cannot reason is that, sure, LLMs cannot reason, but neither can humans, right? I mean, humans can be stupendously irrational. But this argument is flawed on many levels, so let’s unpack it.\nFirst, while it is true that humans can make errors in reasoning, the human brain definitely possesses the capacity for open-ended reasoning, as evidenced by the more than 2000 years of solid math we have collectively built. Moreover, all college students—at least in quant fields—at some point have to solve structured problem-solving exercises that require them to apply logical reasoning to arrive at correct conclusions, such as proving theorems. So, while humans can be pretty stupid at times, we are certainly capable of the most rigorous reasoning when trained to do so.\nBut even more importantly, this assertion is a red herring. Why the fact humans can’t do something immediately makes it ok for a piece of technology to suck at it? Imagine we did this with all our other tech. Sure, that airplane fell down and killed 300 people, but humans can’t fly, so there’s that. Or yes, that submarine imploded, but humans can’t breathe underwater. Or that nuclear power plant melted, but humans can’t stand 3000 degrees of heat, so what’s the big deal?\nNo, we don’t do that. We compare any new piece of technology with our current best solution, and only if the new thing improves upon the old—at least on some metrics—do we consider it worthwhile.\nGranted, we often compare AI capabilities to human capabilities, but this is only because humans are the gold standard for the types of problems we often want AI systems to solve. So we compare LLM’s capacity to generate creative stories with our best writers, and we compare LLMs’ capacity for open-ended dialogue or for emphatic customer assistance with humans because there is nothing out there better than humans at these tasks.\nHowever, there are well-established systems—such as traditional SAT solvers—that excel in structured logical deduction and reasoning tasks. These systems are designed with rigorous validation mechanisms that ensure correctness and reliability in their outputs. They are basically flawless and incredibly fast. So, instead of comparing LLMs to humans in deductive reasoning, let’s compare them with the best solution we currently have for this problem. And there, LLMs definitely suck.\n\n\nArgument 2: Randomness is a Feature, Not a Bug\nThe second most common criticism I received was regarding the stochastic nature of language models. To recap, I claim that since LLMs generate tokens in a probabilistic fashion—which is a fundamental feature of the paradigm—, their output is inherently unreliable when you require absolute accuracy instead of versatility.\nA lot of people correctly argued that, in fact, randomness is essential in problem-solving and a crucial feature of many of the same SAT solvers against I pretend to compare LLMs. How hypocritical of me, they claim, to posit randomness as a limitation when the most effective deductive reasoning algorithms we have are essentially random. And this is true, but only partially, and it makes all the difference. So let me explain.\nRandomness plays a vital role in many computational problem-solving techniques, particularly in search algorithms for hard (read NP-complete or NP-hard) problems. Modern SAT solvers, for example, often employ randomized search strategies to efficiently explore vast solution spaces. By introducing randomness into the search process, these solvers can escape local optima and discover satisfactory solutions more quickly than deterministic methods might allow. This ability to leverage randomness is a powerful tool in the arsenal of computational techniques, enabling systems to tackle complex problems that would otherwise be intractable.\nHowever—and here comes the crucial difference—using randomness in the search process does not imply that the entire reasoning process is inherently unreliable. Randomness is confined to the search phase of problem-solving, where it helps identify potential solutions—potential reasoning paths. However, once a candidate solution is found, a deterministic validation phase kicks in that rigorously checks the correctness of the proposed reasoning path.\nThe distinction between the search and validation phases is paramount in understanding how randomness contributes to effective problem-solving in general. During the search phase, algorithms may employ random sampling or other stochastic methods to explore possibilities and generate potential solutions. This phase allows for flexibility and adaptability, enabling systems to navigate complex landscapes of potential answers.\nHowever, once a potential solution has been identified, it must undergo a validation process that is grounded in deterministic logic. This validation phase involves applying established rules and principles to confirm that the proposed solution meets all necessary criteria for correctness. As a result, any solution that passes this validation step can be confidently accepted as valid, regardless of how it was generated in the first place.\nYou can have millions of monkeys typing in a typewriter, and at some point, one of them will randomly produce Romeo and Juliet, but only Shakespeare can filter the garbage from the gold and decide which pamphlet to publish.\nThat silly metaphor means that randomness is good for exploring hypotheses but not for deciding which one to accept. For that, you need a deterministic, provably correct method that doesn’t rely on probabilities—at least if you want to solve the problem exactly.\nHowever, in stark contrast to traditional problem-solving systems like SAT solvers, LLMs lack a robust validation mechanism. While they can generate coherent and contextually relevant responses based on probabilistic reasoning, some of which may be correct reasoning chains, they do not possess a reliable method for verifying the accuracy of those outputs. The verification process is also stochastic and subject to hallucinations, rendering it utterly unreliable.\nSo, since LLMs evaluate their own outputs using the same probabilistic reasoning they employ for generating them in the first place, there is an unavoidable risk that incorrect conclusions will be propagated as valid responses. The monkeys are the also the editors.\n\n\nArgument 3: LLMs Can Be Turing-Complete\nThe final argument I want to address is the notion that LLMs can be made Turing-complete by duct-taping them with some Turing-complete gadget. Here’s a brief recap of what this means.\nLLMs have a fixed computational budget—a fixed number of matrix multiplications they perform per input token. This means there are problems that are inherently outside the realm of what they can solve. These problems fall into two categories.\nFirst, NP-Complete problems—such as the very straightforward problem of determining whether a logical formula is valid—are a class of decision problems for which no known polynomial-time solutions exist. Moreover, most experts believe no such algorithm can exist. Thus, these problems probably require an exponential amount of computation for sufficiently large instances. Thus, given the fixed computational budget of LLMs, no matter how big your stochastic parrot, there will always be a logical formula that is simply to large for it to solve.\nOn the other hand, we have semi-decidable problems, those for which an algorithm can confirm a solution if one exists but may run indefinitely if no solution is found. For these problems, we simply have no option but to keep searching for a potentially unbounded amount of time. And since LLMs are computationally bounded, there are solvable problem instances that simply would require more computing steps than the LLM can produce.\nNow, all of the above is clear to anyone who even superficially understands how LLMs work. However, a common argument posited by critics is that LLMs can be rendered Turing complete by integrating them with external tools, such as code generators or general-purpose inference engines, or even easier, let’s wrap it in a recursive procedure that can simply call the LLM as many times as necessary.\nAnd this is true. You can trivially make an LLM Turing-complete, in principle, by duct-taping it with something that is already Turing-complete. You can also build a flame thrower with a bamboo stick, some duct tape, and a fully working flame thrower.\nHowever, simply making LLMs Turing complete in principle does not guarantee that they will produce correct or reliable outputs. The integration of external tools introduces complexity and potential points of failure, particularly if the LLM does not effectively manage interactions with these tools.\nThe problem is, when you combine stochastic output—prone to hallucinations—with external tools that require precise inputs, you get LLMs that, in principle, have access to all the resources they may need but are incapable of using them reliably.\nWhen relying on external systems for reasoning tasks—for example, having your LLM call a SAT solver when necessary—it is crucial that LLMs can consistently identify the appropriate tool to use and provide it with the correct arguments. However, due to their probabilistic nature and susceptibility to hallucinations, LLMs struggle to do so reliably. And even if they successfully invoke an external tool, there is no guarantee that they will interpret or apply the tool’s output correctly in their reasoning process.\nSo, Turing-incompleteness or bounded computation may not be a knockout argument on its own, but when combined with the other inherent limitations of LLMs—crucially, their unreliability—it is clear there are no guarantees even the most advanced models won’t fail to solve some reasoning task.\nAnd here is the final kicker: approximate reasoning is not good enough. If the LLM fails one out of every million times to produce the right deduction, that still means the LLM cannot reason. For all practical purposes, you may be happy with a model that gets it right 9 out of 10 or 99 out of 100, but in mission-critical tasks, nothing short of sound, reliable reasoning is good enough.\nAnd that’s the claim: LLMs are incapable, by design, of sound reasoning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Language Models Cannot Reason</span>"
    ]
  },
  {
    "objectID": "reasoning.html#improving-llm-reasoning-skills",
    "href": "reasoning.html#improving-llm-reasoning-skills",
    "title": "7  Large Language Models Cannot Reason",
    "section": "Improving LLM reasoning skills",
    "text": "Improving LLM reasoning skills\nHowever, we need not throw the hat here. Researchers and practitioners have explored several innovative strategies, including Chain of Thought prompting, self-critique mechanisms, and integrating external tools to improve the reasoning skills of large language models.\nCoT prompting encourages LLMs to articulate their thought processes, allowing them to break complex problems into manageable steps and improve their accuracy in reasoning tasks. On the other hand, self-critique aims to refine outputs through an internal evaluation process, yet it has shown mixed effectiveness in meaningfully correcting errors. Additionally, incorporating external tools such as reasoning engines and code generation systems can significantly augment the LLMs’ capabilities by providing structured logic and formal verification.\nHowever, each approach has its own set of challenges, and their potential and limitations in fostering true reasoning abilities within LLMs need to be carefully examined.\n\nChain of Thought\nChain-of-thought prompting has emerged as a promising technique for enhancing the reasoning capabilities of large language models. By guiding models to articulate intermediate reasoning steps before arriving at a final answer, CoT prompting helps decompose complex problems into manageable parts. This method has improved performance across various reasoning tasks, such as arithmetic and commonsense reasoning.\nCoT prompting instructs LLMs to break down complex problems into simpler, sequential steps and then tackle each step independently. This structured approach enables the model to tackle each component individually, improving response accuracy and precision. Studies have shown that this technique can significantly boost performance on reasoning tasks, particularly when the model has a sufficient number of parameters (around 100 billion) to use the benefits of CoT prompting effectively.\nBy encouraging models to articulate their thought processes, CoT prompting taps into the extensive pool of knowledge that LLMs acquire during training. This mechanism helps models apply relevant information more effectively, addressing their inherent difficulties with logical reasoning and problem-solving.\nAdditionally, CoT makes the LLM “think harder” in the sense it forces the model to produce what we can consider “internal thought” tokens. Thus, we may view it as a way to produce additional computation on the input before deciding on the response.\nHowever, despite its advantages, CoT prompting remains insufficient for several reasons.\nThe effectiveness of CoT prompting is highly contingent on the quality and diversity of the prompts used. If the examples provided are not representative or sufficiently varied, the model may struggle to generate coherent reasoning chains, leading to suboptimal performance. This reliance on effective prompt engineering can limit the technique’s scalability and generalizability.\nAnd again, the stochastic nature of LLMs means that even with CoT prompting, outputs can vary significantly across different runs due to randomness in generation processes. This variability can lead to inconsistent reasoning outcomes, undermining the reliability of the model’s responses.\nUltimately, CoT extends the computation budget by a finite amount. Unless we try some cyclic scheme where the LLM is prompted to continue thinking, potentially indefinitely, until satisfied, their fundamental limitation on Turing incompleteness remains.\n\n\nSelf-critique\nAnother intuitive approach to improving reasoning is self-critique, which involves evaluating and refining an LLM’s responses with the same model, using prompts that instruct the model to read its previous output, highlight potential errors, and try to correct them. A form of after-the-fact chain-of-thought, if you might. However, recent research has highlighted significant limitations in the effectiveness of this self-critique capability.\nWhile LLMs can generate multiple ideas and attempt to critique their initial outputs, studies indicate that they cannot often meaningfully self-correct. The assumption that verification of correctness should be easier than generation—a fundamental idea in computational complexity theory—does not seem to hold true, in general, for LLMs. This is particularly problematic in reasoning tasks where the model struggles to adequately assess its outputs’ accuracy. For example, if an LLM generates a flawed answer, its attempt to critique and revise it can lead to further errors rather than improvements.\nResearch shows that self-correction techniques in LLMs are heavily contingent on the availability of external feedback. In many cases, LLMs perform better when they have access to an external verifier or additional context rather than relying solely on their internal reasoning capabilities. For example, when solving complex problems, such as graph coloring or planning tasks, LLMs often fail to produce reliable solutions without external guidance.\nInterestingly, attempts at self-critique can sometimes degrade performance rather than enhance it. Studies have shown that when LLMs engage in self-critique without external validation, they may generate false positives or incorrect conclusions. If you push harder, you can easily fall into a cycle of self-reinforcement of invalid or erroneous arguments, making the LLM increasingly more certain despite it getting worse and worse.\n\n\nExternal tools\nIntegrating external tools, such as reasoning engines or code generation systems, into large language models represents a promising—and, for me, the only really viable—approach to enhancing their reasoning capabilities.\nConnecting LLMs to external reasoning engines or logical inference tools makes it possible to augment their reasoning capabilities significantly. These tools can handle complex logical deductions, mathematical computations, or even domain-specific knowledge that the LLM might not possess inherently. This integration allows for more accurate and reliable outputs, as the external tools can apply formal logic and structured reasoning that LLMs typically struggle with.\nSimilarly, external code generation systems enable LLMs to produce executable code for specific tasks. This capability can streamline software development processes and improve efficiency in generating functional code snippets. The external systems can provide rigorous checks and balances that help ensure the correctness of the generated code.\nBy leveraging these external resources, LLMs can potentially overcome some of their inherent limitations in logical reasoning and problem-solving. For starters, an external inference engine will be Turing-complete, so we scratch that problem down, right?\nNot so fast. Unfortunately, this approach has many challenges, particularly regarding the LLM’s ability to generate the correct input for function calls or code execution. It all circles back to the original sin of LLMs: stochastic output.\nFirst, the effectiveness of function calling or code generation hinges on the model’s ability to accurately interpret a task and generate appropriate inputs. If the model misinterprets the requirements or generates vague or incorrect prompts, the external tool may produce erroneous outputs or fail to execute altogether. This reliance introduces a potential failure point where the model’s limitations in understanding context and intent become apparent.\nMany reasoning tasks require a nuanced understanding of logic and context that may exceed the capabilities of language models. For instance, when generating inputs for a logical inference engine, the model must understand the problem and articulate it in a way that aligns with the system’s requirements. If the model fails to capture these nuances, it may lead to incorrect deductions or ineffective reasoning processes.\nTranslating text into code or structured queries makes it more complex and can undermine reasoning capabilities. This conversion requires programming syntax and logic knowledge that may not be intuitive for an LLM trained primarily in natural language data. Mistakes in this translation can spread to the external system, causing more errors.\nWhile external tools can, in principle, improve the reasoning capabilities of an LLM by providing structured logic and formal verification, they cannot compensate for LLMs’ basic limitations in generating precise inputs. Therefore, there is no formal guarantee that the outputs from this integration will be logically sound or appropriate for the context, simply because of the age-old adage: garbage in, garbage out.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Language Models Cannot Reason</span>"
    ]
  },
  {
    "objectID": "reasoning.html#conclusions",
    "href": "reasoning.html#conclusions",
    "title": "7  Large Language Models Cannot Reason",
    "section": "Conclusions",
    "text": "Conclusions\nThe purpose of this article is to convince you of two claims:\n\nLarge Language Models currently lack the capability to perform a well-defined form of reasoning that is essential for many decision-making processes.\nWe currently have absolutely no idea how to solve this in the near future.\n\nThis matters because there is a growing trend to promote LLMs as general-purpose reasoning engines. As more users begin to rely on LLMs for important decisions, the implications of their limitations become increasingly significant. At some point, someone will trust an LLM with a life-and-death decision, with catastrophic consequences.\nMore importantly, the primary challenges in making LLMs trustworthy for reasoning are immense. Despite ongoing research and experimentation, we have yet to discover solutions that effectively bridge the gap between LLM capabilities and the rigorous standards required for reliable reasoning. Currently, our best efforts in this area are nothing but duct tape—temporary fixes that do not address the underlying limitations of the stochastic language modeling paradigm.\nNow, I want to stress that these limitations do not diminish the many other applications where LLMs excel as stochastic language generators. In creative writing, question answering, user assistance, translation, summarization, automatic documentation, and even coding, many of the limitations we have discussed here are actually features.\nThe thing is, this is what language models were designed for—to generate plausible, human-like, varied, not-necessarily-super-accurate language. The whole paradigm of stochastic language modeling is optimized for this task, and it excels at it. It is much better than anything else we’ve ever designed. But when we ask LLMs to step outside that range of tasks, they become brittle, unreliable, and, worse, opaquely so.\nThe emergence of models like OpenAI’s o1, which boasts impressive reasoning abilities, may seem like a significant step forward. However, this approach does not represent a fundamentally new paradigm in logical reasoning with LLMs. Deep down, this is “just” a way to explicitly incorporate chain of thought prompting in a fine-tuning phase and teach the model via reinforcement learning to select mostly coherent paths of deduction.\nThus, while definitely an impressive technical and engineering feat, o1 (terrible name) —and any future models based on the same paradigm— will continue to share the same core limitations inherent to all LLMs, only mitigated using some clever tricks. While they may excel in certain contexts, caution must be exercised in interpreting their outputs as definitive reasoning.\nIf LLMs are to fulfill even some of our highly unrealistic expectations for them, we must prioritize solving the challenge of provably correct reasoning. Until then, all we have is a stochastic parrot—a fun toy with some interesting use cases but not a truly transformative technology.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Large Language Models Cannot Reason</span>"
    ]
  },
  {
    "objectID": "alignment.html",
    "href": "alignment.html",
    "title": "8  Why AI Alignment is So Hard",
    "section": "",
    "text": "The AI alignment problem\nWhy Is AI Alignment the Hardest Problem in Computer Science? In this section, we will explore why AI safety, specifically AI alignment, is potentially the most difficult challenge in engineering.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Why AI Alignment is So Hard</span>"
    ]
  },
  {
    "objectID": "alignment.html#the-ai-alignment-problem",
    "href": "alignment.html#the-ai-alignment-problem",
    "title": "8  Why AI Alignment is So Hard",
    "section": "",
    "text": "Defining AI alignment\nAI alignment involves ensuring that an artificial intelligence system behaves according to a user’s expectations, preferences, or values. The interpretations can vary depending on context, but in general, the expectation is that a system will perform aligned with the user interests, that is, in a way that satisfies the user actual needs and wants, and not just some superficial approximation of them.\n\n\nThe need for alignment in Artificial Intelligence\nFirst, lets question why alignment is necessary in artificial intelligence but not in other high-tech products engineering tools such as cars, planes, or rockets. The primary reason lies in the level of complexity and the nature of the interfacing when dealing AI systems compared to other engineering tools.\n\nWhat makes AI different from other high-end technologies we have?\nThe more advanced a tool, the more you can focus on telling it what you want to do instead of how to do it. For example, with simple tools like a hammer, you control every action. With complex tools like cars, you still don’t say “reach the destination” –at least not yet. Instead, you perform some mid-level actions like steering and accelerating, which in turn translate to lower-level actions like moving pistons.\nThat is, as the tool becomes more advanced, the way you use it becomes closer that what you want and farther from what the tool must do to achieve that objective. Artificial intelligence lies in the declarative end, the point where you only tell the tool your end goal and let it figure out all the steps to achieve that. Actually, we could make the case that AI can be defined precisely as the field dedicated to making tools that do what you want.\nConsider driving a car –a regular car, not a self-driven one. To get where yow want, you need the car to steer in the right direction and accelerate or brake at the right times. That is, your high-level objective –getting somewhere– is decomposed into many low-level instructions. You cannot simply ask the car to drive itself –again, a traditional car. You have to steer the car and accelerate it. So the system that is translating a high-level, perhaps abstract instruction like “get me home fast but safely” into precise low-level instructions, is you.\nNow contrast this with a self-driven car. You just say “get me home” –the “fast but safely” part is assumed implicitely–, and the AI system in the car has to “understand” this high-level instruction and decompose it the myriad of low-level instructions that actually make the car do the thing you want it to do.\nBut here is the crucial part, “get me home” encodes a far larger set of assumptions than what you usually imagine, and there is an infinite way in which an AI agent could be said to have fullfilled that request without actually doing what you intended it to do.\nWhen you say “get me home” to a human taxi driver, they usually implicitely assume you’re also asking the following: - do not produce me any physical or psycological harm; - get there reasonably fast, - but do not drive carelessly; - take the fastest route if possible, - but take a detour if its necessary even if it costs me a bit more; - do not engage in uncomfortable conversations, - but do engage in reasonably well-mannered conversations, - or leave me along altogether, depending on my mood; - do not harm any pedestrians or animals, - but if you must harm an animal to avoid a fatal or very dangerous accident, please do; - …\nThese are all reasonable assumptions that any human knows from common sense, because we all share a common understanding of what it means to live and act in the human world. But an AI doesn’t come with that common sense preprogrammed.\nIn fact, common sense reasoning seems to be one of the hardest skills for modern AI to acquire, at least in part because by the virtue of it being “common”, which means we don’t have large corpora of explicit examples of this type of reasoning, like we have for the more specialized skills.\nAnd that is the reason we need alignment. When we tell a tool what we want instead of how to do it, we need the tool to interpret that want in a context that is full of assumptions, restrictions, and trade-offs which are often implicit. Alignment means having an AI system apply the right implicit context, and find the solution to our request that is, as the name implies, more closely aligned to what we really want instead of just any solution that superficially fits the explicit request.\nThe crucial reason alignment is hard is due to the interplay between two critical parts of the AI equation: the inherent complexity of the world and the unavoidable brittleness of the way we model it.\nLet’s break it down.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Why AI Alignment is So Hard</span>"
    ]
  },
  {
    "objectID": "alignment.html#why-ai-alignment-is-hard",
    "href": "alignment.html#why-ai-alignment-is-hard",
    "title": "8  Why AI Alignment is So Hard",
    "section": "Why AI alignment is hard",
    "text": "Why AI alignment is hard\nMany reasons make AI alignment one of the hardest problem a system designer can face. Some of those reasons involve our incapability to produce a robust enough description of the task we want to solve –that is, we cannot fully describe the context and all the implicit assumptions and restrictions that apply in that context. These reasons are related to the nature of the problem itself –getting any intelligence agent to do what you want is intrinsically hard; if you’re a parent, you know exactly what I mean. Other reasons are related with the nature of the solution we currently have, that is, systems built with machine learning, trained on imperfect datasets to maximize imperfect proxy metrics.\nThese are interrelated but separate challenges, so let’s take them one at a time.\n\nImplicit contexts\nWhen using a regular tool –like a hammer, a calculator, or Excel– you have an end goal in mind, but the tool doesn’t need to understand that goal; it just needs to follow your precise instructions. However, when working with AI, there are many assumptions about how the world works that aren’t explicitly described in your instructions.\nFor instance, if you tell an advanced AI to make coffee, there are numerous implicit restrictions: don’t destroy the coffee machine, don’t harm any animals, don’t walk through walls, etc. Humans generally understand these unstated rules because we share a common understanding of the world. So there is a significant difference between systems that require specific instructions on how to perform tasks and those that simply need to be told what tasks to accomplish.\nWhen you want to tell a system what to do instead of how to do it, you must be very precise in specifying everything it needs to know. In a restricted domain, the constraints may be simple enough to be explicitly encoded or learned from data. For example, in a factory setting, a robotic arm is simply physically incapable of destroying the world, so it doesn’t need to know much about anything outside the narrow task of car painting.\nHowever, training systems for open-ended decision-making in the real world is far more complex. It’s hard to imagine a training scenario that is as intricate as real life. Gaining all the necessary experience to understand the human world like humans do would require something like raising a child from scratch. And the majority of assumptions in those contexts can’t be learned from data, because we simply don’t have training data for “how to be a human”.\n\n\nUnclear trade-offs\nHowever, the implicit context problem carries an even bigger challenge. While many of the things an AI must care about implicitly are restrictions –e.g., do not kill the passenger– the hardest problem is when they involve tradeoffs instead.\nThis is a fundamental issue built into most optimization problems in the real world. On one hand, you want a system to achieve its objective as effectively as possible. On the other hand, you want it to do so with minimal side effects. These two goals are often contradictory – for example, driving fast versus driving safely.\nMany of these unwanted side effects are implicit restriction: you don’t want to kill any pedestrians or harm the passengers. However, some side effects are not hard constraints, but tradeoffs. If you want zero chance of getting in a car accident, the only solution is not to drive. So, you indeed want your AI system to correctly trade off a small risk of getting hurt for the possibility of actually solving the task –getting you from A to B. Pragmatism involves trade-offs.\nAnd we humans frequently make these trade-offs unconsciously, e.g., between getting somewhere faster and taking on a bit more risk in the highway, or going the longer, safer way. This kind of trade-off is at the heart of any complex problem– trade-offs are the very reason engineering problems are hard to begin with!\nWith an AI system, however, it becomes even worse, because the system not only needs to understand the many implicit constraints and trade-offs in the world, but also it needs to understand how you value those trade-offs. You would need to not only specify potential side effects butto give them appropriate negative values, so the system avoids them while still achieving its primary goal.\nThis challenge arises because most machine learning systems optimize performance metrics. And the nature of optimization involves comparing numerical quantities. To optimize your goal of a fast yet safe journey, you must quantify these trade-offs. For example, how much inconvenience is being late compared to the risk of a sudden hard brake? Is it worth risking a small head injury to arrive 20 minutes earlier? How do you put numbers to being early versus being safe?\nFurthermore, for any value you fail to quantify, your AI will be compelled to prioritize performance over that factor, since there’s no penalty for it. So, it’s crucial to quantify all critical side effects. If you don’t specify a crucial dimension –like car damage–, you’re in big trouble. To save time, an AI system might trade off any arbitrarily high amount of car damage for a small reduction in time.\nSince saving even minute of time has some positive value, and no amount of car damage has any negative value, as long as the car reaches the destination –i.e., it is not absolutely destroyed– the AI is free to choose a marginally better route regardless of how much more damage the car takes. You’ll end up with a system that reaches the destination as fast as possible but considers every car disposable.\n\n\nImperfect metrics\nIt is by now well-known to every system’s designer that when a proxy metric becomes an objective, it loses its usefulness as a metric. Yet, this is the dialy job of a machine learning engineer. In machine learning, we are always turning proxy metrics into optimization objectives because, in a sense, that is the only thing we can do.\nIn a typical machine learning problem, we have to turn a complex task into something measurable that our AI can optimize for. So, for the AI, the metrics are the actual task. This isn’t too much of a problem if systems aren’t very good at optimizing metrics, as they tend to work closely to your intended outcome. But as AI systems become better at optimizing our metrics, they can much better exploit the difference between the proxy metric and the actual desired performance.\nThis leads to an interesting paradox: the smarter the system, the more likely it will fail to do what you want them to do. A first reason for this seemingly paradoxical phenomenon is tha imperfect metrics tend to match our desires in the general cases, but the differences with our true intentions are more accentuated in the more extreme cases. It’s like classic mechanics versus general relativity. The former works perfectly for most cases, except if you really need precise calculations of complex astrophysical phenomena.\nIn the same vein, imperfect metrics –such as getting a high score in a videogame, getting high grades in college, or running for a long distance without crashing– are easier to satisfy up to some degree if you’re actually doing the right thing –playing the game optimally, studying very hard, or driving safely. But the easiest way to satisfy these imperfect metrics to a very high level –like acing the SAT– is to game the system. Instead of studying super hard and really learning a lot, just study tons of SAT tests and learn to answer those exact questions, without really understanding much of the underlying theory.\nThis phenomenon is one of the many ways in which overfitting shows up in machine learning, and it’s well-known that the harder you optimize a metric the more likely your system will learn the quirks of that specific metric and fail to generalize to the actual situations where you expect it to perform.\nAnd while this can and will happen by accident, there as an even more insidious problem. The smarter the system, the more likely it will, intentionally, learn to game the system.\n\n\nReward hacking\nImperfect metrics are a problem in all machine learning scenarios, but they become even more challenging in reinforcement learning. As a quick recap, reinforcement learning is when, instead of showing an AI examples of a well-done task, you just let it try things out, and reward it when those attempts lead to progress.\nThe reason we need reinforcement learning is because, for many complex problems, it is simply impossible to produce a sufficiently large dataset of good examples. Sometimes it’s unfeasible –e.g., collecting thousands of hours of expert drivers across many scenarios– and sometimes is, even in principle, impossible –e.g., when you’re building a system to do something you yourself can’t do, like, dunno flying a drone through a building in flames?\nSo, instead of examples, we let the AI loose and evaluate if they reach the intended goal. For example, you let an AI take control of your car –say, in a simulated environment that is sufficiently detailed, like GTA5– and reward it for how many miles it can stay on the road without crashing.\nNow, what is the easiest way to optimize that metric? Maybe something like drive at 2 Km/h? That’s what your AI –if it’s smart– will learn to do. So now you add a new restriction, say, distance only counts if the AI goes at over 40 Km/h. Then the AI will learn to drive forward for 100 meters, shift to reverse, drive back slowly, and repeat. You can keep adding constraints and making the evaluation metric as complicated as you want but the key point is this: all metrics are gameable, and the smarter your AI system is, the better it will be at gaming whatever metric you design.\nThis happens, again, because the AI doesn’t know what you truly want, only what you are measuring. And when metrics become an objective, they cease to be good metrics.\nTo address this, instead of designing an explicit metric, we can let AI systems act and provide feedback on whether their actions are good or not. Then, another machine learning system learns to approximate the evaluators’ assessments, and acts as an implicit performance metric. This creates a two-level ML system, where each is trying to game the other. This process is called reward modelling, or alternatively, reinforcement learning with human feedback (RLHF), and is our current best approach to prevent reward hacking.\nHowever, there are still challenges even with RLHF. Your evaluator AI can learn the wrong model from your feedback, because, again, it is being trained to optimize some imperfect metric –like minimizing the error between its predictions and yours. In the end, you’re pushing the problem of reward hacking one level up, but not getting rid of it.\nAnd finally, even if your system behaves as intended, how can you know it is actually doing so because it truly understands your intentions?\n\n\nInternal objectives\nThe final challenge I want to address is the interplay between internal and external objectives. Today, the most powerful learning algorithms and problem-solving methods we have are all based on optimization. Optimization algorithms power machine learning, symbolic problem-solving, operations research, logistics, planning, design, etc. As AI designers, if we turn optimization to create powerful decision-making algorithms and train a highly intelligent AI, it’s likely that the AI’s internal processes will also involve optimization.\nWhat this means is, suppose you train a highly capable AI agent to solve problems in the real world. This agent would be capable of long-term planning, self-reflection, and updating its own plan as it explores the world. It is sensible to think that, whatever this agent is doing internally to plan its solution, it will use some form of optimization algorithm. Maybe the agent will rediscover reinforcement learning and use it to train in real time its own mini-agents (like tiny homunculi inside its artificial mind).\nIf this looks like sci-fi, consider we humans are basically intelligent agents optimized by evolution to solve the problem of staying alive –I know this is a huge oversimplification, but please, biologists out there, don’t crucify me yet, this is just a useful analogy. So, in the process of solving the problem of staying alive, we came up with optimization algorithms of our own that run inside our brains. A sufficiently intelligence AGI would presumably be able to do the same, right?\nNow here is the problem. You give this AGI some external objective to solve, and it will come up with internal objectives to optimize for. But we might not be able to see this internal optimization algorithm at all. If the AI resembles anything we have today, it will be a massive black-box number-crunching machine.\nJust like you can’t really read out of a human brain what their true objectives are –at least, not yet– we might never be able to truly understand what is the AI optimizing for internally as it strives to solve our problem. We can observe external behavior but might never see the actual internal objectives. All we can do is judge the system based on its actions.\nIn essence, we can only evaluate how agents –humans or not– act, but not their true motivations. And if someone always acts as if their motivations are aligned with ours, it may be difficult to identify any misalignment that could arise in the future. Maybe they are aligned with 98% of our objectives, or maybe only while there is no solar eclipse or some other weird stuff like that. We simply can’t know for sure.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Why AI Alignment is So Hard</span>"
    ]
  },
  {
    "objectID": "alignment.html#is-it-all-lost",
    "href": "alignment.html#is-it-all-lost",
    "title": "8  Why AI Alignment is So Hard",
    "section": "Is it all lost?",
    "text": "Is it all lost?\nI hope you now understand why this is a devilishly hard problem to solve. The very nature of intelligence makes this an adversarial situation. We want systems that are both highly self-sufficient but also very dependable. We need them to think and act on their own, but we need to trust them. And the smarter they become, the more blindly that trust has to be, and the more potential for catastrophe we find ourselves in.\nAll is not lost, though. There’s an enormous body of research in the alignment problem, and while there are no silver bullets yet –and perhaps never will– we’ve made significant progress.\nOne final thought. So far we’ve been focusing on internal challenges to AI alignment –challenges related to the task and the solution. But there’s an elephant in the room. AI alignment is literally having an AI aligned with our values. But whose values? We’re all different, and we all have different opinions about what’s important. So that’s a crucial conversation we need to have as these systems start to impact the daily lives of people all around the world.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Why AI Alignment is So Hard</span>"
    ]
  },
  {
    "objectID": "open-source.html",
    "href": "open-source.html",
    "title": "9  The Future of AI is Open Source",
    "section": "",
    "text": "What does open-source AI look like?\nOpen source is progressively dominating the realm of software, starting from the core layers and expanding towards the infrastructure layers of computing. The primary type of software likely to be open source is infrastructure software. For instance, operating systems, virtualization software, and many drivers are open source. Moving up the hierarchy, development tools also fall within the realm of open source, as they form a slightly higher infrastructure level.\nHowever, regarding consumer applications, open source does not excel similarly. Open-source compilers tend to be superior, but image, video, and audio editors are not competitive compared to closed-source alternatives such as Photoshop or other Adobe products. Similarly, open-source video games are far behind their closed-source counterparts regarding entertainment value. The closer the software is to lower-level implementation details, the more beneficial it becomes to adopt an open-source approach.\nNow, let’s examine how this applies in the context of Artificial Intelligence. First, it’s important to note that the development tools and frameworks in AI, such as TensorFlow and PyTorch, while open source, are not the core of my argument —their open-source nature primarily stems from their classification as development tools, even though they are part of the AI stack.\nLooking closely at language models within the AI stack, we can draw a parallel to basic infrastructure. For instance, foundation models —models trained on extensive text corpora, like the Llama family models and their various iterations— serve as fundamental building blocks. These models can be likened to the operating system level, forming the core infrastructure of the AI ecosystem.\nSuddenly, we have the option to commercialize fine-tuned adapters for specific domains. Let’s say you have Llama as an open-source model. You can either fine-tune it yourself using your own infrastructure or provide me with your data, and I will fine-tune it for you on my infrastructure.\nMoreover, there is an opportunity to sell fine-tuned versions of models for commercializable domains. For instance, I take an open-source model trained on large data. I then fine-tuned it on a small amount of data specifically for, say, the market valuation domain. This model can be used for generating business models, business plans, enterprise emails, quarterly reports, etc.\nEssentially, you can take one of these infrastructure-like, foundational models, fine-tune it for a specific domain using your collected data and computational and human resources, and it would make sense for you to sell it. Thus, as we get closer to the application layer, it becomes more sensible to keep things closed.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Future of AI is Open Source</span>"
    ]
  },
  {
    "objectID": "open-source.html#to-open-source-or-not-to-open-source",
    "href": "open-source.html#to-open-source-or-not-to-open-source",
    "title": "9  The Future of AI is Open Source",
    "section": "To open-source or not to open-source",
    "text": "To open-source or not to open-source\nThen why don’t companies like Google or OpenAI open source their foundation models? This question is worth pondering because not all companies are keen on open-sourcing AI. Currently, only Meta is trying to open-source very large models. The remaining open-source models are typically smaller and built upon existing open-source foundation models. The large-scale models are generally kept closed source.\nSo, what is preventing companies like Google and OpenAI from joining the open-sourcing trend? Baring some extreme views, the fundamental argument these companies have to choose open or closed source is, of course, an economic one. What is the strategy that will provide the most profit? Call me a cynic, but I don’t get fooled by their appeal to safety or privacy principles. These are for-profit companies, so their incentives are aligned with their investors —and there’s nothing inherently wrong about that, if you ask me.\nSo let’s examine, from their point of view, the financial advantages of open-sourcing or not their foundation models. Collecting large datasets and investing significant computing power in training a huge model puts your competitors in a difficult position. They need to invest similar resources to compete with you. Consequently, only major players like Google, Meta, and Microsoft could achieve this independently.\nIn contrast, if, say, OpenAI chooses to open source GPT-4, they would give away whatever advantage they had initially. Doing so saves competitors from replicating the extensive computing power needed to utilize their model. Thus, OpenAI could only differentiate itself through the fine-tuned and application layers. Their advantage would be not the GPT-based model but just the ChatGPT application.\nIt is understandable why these companies are unwilling to open source their foundation models as they invest significant resources in building them. Unlike projects such as the Linux kernel, which can thrive on contributions from hobbyists worldwide, developing a foundation model appears to require the backing of a large corporation with substantial financial and infrastructural capabilities.\nTraining GPT4 is significantly more expensive than creating the Linux kernel, at least in the short term. One may argue that the Linux kernel has required extensive monetary investment in volunteer hours over many years, and these numbers may amount to a substantial quantity. However, when faced with training a massive model in just six months, it becomes evident that no open-source software, regardless of its nature, can allocate millions of dollars within such a short timeframe to go from inception to completion. Thus, this distinction between traditional open-source and AI open-source is crucial.\nHowever, Meta did open-source their model. Although they initially were reticent to go full open-source —they released Llama under a non-commercial research license until it was “accidentally” leaked. Nonetheless, Llama 2, the newest model, is commercially licensed. There is an unusual restriction, though: companies generating billions in revenue are, in principle, prohibited from using it commercially.\nThis sounds like Meta wants to have their cake and eat it, too. They aim to gain recognition for being a cutting-edge company that shares open-source projects —crucial for their credibility with developers since they are not the “cool” company; Google holds that reputation. Thus, Meta seeks the social credit associated with being the company that shares significant projects like Llama while simultaneously striving to limit competition from major players.\nMeta sees Google and the other big players as the only competition because, unlike other AI-first startups like OpenAI, Hugging Face, Anthropic, etc., Meta’s core product is not language models or anything AI-related. Like Google, Meta’s core product is much bigger and darker: advertisement.\nAI and language models serve only as valuable tools to enhance their core business model’s profitability. In this regard, Meta does not directly compete with many small startups attempting to develop ChatGPTs for various domains. Meta is indifferent to such competition.\nWhether you use Llama to create a chatbot for business purposes, generate fiction, or automate emails, it bears no significance to them. They are distinct from Grammarly, Hugging Face, and countless other startups focused on constructing AI-powered tools. This is not their area of expertise or profit.\nConsequently, they are not bothered by granting these smaller companies a more convenient life by releasing their models. Their primary concern is making it harder for Google, Amazon, Microsoft, and Apple to outpace them. Hence, by outmaneuvering these four entities, they can benefit fully from the clout of being open-source friendly without any of the business drawbacks.\nThe reasoning behind Meta’s decision to open source Llama, I think, is to strengthen their status as a cool company while safeguarding their tech from its real competitors. Despite not sharing Meta’s ethos, I agree with their choice to open-source AI. This move benefits the community, advances scientific research, and enhances business prospects when embraced by all.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Future of AI is Open Source</span>"
    ]
  },
  {
    "objectID": "open-source.html#the-advantage-of-open-sourcing",
    "href": "open-source.html#the-advantage-of-open-sourcing",
    "title": "9  The Future of AI is Open Source",
    "section": "The advantage of open-sourcing",
    "text": "The advantage of open-sourcing\nHowever, doesn’t open-sourcing your model put you at a disadvantage in a world where no one else is doing it? Aren’t companies like Google, Microsoft, Amazon, and Apple, by building foundation models but not open-sourcing them, still beating you to the punch?\nI believe not. Open sourcing a foundation model and allowing a vast community of researchers, hobbyists, entrepreneurs, start-ups, and students worldwide to contribute offers numerous advantages. The extensive collaboration from these groups enables the development of innovative ideas and products on top of the foundation model. I argue this far outweighs any potential drawbacks of keeping the model closed and restricting product development solely to the company.\nIn the future, all our creations will rely either on closed services like ChatGPT or open-source models like Llama. The considerable brand recognition gained from having a community of users of your foundation model is an invaluable asset. However, it is not the sole or most significant benefit of open-sourcing your foundation models.\nThat is collaboration. You see, the most significant issue that needs to be addressed in language models if they are to have lasting value and not be just a passing trend is reliability. The challenges of hallucinations, prompt reproducibility, and biases can all hinder the progress of language model technology and even bring a new AI winter. To truly become more than mere entertainment tools and instead serve as foundational infrastructure that can be relied upon for building applications, language models must tackle these fundamental problems.\nHowever, solving these problems requires going beyond the API layer. Any solution to hallucinations requires modifying the model —adjusting the weights, altering the architecture, or refining the sampling process. These changes must be implemented at the model level rather than relying solely on API access.\nWith open-source models, anyone can contribute to addressing hallucinations, grounding the model, improving biased language, enhancing adherence to prompting, and increasing reliability. The availability of model weights and access to the source allows for a seamless combination of efforts. This approach allows for the multiple benefits of a distributed community of researchers and developers addressing the core issues, in the same sense that reliability issues of traditional software are often solved by open collaboration.\nIn contrast, if you operate on a closed-source basis and keep the model internally within your company, even if you have the power to invest as much money as you desire, the resource that remains most restricted is the availability of sufficiently skilled individuals who can tackle these challenges effectively.\nThrowing money allows for extensive data collection and scaling computational power. However, scaling the number of qualified personnel is not as easy. Even with substantial resources, OpenAI, Microsoft, or Google cannot recruit every artificial intelligence engineer, data scientist, or machine learning theorist worldwide.\nThe key limitation to making AI truly useful lies in the scarcity of human resources.\nOpen source is the key to scaling in human resources. This model promises unlimited and mostly free access to a vast pool of talented individuals eager to work with you. In contrast, closed source restricts your access to only those individuals you can persuade to work for you, often depending on the salary you offer. No matter the amount you pay, there will always be exceptionally bright individuals who choose to work for your competitors instead of you.\nOpen-source language models, such as Llama, have the leading edge here. Unlike GPT, the community will develop and improve Llama and its successors. This means they can become more reliable, customizable, and practical compared to closed-source models. As open-source OSes, compilers, databases, and development tools have demonstrated before, I think open-source language models will eventually become more robust and generally superior to anything you can develop in closed source.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Future of AI is Open Source</span>"
    ]
  },
  {
    "objectID": "open-source.html#conclusions",
    "href": "open-source.html#conclusions",
    "title": "9  The Future of AI is Open Source",
    "section": "Conclusions",
    "text": "Conclusions\nThe current state of open-source language models versus closed-source models is far from the ideal I just painted. Today, closed-source models far exceed open-source models regarding reliability and robustness. Anyone who has attempted to build applications using GPT through OpenAI’s API and then transitioned to Llama or other open-source models has witnessed their unreliability and general lack of robustness.\nHowever, this situation is not unique to language models. This unreliability is expected in initial open-source projects, as the first generation of any open-source software is typically subpar.\nLinux, for instance, took a considerable amount of time before it became a robust operating system, surpassing all closed-source alternatives. Similar patterns are seen with compilers, database engines, IDEs, and frameworks. Initially, closed-source solutions have advantages in terms of development speed, benefiting from massive upfront investment in computing and data, which open-source initiatives cannot match.\nNevertheless, we witness open-source scalability in infrastructure-oriented software. This is because the potential of open source is nearly limitless, tapping into a vast pool of human contributors. In contrast, closed-source tools remain constrained by the number of individuals they can hire. As a result, open source holds the advantage in the long run, at least at the foundational level.\nCurrently, open-source machine learning and language models are in their early stages. Therefore, you should expect closed-source models to outperform open-source models for the next few iterations, perhaps even a few years. However, as history has demonstrated, open source will eventually catch up and become the optimal solution. Closed-source AI companies will gradually transition to open-source and contribute to the community. Eventually, the infrastructure layer of AI will become open-source, like most software infrastructure has.\nFor developers today, I advise continuing to use closed-source models for production but keeping tabs on the open-source landscape. Become an early adopter and contributor, and help enhance the robustness and accessibility of AI for future generations. Computer Science is undergoing a massive revolution akin to the 60s and 70s. This time, you can be one of the pioneers.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Future of AI is Open Source</span>"
    ]
  },
  {
    "objectID": "road-to-agi.html",
    "href": "road-to-agi.html",
    "title": "10  The Road to AGI",
    "section": "",
    "text": "General AI is not (just) scaled-up narrow AIs\nLet’s begin by quickly reviewing what we mean when we say “narrow” and “general” AI, and then I’ll tell you why I think narrow times x for any x is not equivalent to general.\nAI is cognitive automation. It’s about building computer programs that solve tasks we humans rely on our cognitive abilities —from puzzles to perception to reasoning to planning and anything else where humans employ their intelligence to solve problems.\nNarrow AI is just AI within a concrete, well-defined domain. Playing chess at the world-champion level is narrow AI. Playing Go like freaking gods is narrow AI. Self-driving cars are narrow AI. And yes, ChatGPT is narrow AI. I’ll tell you in a moment why.\nIn contrast, general AI is AI that works across all domains in which human intelligence can be effectively deployed. This includes, crucially, domains that we don’t even know exist today, but when we find them, we will function there just as effectively as in everything else —just like modern physics didn’t exist in Aristotle’s time, but there’s no cognitive difference between Einstein and Aristotle. Raise an Aristotle in 1900, and he would master relativity just fine.\nThe difference between narrow and general AI is in the level of generalization. I will borrow a bit from Francois Chollet in his paper On the measure of intelligence, which I truly recommend, although I’ll rephrase things in my terms.\nThe notion of generalization in AI, and more specifically machine learning, is the ability of a model to perform effectively outside the conditions in which it was designed —or trained— to work.\nIn its simplest form, we expect at least out-of-training generalization, i.e., that models perform well in instances they didn’t see during training. The opposite of that is called overfitting —performing well in training instances but terrible in everything else. The only thing we need for out-of-training generalization is our model to interpolate correctly between training samples. Any sensible model that doesn’t over or underfit the training set will exhibit this mediocre level of generalization.\nThe next step is out-of-distribution generalization, i.e., models that not only work for unseen instances that are pretty similar to training instances but also instances that are quite different. Of course, “quite” is doing much of the work in that sentence. Technically, you want the model to extrapolate beyond the training examples. For that, you need sensible priors that capture what you expect the data distribution to be outside the training convex hull. Most of us think of this level of generalization when we say that a model generalizes well to unknown inputs.\nAnd finally, the ultimate step is out-of-domain generalization, i.e., models that can be applied to new types of problems in completely new domains without retraining. And we still have no idea what it takes to get there.\nSo this is the line that separates narrow from general AI. Up until out-of-distribution generalization, all we currently have is narrow AI.\nThe reason is that this type of generalization is developer-aware, to put it in Chollet’s terms. This means that even if the test instances are unknown to the model, they are known to the developer. That’s why we can find sensible priors and inductive biases that extrapolate effectively. Because we know beforehand how the test instances will look, we can design models that will perform well on those instances, even if the models are not exposed to those exact instances during training.\nExtreme generalization —as out-of-domain generalization is also called— is a completely different beast. To get a narrow AI in any given domain, our model has to master one skill: playing chess, identifying pedestrians, or folding molecules. But crucially, in each of these cases, the actual learning algorithm is designed ad-hoc.\nBut to get to general AI, our models must master skill acquisition. That is, they have to learn to learn. In a sense, a general AI is an AI researcher, automated. And yes, there is research in this direction —the field is aptly called meta-learning— but we’re far from even automatically discovering the most basic learning algorithms we have already designed.\nThus, back to my initial claim: no amount of scaling-up narrow AIs will lead us to general AI. There is no point where learning disparate skills —object detection, speech synthesis, chess playing; you name it— suddenly leads us to learn how to learn. There’s a qualitative jump there, in the same sense as making ever higher skyscrapers won’t lead us to the moon. It’s not just a matter of scaling what has worked so far. You need a completely different theory.\nBut wait —I hear you say— aren’t you making some gross generalization when you gloss over the term “concrete skill”? Isn’t “actually learning” just another concrete skill?\nYes, it is. But it is not a skill that can be learned via the typical machine learning framework. No amount of data in any specific domain will lead a model to find a hypothesis outside its hypothesis space. And in any specific domain, at least for any machine learning approach we currently have, the hypothesis space only contains hypotheses about the entities in that domain. It doesn’t contain hypotheses about the hypothesis space itself.\nBut what about LLMs? — you ask— Don’t they display generalization to many unseen tasks with very few examples? Isn’t in-context learning an instance of learning to learn?\nWell, kind of. To be clear, LLMs are the most general models we currently have. They generalize past what we expected at this point of technological development and exhibit an uncanny ability to adapt to new tasks as long as we frame them in an appropriate linguistic framework. Thus, I claim LLMs are, while super exciting and impressive, still just narrow AI.\nIn short, while I believe NLP to be AI-complete —meaning that actually solving NLP entails solving all of AI— I strongly believe the stochastic language modeling paradigm we currently use in all LLMs, is not NLP-complete. That means there are unsolved problems in NLP that no amount of data and computing power will solve until we find something better than LLMs.\nLet’s go there, then.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Road to AGI</span>"
    ]
  },
  {
    "objectID": "road-to-agi.html#emergent-abilities-in-llms",
    "href": "road-to-agi.html#emergent-abilities-in-llms",
    "title": "10  The Road to AGI",
    "section": "Emergent abilities in LLMs",
    "text": "Emergent abilities in LLMs\nWe already agree that narrow or domain-specific AI can’t simply scale to general-purpose AI with more data and computing power. The gist of the argument is that learning to learn —which is what is required for general AI— is a completely different beast than learning any other domain skill.\nBut one thing we left hanging in the previous section was the discussion of whether current large language models are something “above” narrow AI, even if not still completely general. One argument often made in favor of their generality is that these models exhibit surprising abilities they were not explicitly trained for, so-called emergent abilities.\nSo let’s briefly analyze emergent abilities, where they come from, and how significant they are in discussing narrow vs. general AI. As I usually do, instead of simply giving you my opinionated views, I will try to lay out a framework for thinking and talking about emergent abilities in computational systems that help you reach your conclusions —though I will also tell you my conclusions.\nAs an appetizer, I sincerely recommend you read this Twitter/X thread from Francois Chollet, one of my all-time favorite level-headed voices in AI.\n\nWhat is emergence?\nIf you read Chollet’s thread you’ll notice he uses a particularly loose definition of “emergent”, taken as some surprising ability or property you didn’t plan explicitly. The problem with this definition is that it is very subjective: different people looking at the same system will see emergence or not depending on their expectations.\nIf we’re going to have a serious discussion about emergence, we need a more objective definition. I will propose a slightly better definition, although it won’t be without issues.\n\nA property or ability in a system is emergent if it is displayed at the system level but not at any component’s lower level.\n\nNow that we have a working definition let’s run a sanity check. The least you want from a definition is that it is nontrivial. A trivial definition is always true or false. A nontrivial one will apply to some cases but not all. So let’s see if we can find sensible examples of both emergent and non-emergent properties.\nI’m focusing on computational systems in this post, although this discussion extends to all sorts of systems. But let’s keep it simple and focused.\nA straightforward example of a non-emergent property in computational systems is error tolerance. Find any error-tolerant system —e.g., a highly distributed database— and you can almost certainly pinpoint which components confer that property to the system, e.g., redundant storage, consensus algorithms, etc. The point is you don’t get error tolerance for free; rather, it is often baked into the system in the form of some explicit component implementing that functionality.\nIn contrast, a nontrivial emergent property in a computational system is Turing completeness, the ability to perform any possible computation. The simplest Turing-complete system —the basic Turing machine— is a smart wiring between a plain and simple finite state machine with a plain and simple read-write memory tape. Neither the state machine nor the tape themselves are Turing-complete. It is only their interaction that makes the system as capable.\nThese two examples highlight an alternative definition for “emergent” that aligns more with Chollet’s intuition and the common definition —that an emergent ability is something you discover in your system rather than explicitly designing for it.\n\nA property or ability in a system is emergent if it is caused by the interaction between components and not by any of the components sole function.\n\nThere is one final distinction to be made between weak and strong emergence. Weak emergence is when an emergent property can be reductively explained by looking at the relationships between system components. Strong emergence is when that’s not the case: no matter how you decompose the system into components, no reductive theory explains —i.e., predicts— the emergent property from those components.\nIn the above sense, Turing-completeness is a weakly emergent property, because we can explain how the interaction between a finite state machine and an infinite memory, neither of which is Turing-complete by itself, suddenly gives rise to this property.\nSo far, we don’t know if there are strongly emergent properties in the world, let alone computational systems, but a good candidate is consciousness. If materialists are right and consciousness is just an emergent property of matter, i.e., all mental states correspond to some physical states of the brain, then one possible solution to the hard problem is claiming that consciousness is a strongly emergent property in sufficiently complex information processing systems; thus, it cannot be reductively explained.\n\n\nEmergence in language models\nNow that we have non-trivial examples of emergent and non-emergent properties in computational systems, let’s turn to the specifics of machine learning.\nChollet uses a simple example of emergence in language models in his thread: encoding semantic properties in word embeddings. When we train a word embedding model, like word2vec, we don’t explicitly design it so that specific directions in the embedding correspond to semantic properties like gender, time, location, or relations like synonyms, meronyms, or antonyms.\nWe train the model so that word embeddings are useful for predicting a word’s context distribution. It just happens that the best way to encode a word’s context in a fixed-length real-valued vector is such that words with similar semantic relations end up having similar geometric relations. Thus, this is an emergent property.\nHowever, from the previous discussion, I hope you agree this is a weakly emergent property. We understand pretty well why certain directions in the embedding space tend to approximately encode certain semantic properties. It is surprising but explainable, as much research has been published on the explanation. Crucially, we now know how to train word embeddings such that certain directions encode specific semantic properties, so we can design for this ability.\nNow, the most interesting emergent ability in machine learning, this time in large language models like GPT-3, is, of course, in-context learning. This is the ability to prime a language model to solve a seemingly novel task via carefully constructed prompts without fine-tuning the weights. Most, if not all, of the surprising use cases of LLMs, are ultimately reducible to particular cases of in-context learning.\nBut is in-context learning a weak or strongly emergent ability? We still don’t have a precise explanation for why it happens. Specifically, we still cannot say, “these are the exact interactions between these components that make in-context learning appear.” However, this doesn’t mean there isn’t an explanation; it just means we still haven’t found it. I personally believe that in-context learning is weakly emergent and that we will find a convincing explanation sooner rather than later.\n\n\nCan LLMs reason?\nOne especially exciting thing about in-context learning is that, for some tasks, it implies LLMs need to construct internal world models. The canonical example, which appears in the famous “GPT-4 sparks of general intelligence” paper, is the ability to solve Otello puzzles. After analyzing the weight activations on certain Otello prompts, researchers discovered what seemed like a pretty explicit representation of the Otello board, together with correct topological relations between the pieces and their locations.\nA recent preprint argues that, after a thorough evaluation, GPT-4 cannot reason. According to the author, the conclusion rests on a set of carefully designed reasoning problems with more sophisticated qualitative analysis. We must remember that this paper is not peer-reviewed, so take it with a grain of sand, but after its publication, there have been many more examples showing catastrophic reasoning failures in GPT-4. At the moment, no one serious would claim any of the existing models actually reasons at a level similar to most humans.\nHowever, beyond empirical evaluations, we can make a couple of formal arguments to highlight why the current “pure” language modeling paradigm has to be incapable of fully-fledged reasoning. It’s a classic argument in computability theory, and it goes like this.\nIf any computational model can reason —to the full extent of the meaning of this word in the context of artificial intelligence—, it must be able to perform Turing-complete computations, for if a model is not Turing-complete, that means there are decidable problems it cannot solve. One key aspect of Turing completeness is that it requires potentially unbounded computation. That is, there are some problems for which, in some inputs at least, the model must be able to spend an arbitrary amount of computation.\nA simple example is answering an open math question, like, “What is the smallest odd perfect number?” We don’t know, first, if there are infinite perfect numbers; second, how far between they are; and third, if there is an odd one. However, we can write a simple program that tries every number in order and stops when it finds an odd perfect number. The program will either never stop or find the smallest one.\nNow, by design, GPT-4, or any pure language model, cannot “think forever.” The amount of computation spent in answering any prompt is proportional to the input length in a very determinate way, which depends only on the model size. Thus, by a simple counterargument, LLMs cannot reason, at least to the extent of answering semi-decidable questions.\nNow, you can claim, “GPT-4 can reply that this question is unknown; and that is true.” And yes, it can. But you don’t need reasoning to give me that reply. You only need to repeat the Wikipedia entry on perfect numbers. We expect more from fully-fledged reasoning systems.\nThere is a way out, however. GPT-4 could generate the necessary code to answer this question and run it. And that is precisely what the newest iteration of ChatGPT does with Code Interpreter. Tying a language model with a code interpreter gives you, potentially, a system capable of Turing-complete reasoning —and trivially so because the code interpreter is already Turing-complete. However, it remains to be seen if the language model can generate the correct programs.\nThis is what gets more excited about LLMs in the near term. Wiring them with formal systems can lead to qualitatively superior functionality. The cloud on the horizon is that last caveat: we don’t know if language models trained on existing code will be capable of generalizing to generate new, unseen code for novel problems. And we will never know for sure because proving that any given program has a non-trivial semantic property is generally undecidable.\nHowever, humans are also bounded by this formal limitation —unless you believe in magic, which I don’t— and we get along with programming pretty well. Thus, we don’t need magical LLMs that can solve undecidable problems. We need LLMs that, in practice, can solve the same problems we can, just faster and slightly more reliably. That would already be a massive transformation of the entire field of Computer Science.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Road to AGI</span>"
    ]
  },
  {
    "objectID": "road-to-agi.html#the-road-to-general-purpose-ai",
    "href": "road-to-agi.html#the-road-to-general-purpose-ai",
    "title": "10  The Road to AGI",
    "section": "The road to general-purpose AI",
    "text": "The road to general-purpose AI\nLLMs —and most, if not all, practical machine learning models— are computationally bounded. That means they perform a fixed amount of computation for any given input that can be quantified solely on the size of the model and the input, irrespective of the content of the input.\nThis immediately restricts the class of problems these models can solve because, intuitively, two prompts of similar size can encode problems of widely different complexity. Formally, we can say that a computational system, to be Turing-complete, must have inputs in which you cannot predict beforehand the amount of computation necessary. It can even be an infinite amount. Thus, any computational system that performs a fixed amount of computation for all inputs is bound to be less than Turing-complete. There are questions it simply cannot answer correctly.\nHowever, this previous description only applies to the “pure” language modeling paradigm in which the input and output are natural language text. We can attempt to bypass it by allowing the language model to output computer code and, crucially, execute that code before answering.\nThis makes the whole system Turing-complete, and trivially so because the language model could generate the correct code to answer any semi-decidable question and execute it. You get Turing completeness because one of the components of the system is already Turing-complete. So it’s not even an emergent ability in the sense in which we defined it in the last issue.\nBut… that an AI system is, in principle, Turing-complete only tells us a part of the story. There’s no guarantee that by hooking a language model with a code interpreter, you can suddenly solve all solvable problems. For that, we need the model actually to output the correct code for a given prompt, and this is where things turn interesting.\n\nTowards program synthesis…\nThis is the problem of program synthesis. On the surface, program synthesis looks like just one specific domain of generative AI. Instead of generating natural language text, or beautiful images of cats, the model is asked to generate code for a given problem. However, this problem is AI-complete.\nThe reason is simple. If a computer can generate a correct program for any given question, it can definitely answer any given question. Thus, you can reduce all of AI to program synthesis. Image generation? Generate a program that tells Photoshop what to do. Self-driving cars? Generate a program that tells the car how to move. Passing the Turing test? Generate a program that is a perfect chatbot.\nI’m stretching this to an extreme to show that, since all a computer can do is run programs, if an AI system can always generate the correct program for any given question, it can do anything that any other computational system can do. This is, by definition, AGI.\nPut another way. Suppose there exists some task T that a computer program P can do. Now take our program synthesis AI and ask it to solve that task T. Doesn’t know how to do it? Never mind, it can just generate and run the exact code of program P.\nIn fact, program synthesis is exactly what programming is about. We take a problem description and write code that solves it. We don’t need to know how to play chess best, the correct trajectory for a rocket to land on Mars, or anything else. We just need to be sufficiently smart to be able to code a program to solve that problem.\n\n\n…and beyond\nDoes this mean ChatGPT with Code Interpreter is a step closer to AGI? Well, kinda, but we’re still far, far away. What current LLMs can do is a very restricted form of program synthesis. If you’ve used Copilot or any other AI code generator, you surely understand this. For all the impressive it is, it still makes enough mistakes at anything but the most trivial prompts that it requires constant supervision by someone who can actually code.\nWe should expect this to improve, though. The question is whether the current LLM paradigm (self-supervised pretraining+ instruction fine-tuning+ RLHF) can get us all the way to full program synthesis from natural language.\nAs usual, we have reasons to suspect this problem is not entirely solvable. A formal argument against perfect program synthesis is Rice’s theorem. In short, determining whether a given program satisfies some arbitrary semantic properties (such as never going into an infinite loop or never de-referencing a null pointer) is generally undecidable.\nHowever, unless you believe human brains are qualitatively superior to Turing machines, we are also limited by this argument, and we consider ourselves general-purpose programmers —at least I do!\nThus, general-purpose program synthesis need not be perfect. It just needs to be good enough, at the level of the best human programmers. Can we get there?\nI believe we can, but we need some paradigm shifts. A quick comparison between how humans learn to and actually do program synthesis will show the vast chasm between humans and LLMs concerning coding skills.\nFirst, humans don’t have a training mode or an inference mode. We are never-ending learners, always improving our own understanding of every action we take. Second, humans don’t write working code at once. We write it, test it, debug it, improve it, refactor it, and start over. Furthermore, we don’t interact with code sequentially, one token after the other. We insert, delete, and move things around as necessary. And finally, we don’t just take natural language prompts as input. We formulate follow-up questions, develop counter-examples, and generally maintain a natural language conversation parallel to the code evolution.\nNone of this is, I believe, outside the reach of machine learning. But we need a new paradigm. An agent that can write and tweak a codebase arbitrarily, interspersed with a natural language conversation about it, and frequently executing that code and observing its behavior, all that while being able to update itself —not necessarily through gradient descent— to learn continuously from the whole process.\nGeneral purpose program synthesis from natural language is the hardest problem of Artificial Intelligence, if only because it subsumes all the other problems. And we have made significant steps already but are still at the earliest stages in this race.\nThe road to AGI necessarily goes through program synthesis. But we can’t stop there. Useful and safe AGI must be able to reason and act while respecting human values if it will help us reach our greatest potential as species. However, making sure a sufficiently smart AI is safe —and not just pretending to be safe— may very well be the hardest engineering problem we’ve ever faced!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Road to AGI</span>"
    ]
  },
  {
    "objectID": "epilogue.html",
    "href": "epilogue.html",
    "title": "The Road Ahead",
    "section": "",
    "text": "Hallucinations\nThe first major challenge is the issue of hallucinations. Despite the models’ remarkable ability to produce coherent text aligned with user preferences, they can still veer off course into a response sequence that strays from the conversation. This randomness in sampling makes it impossible to guarantee that a model won’t generate text that deviates from what’s expected, no matter how finely tuned it gets. These deviations range from factual inaccuracies to more complex issues like promoting racism or discrimination. This phenomenon, often called “jail-breaking mode,” poses a significant obstacle to the model’s reliability.\nAddressing hallucinations becomes even more critical when using these models for tasks beyond writing or editing text. For instance, the model is used for tasks like booking flights or hotels, where the user may not have full visibility into the prompt used to generate the model’s response, making it difficult to detect any hallucinations in the model’s output.\nThese hallucinations are the primary roadblock to scaling language models for practical applications beyond simple conversational agents, presenting the most pressing issue for the widespread use of generative AI and large language models today.",
    "crumbs": [
      "The Road Ahead"
    ]
  },
  {
    "objectID": "epilogue.html#biases",
    "href": "epilogue.html#biases",
    "title": "The Road Ahead",
    "section": "Biases",
    "text": "Biases\nDealing with biases is another major challenge in making these models work in real-world applications. These models are trained on huge amounts of data, and as a result, they inevitably contain discriminatory and harmful biases inherited from the training data. When combined with the issues of hallucinations and jailbreak, it’s always possible, whether intentionally or unintentionally, for these models to exhibit biased behavior.\nFurthermore, as we use reinforcement learning and human feedback to encourage these models to behave fairly and unbiasedly, they lose their original effectiveness. They may refuse to answer slightly biased questions, some dubbed the “wokeness problem of AI.” That is, striving to make them as harmless and unbiased as possible can make them less responsive in situations where bias or discrimination isn’t a concern.\nDetermining the precise boundary between a biased or discriminatory response and one that isn’t is challenging when using reinforcement learning, which has a low sample density. As a result, the boundary of these two sets is likely to be quite jagged and hard to define. Consequently, we currently don’t have a solution to unbias these models effectively. We don’t yet know how to remove biases from a dataset or model without significantly harming performance.\nThis issue will continue to manifest in any situation where these models are employed to make or assist with decisions that involve people and ethical considerations regarding their data. Thus, as AI regulation becomes more prevalent, ensuring fairness is a major challenge in the widespread adoption of LLM-driven applications.",
    "crumbs": [
      "The Road Ahead"
    ]
  },
  {
    "objectID": "epilogue.html#understanding",
    "href": "epilogue.html#understanding",
    "title": "The Road Ahead",
    "section": "Understanding",
    "text": "Understanding\nFinally, let’s consider a more fundamental limitation: whether language models can develop accurate internal world models solely from linguistic interaction.\nThe GPT-4 paper, along with many anecdotal claims, suggests that sufficiently complex language models do form internal world models, enabling them to engage in something resembling actual general-purpose reasoning during next token prediction. However, skeptics from various fields question the ability to learn to build a world model purely from linguistic interactions, arguing that grounding in experiential reality is necessary.\nAn internal world model refers to an internal representation of the domain in which the language or machine learning model operates, allowing it to make predictions and plans. This is the first step to truly understanding some task or domain.\nLong-term planning is a fundamental problem in AI that must be addressed for agents to effectively interact with the world and make decisions. The notion that accurate world models can be developed purely from language interactions suggests that these models could learn how processes work and perform inference, all from learning about the world through language alone.\nMany researchers claim that large language models do not have internal representations. When prompted with complex sequences of instructions, these models can miss the intended meaning altogether. In contrast, humans can simulate these instructions in their minds, keeping track of the final outcome without remembering the entire sequence. This ability allows humans to accurately answer questions about the scenario in real-time.\nAn argument against LLMs’ internal models is that their error rates seem to increase with the length of prompts. As an example to understand this argument, consider a game of chess. In the game played mentally, one only needs to track the current board state to answer questions, regardless of the conversation’s length. So, unless memory is faulty or one gets tired, there is no reason why the probability of making a mistake (like forgetting how a piece moves) would increase as the game gets longer if one has an internal model of the game.\nHowever, without internal world representations, LLMs might only learn correlations between sequences of instructions and end states, leading to a higher likelihood of mistakes with longer prompts. This limitation is significant because if these language models are to be used for tasks beyond linguistic manipulation, such as planning, problem-solving, or decision-making, they must be able to simulate and predict outcomes for extended sequences. Without the ability to learn a model of the world, these models will be severely limited in the complexity of problems they can solve.\nFurthermore, building internal models from linguistic interaction alone may be fundamentally impossible. If true, then we will need a qualitative improvement in our AI systems, a new approach that surpasses the capabilities of large language models.",
    "crumbs": [
      "The Road Ahead"
    ]
  },
  {
    "objectID": "epilogue.html#moving-forward",
    "href": "epilogue.html#moving-forward",
    "title": "The Road Ahead",
    "section": "Moving forward",
    "text": "Moving forward\nAs we welcome AI into our lives, we can contemplate what has undoubtedly been the some of the most impactful years of Artificial Intelligence since the birth of the field around the 1960s. The next few years will be about cristalizing the many potential applications of AI into actual, useful products that might usher a new era of abundance like we’ve never seen before. Even if we hit a major theoretical barrier, and there is now new scientific development that has led to AGI for decades, we still have years ahead to harvest what’s already possible with current AI technology.\nBut the future is never certain. Artificial intelligence is the most powerful technology we have, and as such, it is also the most dangerous. There are deeply troubling problems and challenges in the short and medium term, for which we have little guarantee we can solve them. Only with the collective effort of researchers, engineers, lawmakers, creators, and the general public can we stand a chance to overcome them.\nIn any case, if you feel like the year has passed and you got left behind, worry not; today is the best moment to get into AI. Whatever your profession and your interests, there is something in the field of AI for you. If you care about fundamental theory, AI questions are some of the most challenging open questions in math and logic. If you care about engineering, some of the most interesting pragmatic problems are about scaling AI. If you care about ethics and society, one of the most critical questions ahead is how to deal with this tech’s massive impact on our lives. And if you just want to have fun, you now have one of the most powerful technologies ever made at your fingertips.",
    "crumbs": [
      "The Road Ahead"
    ]
  }
]