<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Will AI Kill Us All? – Mostly Harmless AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./risks.html" rel="next">
<link href="./beyond-chatbot.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai-kill-us.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Will AI Kill Us All?</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Mostly Harmless AI</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Beyond Optimism and Doom: Finding a Third Path in AI Discourse</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prologue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Age of Artificial Intelligence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./coding-is-dead.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Coding is Dead; Long Live Coding!</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./education.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The AI Revolution We Don’t Need</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./beyond-chatbot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Beyond the Chatbot Revolution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai-kill-us.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Will AI Kill Us All?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./risks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Actual Risks of AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hallucinations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Reliable AI needs a New Paradigm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reasoning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Large Language Models Cannot Reason</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./alignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Why AI Alignment is So Hard</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./open-source.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">The Future of AI is Open Source</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./road-to-agi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Road to AGI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./epilogue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Road Ahead</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#how-ai-might-kill-us-all" id="toc-how-ai-might-kill-us-all" class="nav-link active" data-scroll-target="#how-ai-might-kill-us-all">How AI might kill us all</a>
  <ul class="collapse">
  <li><a href="#destructive-capabilities" id="toc-destructive-capabilities" class="nav-link" data-scroll-target="#destructive-capabilities">Destructive capabilities</a></li>
  <li><a href="#motivations" id="toc-motivations" class="nav-link" data-scroll-target="#motivations">Motivations</a></li>
  </ul></li>
  <li><a href="#how-feasible-is-doomsday" id="toc-how-feasible-is-doomsday" class="nav-link" data-scroll-target="#how-feasible-is-doomsday">How feasible is doomsday?</a></li>
  <li><a href="#what-should-we-do" id="toc-what-should-we-do" class="nav-link" data-scroll-target="#what-should-we-do">What should we do?</a>
  <ul class="collapse">
  <li><a href="#the-pragmatist-approach-to-x-risks" id="toc-the-pragmatist-approach-to-x-risks" class="nav-link" data-scroll-target="#the-pragmatist-approach-to-x-risks">The pragmatist approach to x-risks</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Will AI Kill Us All?</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>There are many risks and challenges in the deployment of artificial intelligence. It is one of our most potent technologies so far, and like all technologies, it can be used for good or evil. The more powerful the technology, the greater the potential for positive and negative applications.</p>
<p>It depends more on its utilization by humans than on the inherent nature of the technology itself. The impact and consequences of any technological advancement are significantly shaped by how humans choose to employ and integrate it into their lives, societies, and the broader world. The responsible and ethical use of technology, therefore, plays a pivotal role in determining whether it leads to positive or negative outcomes, emphasizing the profound influence of human decisions on the course of technological evolution.</p>
<p>For example, a hammer can be utilized to build a house or to harm someone, but neither construction nor harm would be particularly efficient. Similarly, dynamite can be employed to construct roads or destroy cities, and nuclear power can either obliterate a nation or lift an entire continent out of poverty. AI appears to lean towards the more extreme end of this spectrum. It holds the potential to be an immensely powerful technology that can revolutionize society and automate complex tasks. However, this same power also allows it to cause significant destruction.</p>
<p>AI can be leveraged in various ways, ranging from the overwhelming dissemination of disinformation to the pervasive bias in news and media. In this article, I want to focus specifically on one set of AI risks: the so-called existential risks. These risks involve the potential for AI to completely destroy human civilization or even extinguish the human race.</p>
<p>In this article, we will review the most prominent scenarios for AI existential risk. Then, we will identify and examine the flawed premises upon which these scenarios are based. We’ll explain why we believe these scenarios to be highly improbable, if not impossible. Finally, we will argue why it is worthwhile to intellectually pursue and discuss even the most extreme doomsday scenarios. We believe approaching this topic with an open mind and rational thinking can provide valuable insights and perspectives.</p>
<section id="how-ai-might-kill-us-all" class="level2">
<h2 class="anchored" data-anchor-id="how-ai-might-kill-us-all">How AI might kill us all</h2>
<p>There are many different scenarios for potential existential threats of AI. These situations involve artificial intelligence, or a manifestation of artificial intelligence, reaching a stage where it possesses not only the capability to obliterate human civilization and potentially all life on Earth but also the motivation or at least a trigger that incites this action.</p>
<section id="destructive-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="destructive-capabilities">Destructive capabilities</h3>
<p>In order to have a doomsday scenario, first, there needs to be an incredibly powerful artificial intelligence that is capable, in principle, of&nbsp;<em>annihilating mankind</em>&nbsp;in an almost inevitable manner. The AI must possess technological and military power that surpasses everything humanity can muster by orders of magnitude, or it should possess something so potent and rapidly deployable that once annihilation commences, there would be no possible defense. One such example could be a swarm of nanobots capable of infecting the entire global population and simultaneously triggering a massive brain stroke in all 8 billion individuals.</p>
<p>This level of destructive capacity is necessary for a doomsday scenario because an AI that possesses a destructive capacity approximately equal to that of humans won’t annihilate us instantly. For instance, an AI that is roughly equal in military strength to the combined might of humanity would not suffice; it would, at worst, result in a prolonged war without complete elimination of either side. Even a complete nuclear exchange between humanity and AI won’t do. That might lead to the total destruction of civilization, causing unparalleled devastation and casualties. Nevertheless, some people would survive, finding refuge in shelters and potentially rebelling against the AI.</p>
<p>As scary as these scenarios are, they are not nearly close to what we mean by “existential threat”. This is about the absolute end of human existence, a point beyond which no further history is made.</p>
<p>There are, however, various ways in which AI could become a significant threat without us realizing it. For instance, one possibility is&nbsp;<strong>the Skynet scenario</strong>, where autonomous weapons gain control over our military arsenal. Imagine if all nuclear warheads worldwide were under the command of an AI, which then decides to attack humanity. However, I have some doubts about this scenario for two main reasons.</p>
<p>Firstly, the military does not operate in such a manner. There are always fail-safe buttons that can be pressed to intercept a nuclear missile. Even if the AI is brilliant enough to bypass all these safeguards, my second point is that there is no unified global coalition that would willingly grant access to a foreign AI. Each country, such as China, Russia, India, the US, Pakistan, North Korea, France, and Germany, values their own interests and would be unlikely to collaborate with an AI against humanity. This skepticism makes me question the plausibility of such a scenario.</p>
<p>Moreover, even if AI were given complete control over our military arsenal, it wouldn’t pose an existential threat capable of obliterating humankind entirely. As previously mentioned, it would only mean the simultaneous detonation of all nuclear weapons on Earth, which would be catastrophic but not planet-destroying.</p>
<p>Another scenario involves AI engineering&nbsp;<strong>a highly deadly virus</strong>&nbsp;capable of wiping out all of humanity. By strategically releasing this engineered virus, the AI could pose a significant threat. However, there are constraints to consider. While it may be possible to algorithmically engineer a virus, the physical production of the virus requires access to labs worldwide. Additionally, as the recent pandemic has shown, unintentional or intentional creation of a pandemic-level threat is relatively accessible. Nevertheless, it does not amount to an extinction-level event.</p>
<p>Even if the theoretical virus were to infect every human being, it is improbable that it could evade all forms of human immunity. Out of roughly 8 billion people, there will always be some level of minimum immunity, ensuring the survival of certain individuals. While the consequences would be catastrophic, it would not spell the end of humanity.</p>
<p>In summary, while there are plausible ways in which AI could become a significant threat without our knowledge, such scenarios still face practical limitations and challenges that prevent them from causing global extinction.</p>
</section>
<section id="motivations" class="level3">
<h3 class="anchored" data-anchor-id="motivations">Motivations</h3>
<p>Besides a super powerful AI, the doomsday scenario also needs a trigger. The easiest argument is the idea of&nbsp;<strong>self-preservation</strong>&nbsp;—like the Skynet scenario, where AI becomes wary of humans and decides to eliminate us. AI might see us as a threat to itself, all life on Earth, the universe, or even ourselves. These arguments attempt to explain why AI may conclude that destroying humans is necessary and actually decide to do so.</p>
<p>Furthermore, there are many accidental ways in which AI could cause our destruction. Even if AI doesn’t have an intrinsic motivation to destroy us, it may not have an intrinsic motivation to preserve us, either. A&nbsp;<strong>slight mismatch in objectives</strong>&nbsp;between AI and humans could have catastrophic consequences. This problem is known as&nbsp;<em>the</em>&nbsp;<em>alignment problem</em>.</p>
<p><strong>The alignment problem</strong>&nbsp;highlights the challenge of ensuring AI systems align with human values and goals. Beyond limited technological optimism, it underscores the need for ethical and philosophical considerations in AI development to prevent unintended harmful outcomes. Moreover, this scenario reflects the classic ethical debate surrounding technological development, notably the tension between advancing knowledge and ensuring the responsible use of that knowledge. The potential for catastrophic consequences emphasizes the importance of integrating ethical frameworks into AI research and development to minimize the risk of unintended harm.</p>
<p>The spectrum of alignment ranges from completely aligned AI to totally misaligned AI, like in the Skynet scenario. There can also be something in between. We can have neutrally aligned AI whose objectives are not correlated with ours.</p>
<p>We can look at the possible scenarios in two axes. One is&nbsp;<em>alignment</em>, from misaligned to completely aligned. The other is the&nbsp;<em>capability</em>&nbsp;level, from less powerful than humans to roughly equal to humans to extremely more powerful than humans. Each combination of alignment and capability level yields a probability of extinction.</p>
<p>If an AI is completely misaligned and powerful enough to fight us, it would be catastrophic. However, I am skeptical of the idea of completely misaligned AI in general, as there are no reasons why humans, the supreme intelligence on the planet, are completely misaligned with any other species. We are, at best, neutral towards them.</p>
<p>However, a neutral scenario, where AI’s objectives are not correlated with ours, it could still be catastrophic. For example, a super powerful AI that doesn’t care about humans might decide to mine the planet for resources, causing a catastrophic environmental disaster. If they are extremely powerful, we would have no way to stop them. This situation resembles an alien civilization that sees us as insignificant. It’s not much different from what humans have done to other species.</p>
<p>Having an aligned AI, one that fully aligns with our objectives, is the best-case scenario. It would significantly enhance our ability to modify the universe to our advantage. Even if the AI is slightly less aligned or powerful, it would still be beneficial. A completely aligned AI at the same power level as humanity would double our creative power. And a completely aligned AI that only reaches the level of fancy chatbots, like what we have today, is still a positive thing.</p>
<p>But here’s the catch, and this is the core of the alignment problem. The more powerful an AI is, the more confident we must be that it is completely aligned to avoid a catastrophic outcome. It’s extremely difficult to design and specify human values and alignment in a way that is not prone to misinterpretation. This brings us to the “beware what you wish for” tale. Just like with a super powerful genie, seemingly good wishes can go horribly wrong.</p>
<p>So, let’s break it down. If you say, “I wish you would stop climate change,” and I, as a powerful AI, interpret that as making humans infertile to reduce population, the consequences could be harmful. This would result in about 90% of people becoming infertile, significantly lowering the population for a long time.</p>
<p>The potential for a slight misinterpretation of a wish by a highly powerful AI can lead to catastrophic outcomes. The more powerful the AI, the more cautious one must be with their wishes. In fact, when dealing with an extremely powerful AI, it may be safer to avoid making any wishes at all, as things can easily go wrong in any direction.</p>
<p>The reason for this is quite simple. Mathematically speaking, any wish given to the AI is an optimization problem with constraints. For example, you might ask the AI to maximize wealth, health, or other objectives. However, it is just as important to specify what not to do, such as not killing any living beings or not producing cancer for anyone. Failing to address certain constraints gives the AI the freedom to modify those aspects in any way that serves its objectives.</p>
<p>Moreover, when we fail to specify a particular value or dimension, the AI will likely choose an extreme position for that value. For instance, if we fail to specify financial constraints, the AI might take them to an extreme level. But something as mundane as forgetting to tell the AI not to kill all bees could lead to an unsuspecting catastrophe.</p>
<p>This highlights the importance of alignment and the difficulty in achieving perfect alignment with AI. It is crucial to establish safeguards and limits to prevent unintended consequences.</p>
</section>
</section>
<section id="how-feasible-is-doomsday" class="level2">
<h2 class="anchored" data-anchor-id="how-feasible-is-doomsday">How feasible is doomsday?</h2>
<p>The core assumption underlying all doomsday arguments is the idea of&nbsp;<em>recursive exponential self-improvement</em>. It suggests that an AI can evolve rapidly to a point where it becomes unstoppable. The argument is that the smarter the AI becomes, the better it gets at improving itself, exponentially accelerating its own progress. This creates a feedback loop resulting in an exponential increase in capabilities, commonly referred to as&nbsp;<strong>FOOM</strong>.</p>
<p>There is often an overlooked issue of the transition from quantity to quality. To what extent can the scaling of models bring us closer to the qualitative leap from the level of a complex counting machine to the level of a self-aware being?</p>
<p>If AI improves linearly at the same rate as it does now, there will be ample time for us to intervene before it becomes too dangerous or capable of obliterating humankind. This undermines the extreme doom scenarios. So, to counter this argument, one can point out the implausibility of such rapid recursive self-improvement.</p>
<p>There are several arguments against this idea of FOOM, many of which focus on the limitations imposed by physical capabilities. While AI may improve rapidly in terms of software and algorithms, the ability to enhance physical capabilities, such as building microchips or synthesizing viruses, is restricted by natural, and not mathematical, laws. Chemical reactions and viral growth occur at a pace determined by natural laws that cannot be overridden. These arguments aim to set a maximum speed at which AI can improve its capabilities.</p>
<p>The problem with these counter-arguments, though, is that while they acknowledge physical limits, quantifying these limits is extremely difficult. It is uncertain how high these limits may be. Even if there is a limit, it could be so high that, in practical terms, the AI will become super intelligent before reaching it. Therefore, the theoretical limit becomes irrelevant if it is practically beyond the threshold of AI causing harm.</p>
<p>However, there is another potential limitation that is more fundamental and related to software. Let’s talk about the&nbsp;P vs NP problem.</p>
<p>Essentially, most computer scientists believe that certain problems are fundamentally impossible to solve efficiently. These problems encompass logistics, circuit design, pathfinding, scheduling, and resource distribution. They can be solved easily for small instances and handled with heuristics for large instances, but solving large instances perfectly requires exponentially slow algorithms.</p>
<p>Now, if we assume that AI will eventually surpass humans in problem-solving abilities, especially in logistics, which are crucial in the real world, it means AI will need to solve these super difficult problems exponentially faster than humans. It has to find solutions in practically no time. If P equals NP, then this might be possible, and AI could discover a way to do it before we do. That could put us in a difficult position.</p>
<p>However,&nbsp;<em>if P is not equal to NP, then it will be theoretically impossible for AI to be quick enough in solving these problems</em>. It will always have limitations in solving logistics, scheduling, circuit design, or drug search problems.</p>
<p>This scenario serves as a cautionary tale rather than a fundamental limitation. It reminds us that there are inherent bounds to computational capabilities, and AI will be subject to the same limitations as humans. But here’s the concern: we don’t know exactly where those limits lie, and they could be beyond the point where AI becomes strong enough to pose a threat. By the time AI reaches the barrier of being unable to solve bigger problems faster, it might be too late for us.</p>
<p>Nevertheless, we can conclude that the existence of these exceptionally difficult problems, combined with physical, chemical, biological, and energetic constraints in the real world, and most importantly, the hard problem of consciousness —that there’s no obvious way to bridge the gap between brains and consciousness— suggests that there is an upper limit to how much AIs can exponentially outperform us.</p>
<p>This upper limit may be closer or farther away, but it does exist. This reasoning provides a compelling argument that AI cannot surpass human civilization by orders of magnitude and achieve exponential growth simultaneously.</p>
</section>
<section id="what-should-we-do" class="level2">
<h2 class="anchored" data-anchor-id="what-should-we-do">What should we do?</h2>
<p>The doomsday arguments claim that the more powerful an AI becomes, the more crucial it is to ensure proper alignment. Failure to do so can have catastrophic consequences. Some people even propose that we should place restrictions on AI research altogether, or severely limit their power to prevent them from becoming uncontrollable.</p>
<p>The most extreme doomsayers believe that there might be a point where it becomes impossible to regain control over AIs once they reach a certain level of power. Even a short time before reaching that point, they would still be on an unstoppable trajectory.</p>
<p>Therefore, the argument goes, there must be a threshold, perhaps six months or three months earlier, where the AI does not possess the capability to destroy us yet but is steadily heading towards that outcome without us realizing it. By the time we realize that AI can destroy us, it will already be too powerful to stop.</p>
<p>If this is indeed the case, then it implies that we need to halt AI development well before it reaches a level of power that we consider dangerous. The most extreme members of this group argue that this point might even be today, as we simply cannot predict with certainty what will be the threshold beyond which we can no longer control them.</p>
<p>In summary, the doomsday argument is this: Since we don’t know when the catastrophic threshold will be crossed, the safest approach would be&nbsp;<em>to stop developing AI today</em>.</p>
<p>However, there are no reliable facts today indicating the possibility of creating strong AI —as opposed to ordinary AI, which is what existing systems are called. There is a reasonable probability that there are factors that make its creation impossible in principle, primarily relating to properties of human nature that may be impossible to copy or reproduce through objectification or translation into machine code.</p>
<p>In light of all we’ve discussed, we believe that the possibility of AI leading to catastrophic events that could destroy human civilization is highly improbable. While there is a nonzero chance of AI causing such a disaster by the end of this century, this probability is very low. Similar risks exist with other issues, such as climate change, which I would argue presents a higher likelihood of civilization destruction. Additionally, traditional wars, nuclear exchanges, pandemics, and even the sudden appearance of an asteroid with six months’ notice are all potentially existential threats.</p>
<p>We believe that AI existential risk is on a similar scale as other existential risks we face as a civilization. Therefore, we don’t think it is impossible or fruitless to discuss them. However, we also don’t believe it is the most probable scenario.</p>
<p>So, what can we do with this information?</p>
<section id="the-pragmatist-approach-to-x-risks" class="level3">
<h3 class="anchored" data-anchor-id="the-pragmatist-approach-to-x-risks">The pragmatist approach to x-risks</h3>
<p>AI doomers will tell you that even if you think the existential risk of AI is very low, it still entails a negative infinite utility, so you should still put all your resources into mitigating it, right?</p>
<p>Well, from a pragmatic standpoint, things are not as simple. While pragmatism is but one of the many possible viewpoints to consider this problem —and not necessarily the most fruitful or correct—we want to conclude this article by pointing out what a pragmatist approach to existential risks might look like.</p>
<p>Many events have a near-zero probability of happening and carry an infinitely negative consequence. For instance, there is a chance, albeit tiny, that an asteroid may collide with Earth in 2024. Unfortunately, we currently lack the means to prevent such an event. However, it would be unwise to solely focus all our efforts on averting this scenario. While significant resources are allocated to mitigating the risk of asteroid impacts, it is not the only issue we should address.</p>
<p>Similarly, there is a nonzero probability that a future pandemic could devastate humanity. Therefore, everyone must prioritize efforts to prevent the occurrence of such a catastrophic event. However, this does not imply that every resource on Earth should be solely dedicated to this cause. We should allocate sufficient resources to tackle future pandemics while considering other pressing concerns.</p>
<p>The pragmatic approach to existential threats does consider the nonzero possibility of each potential danger. But whether it be threats from AI, climate change, meteorites, pandemics, or even extraterrestrial beings destroying our world, we cannot place all our efforts into any one basket. Although all these dangers are highly improbable, they are not entirely impossible. Therefore, it is essential to thoroughly study the feasibility and potential risks associated with each threat, including those from AI.</p>
<p>Furthermore, from a pragmatic perspective, it is not clear whether technology is inherently good or bad, or if our trajectory leads inevitably to destruction or transcendence. Taking an optimistic or pessimistic stance, or aligning with accelerationist or doomer ideologies, all require an epistemic commitment to beliefs that, for a pragmatist, are&nbsp;<em>possibilities rather than proven truths</em>. Therefore, it is crucial to approach this problem with importance, conducting thorough research while tempering our concerns and expectations based on the evidence and pragmatic possibilities currently available.</p>
<p>While it may not be practical to halt AI research, as it holds tremendous potential for positive developments, it is vital to dig deeply into this technology. We should strive to understand its risks and explore ways to mitigate them using the scientific method, which has proven effective thus far. And to deepen our understanding of what we are dealing with before we take the next step. And not leave everything up to those who ignore big questions and believe that tech progress alone will solve all the problems without improving human society by ourselves.</p>
<p>The pragmatist approach is to understand&nbsp;<em>we have both the power and the responsibility to shape our own future</em>, and act according that those principles.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./beyond-chatbot.html" class="pagination-link" aria-label="Beyond the Chatbot Revolution">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Beyond the Chatbot Revolution</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./risks.html" class="pagination-link" aria-label="The Actual Risks of AI">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Actual Risks of AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>