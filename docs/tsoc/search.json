[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Science of Computation",
    "section": "",
    "text": "Preface\nWhat is an algorithm? How does a computer work? Can computers do anything, or do they have inherent limitations? What is the Internet, and how can you stream videos from one side of the world to the other in almost real time? How is software made? What is artificial intelligence, and will it ever surpass us?\nThis book tries to answer these and many other, related questions. In the next few hundred pages, I’ll take you through a journey across the most important topics and themes in Computer Science. We will meet many of its most significant figures –from Alan Turing, Edger Djikstra, and Donald Knuth, to Geoffrey Hinton and Yann LeCunn– and see how they came up with some of the most brilliant ideas in the science of computation.\nWe will dive into some of the most fascinating discoveries in Computer Science, from the neural networks that power large language models, to the packet switching protocols that make the Internet possible. We will learn how computers work, in and out, why are they so powerful, but also what are their intrinsic limitations. In the end, you will gain a big-picture understanding of the whole field of Computer Science.\nThis book contains no complicated math or code. There are absolutely no prerequisites to read it, although it is perhaps better suited for high school-level students or older, because some concepts in Computer Science are somewhat sophisticated and rely on intuitions that students form around that age. It is also a perfect companion for a Computer Science major, as it complements the more theoretically- or technically-oriented bibliography you will encounter in college textbooks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-youll-find-in-this-book",
    "href": "index.html#what-youll-find-in-this-book",
    "title": "The Science of Computation",
    "section": "What you’ll find in this book",
    "text": "What you’ll find in this book\nComputer Science is a relatively new field that emerged in the early 20th century from a massive effort to formalize Mathematics. However, it wasn’t until the mid-60s and early 70s that it gained recognition as a distinct and unique scientific field. This was achieved by establishing the theoretical grounds of computability and complexity theory, which proved there were many interesting, novel, and challenging scientific questions involving computation.\nDespite its short time, Computer Science is an extensive field that encompasses a wide range of knowledge and skills. It goes from the most abstract concepts related to the nature of computable problems to the most practical considerations for developing useful software. To solve problems in this field, one must navigate through various levels of abstraction, making it challenging to provide a concise summary that the average reader can easily understand.\nThe fundamental concepts in Computer Science spread over a vast range of subjects, incorporating multiple areas of math and engineering. Thus it is impossible to divide this field objectively into subfields or separate areas with precise borders. In this book, I will attempt to categorize it into somewhat cohesive disciplines, but keep in mind there is considerable interconnectedness between them.\nThis book is divided into four parts, each subsequently divided into several chapters, encompassing all of Computer Science. The following sections will give you a taste of what you can find in each section and chapter. While the book is best read top-to-bottom—as some of the later ideas rely on earlier content—if you want to learn about a particular subject, feel free to hop and shop around. Whenever some earlier idea is important to understand what your reading, you will find useful links.\n\nFoundations of Computation\nWe’ll start our journey by looking at the foundations of Computer Science. The fundamental question of Computer Science is something akin to “What kinds of problems can be solved with algorithms.” To understand this question, we will look at fundamental concepts that underpin the whole field of Computer Science. Moreover, solving a concrete Computer Science problem will often require using one or more algorithms. Hence, studying which specific algorithm can solve which concrete problem is a big part of Computer Science. While a comprehensive study of the most important algorithms in Computer Science is out of the scope of this book, in the final chapter of this first part, we will look at the basic ideas for designing and evaluating common algorithms.\n\n\nComputational Systems\nMastering the foundations is not enough, though. Ultimately, we need a physical device that can run those algorithms and give us a result at a reasonable cost. Real computers are the physical embodiment of Turing machines, but the devil is in the details. Building a real computer is far more involved than just wiring some cables. Even assuming, as we do in CS, that electronic components always work as expected, the design of resilient computational systems is a whole discipline.\n\n\nSoftware Engineering\nBuilding software that works is extremely hard. It’s not enough to master all the algorithms, data structures, theorems, and protocols. Software products have unique qualities among all other engineering products, especially regarding flexibility and adaptability requirements. Any software that lives long enough will have to be changed in ways that weren’t predictable when the software was first conceived. This poses unique challenges to the engineering process of making software, the domain of Software Engineering. In the fourth part of the book, we will look at all the intricate components that participate in the process of getting an app ready for you to use.\n\n\nArtificial Intelligence\nOur final stop in this broad exploration of Computer Science, we will look at the amazing field of Artificial Intelligence. AI is a broad and loosely defined umbrella term encompassing different disciplines and approaches to designing and building computational systems that exhibit some form of intelligence. Examples of machines exhibiting intelligent behavior range from automatically proving new theorems to playing expert-level chess to self-driving cars to modern chatbots. We can cluster the different approaches in AI into three broad areas: knowledge, search, and learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#whats-next",
    "href": "index.html#whats-next",
    "title": "The Science of Computation",
    "section": "What’s next?",
    "text": "What’s next?\nComputer science is a vast field encompassing both theoretical and practical aspects. It goes from the most abstract computational concepts grounded in logic and mathematics to the most pragmatic applications based on solid engineering principles to frontier research like building intelligent systems. Working in CS also involves navigating complex social interactions between developers, users, and society and requires awareness of the many ethical issues that automation and computation pose.\nMastering computer science is a significant challenge, making it one of the most demanding academic pursuits. However, this discipline offers immense fulfillment since computation lies at the heart of our modern world. Software pervades all sectors and industries. Those proficient in computer science can find work across various disciplines and not only by creating commercial software. Across all scientific disciplines, from understanding the frontiers of space to solving climate change, designing new materials, and extending human life, every challenging problem in every scientific field has computational aspects that require collaboration between computer scientists and experts from that domain.\nThis book will not make you a computer scientist overnight. No book can. That requires years of learning and training. My purpose is much more humble. All I want is to light up your curiosity about these topics, and perhaps, motivate you to, one day, become a computer scientist yourself. At the very least, I hope that this book serves you to understand the where fundamental ideas in Computer Science come from, and how they permeate the technology you use every day, and shape the world you live in.\nOnce you finish this book, you will have an intuitive but solidly grounded understanding of the most important ideas in the vast field of Computer Science. But before we finish, we’ll have one final meetup. In the Epilogue, we will talk about what the near and not-so-near future holds for Computer Science, how we expect technology to change our society, and what can be your role in this future.\nIf any of that sounds intriguing, then read on!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "",
    "text": "The Dawn of Reason\nThe first major attempt to do something akin to formalizing reasoning is Aristotle’s Prior Analytics, where he invented what he called “syllogisms”. Aristotle defined a syllogism as a structured argument where a conclusion necessarily follows from two premises. For example, if we assert that “All men are mortal” (major premise) and “Socrates is a man” (minor premise), we can validly conclude that “Socrates is mortal.”\nThe beauty of this idea is not in what you can deduce of this precise claim–that Socrates is mortal, which is a rather trivial fact–but in its general structure. This syllogism can be generalized to something far more powerful, but still valid:\nWhile this may sound self-evident, the fact that we can apply such a reasoning rule for all possible Xs, Ys, and Zs is remarkable. This is the essence of formal logic: a set of rules for manipulating symbols that allow us to reason about the world in a systematic and rigorous way, regardless of what those symbols mean.\nNow, Aristotle went as far as to define a set of 256 different syllogistic forms. That means, 256 different, general-purpose structures like the one above, that can be applied to all possible objects and obtain true knowledge about them. However, modern logicians recognize that about 15 of those are sound in terms of logical validity.\nThe four main syllogistic forms that Aristotle considered in his first work are:\nThe first one is the more general form of the Socrates-is-mortal example. The other three are variantes where one of the main premises is true or false. All of them are self-evident by the nature of their structure, and you should convince yourself that they are true. Their soundness stems for the meaning of phrases like “all M are P” and “all S are M”. What these phrases mean–in terms of the relationship they define between M, P and S–immediately make the correspoding conclusion self-evident.\nAristotle’s work on what we can call “intuitive logic” is nothing short of transcendental. These four forms are the basis of modern logic, and they are still used today. They are the foundation of what is known as “first-order logic”, which is the most common form of logic used in mathematics and computer science, but we will get there.\nAristotle’s syllogisms were a major step forward in the formalization of reasoning, but they were limited in scope. They could only handle a small number of logical connectives (and, or, not) and a limited number of quantifiers (all, some). This was a significant limitation, as it meant that many arguments could not be expressed using syllogisms.\nOver the centuries, logicians would continue to refine and expand the scope of formal logic. In the Middle Ages, the Islamic Golden Age saw the preservation and expansion of Aristotle’s works, with scholars like Al-Farabi and Avicenna contributing new ideas and interpretations. During the Renaissance, the works of Aristotle and other classical logicians were rediscovered, and new ideas were added to the mix.\nHowever, the primary limitation of these syllogistic forms–that they are a fixed number of forms, and that they can only handle a limited number of logical connectives and quantifiers–would not be fully addressed until the 19th century.\nBut before going there, we have to make a stop, and learn about the dream of one of the most important mathematicians of all times.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#the-dawn-of-reason",
    "href": "intro.html#the-dawn-of-reason",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "",
    "text": "If all M are P, and S is M, then S is also P.\n\n\n\n\nIf all M are P, and all S are M, then all S are also P.\nIf no M are P, and all S are M, then no S are P.\nIf all M are P, and some S are M, then some S are P.\nIf no M are P, and some S are M, then some S are not P.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#the-universal-language",
    "href": "intro.html#the-universal-language",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "The Universal Language",
    "text": "The Universal Language\nThe modern history of Computer Science can be said to begin with Gottfried Leibniz, one of the most famous mathematicians of all time. Leibniz worked on so many fronts that it is hard to overestimate his impact. He made major contributions to math, philosophy, laws, history, ethics, and politics.\nHe is probably best known for co-inventing, or co-discovering, calculus along with Isaac Newton. Most of our modern calculus notation, including the integral and differential symbols, is heavily inspired by Leibniz’s original notation. Leibniz is widely regarded as the last universal genius, but in this story we want to focus on his larger dream about what mathematics could be.\nLeibniz was amazed by how much notation could simplify a complex mathematical problem. Take a look at the following example:\n“Bob and Alice are siblings. Bob is 5 years older than Alice. Two years ago, his age was twice as hers. How old are they?”\nBefore Algebra was invented, the only way to work through this problem was to think hard, or maybe try a few lucky guesses. But with algebraic notation, you just write some equations like the following and then apply some straightforward rules to obtain an answer.\n\\[\n\\begin{array}{rcl}\nb & = & a + 5 \\\\\nb - 2 & = & 2(a-2)\n\\end{array}\n\\]\nAny high-school math student can solve this kind of problem today, and most don’t really have the slightest idea of what they’re doing. They just apply the rules, and voila, the answer comes out. The point is not about this problem in particular but about the fact that using the right notation –algebraic notation, in this case– and applying a set of prescribed rules –an algorithm– you can just plug in some data, and the math seems to work by itself, pretty much guaranteeing a correct answer.\nIn cases like this, you are the computer, following a set of instructions devised by some smart “programmers” that require no creativity or intelligence, just to painstakingly apply every step correctly.\nLeibniz saw this and thought: “What if we can devise a language, like algebra or calculus, but instead of manipulating known and unknown magnitudes, it takes known and unknown truth statements in general?”\nIn his dream, you would write equations relating known truths with some statements you don’t know, and by the pure syntactic manipulation of symbols, you could arrive at the truth of those statements.\nLeibniz called it Characteristica Universalis. He imagined it to be a universal language for expressing human knowledge, independently of any particular language or cultural constraint, and applicable to all areas of human thought. If this language existed, it would be just a matter of building a large physical device –like a giant windmill, he might have imagined– to be able to cram into it all of the current human knowledge, let it run, and it would output new theorems about math, philosophy, laws, ethics, etc. In short, Leibniz was asking for an algebra of thoughts, which we today call logic.\nHe wasn’t the first to consider the possibility of automating reasoning, though. Having a language that can produce true statements reliably is a major trend in Western philosophy, starting with Aristotle’s syllogisms and continuing through the works of Descartes, Newton, Kant, and basically every single rationalist that has ever lived. Around four centuries earlier, philosopher Ramon Llull had a similar but narrower idea which he dubbed the Ars Magna, a logical system devised to prove statements about, among other things, God and the Creation.\nHowever, Leibniz is the first to go as far as to consider that all human thought could be systematized in a mathematical language and, even further, to dare dream about building a machine that could apply a set of rules and derive new knowledge automatically. For this reason, he is widely regarded as the first computer scientist in an age where Computer Science wasn’t even an idea.\nLeibniz’s dream echoes some of the most fundamental ideas in modern Computer Science. He imagined, for example, that prime numbers could play a major part in formalizing thought, an idea that is eerily prescient of Gödel’s numbering. But what I find most resonant with modern Computer Science is the equivalence between a formal language and a computational device, which ultimately becomes the central notion of computability theory, a topic we will explore in our first chapter.\nUnfortunately, most of Leibniz’s work in this regard remained unpublished until the beginning of the 20th century, when most of the developments in logic needed to fulfill his dream were well on their way, pioneered by logicians like Boole, Frege, and Cantor.\nWe will look at that part of the story next.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#the-rules-of-thought",
    "href": "intro.html#the-rules-of-thought",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "The Rules of Thought",
    "text": "The Rules of Thought\nWas Leibniz was looking for, ultimately, was an algebra for knowledge and reasoning. A way to describe truths about the world, and rules to combine them, such that new truths could be discovered. This idea of formalizing thought and reasoning new at Leibniz’s time, as we’ve seen. But the degree of formalization that Leibniz dreamed of wouldn’t arrive until the 19th century.\nThe next major step in this direction was the work of George Boole, an English mathematician who lived in the 19th century. Boole was a self-taught mathematician who started his career as a teacher. He was interested in the foundations of mathematics and logic, and he is credited with being the inventor of Boolean algebra, a mathematical system for reasoning about logical propositions.\nBoole’s system used symbols to represent logical propositions and operations to combine them. This means instead of a finite set of logical rules, as in Aristotle’s syllogisms, we can know come up with an infinite number of logical rules, all proven to be sound.\nTo see how this work, we have to understand what an algebraic language is about. In short, an algebraic language is a language that uses symbols to represent objects and operations to combine them.\nFor example, in arithmetic, we use symbols to represent numbers and operations to combine them. We define what symbols like 1, 2, or 156 mean, and what operations like plus, minus and squared do to any number. Then, we can make general claims like:\nIf X + Y = Z, then Y + X = Z\nHere, we are using X, Y, and Z to denote any possible number, and thus we can come up with infinite number of true statements. For example, we can know all of the following are true:\nIf 1 + 2 = 3, then 2 + 1 = 3\nIf 10 + 20 = 30, then 20 + 10 = 30\nIf 100 + 200 = 300, then 200 + 100 = 300\nAnd so on.\nThis is called the rule (or arithmetic axiom) of the commutativity of addition, which you probably know from elementary school. This rule is akin to one of Aristotle’s syllogisms. A fixed-form logical rule that contains some free variables, such that when we set these variables to appropriate values that make the premises true, then the conclusion is unequivocably true.\nBut contrary to Aristotle’s syllogism, we are not limited to a fixed number of such rules. If instead of saying X and Y are just numbers, we allow them to represent other arithmetic formulas, then these rules for combining numbers become rules for combining formulas themselves, which gives us new formulas ad infinitum.\nFor example, going back to our previous example, if we have:\n\\[\n\\begin{array}{rcl}\nb & = & a + 5 \\\\\nb - 2 & = & 2(a-2)\n\\end{array}\n\\]\nWe can apply the following rules to solve this equation:\n\nSince \\(b = a+5\\), using the rule \\(X = Y \\leftarrow X - Z = Y - Z\\), we can deduce that \\(b - 2 = a +  - 2\\).\nSince \\(b - 2 = 2(a-2)\\), using the rule \\(X = Y and X = Z \\leftarrow Y = Z\\), we can deduce \\(a+5-2=2(a-2)\\).\nThus, \\(a+3=2a-4\\).\nThis, in turn, means \\(3+4=2a-a\\).\nAnd so, \\(7=a\\).\n\nFollowing in this manner, we can then deduce that \\(b=5\\) using a similar reasoning.\nThis is exactly what Leibniz saw in algebra, and why he wanted a similarly powerful mechanism for manipulating general claims about the world instead of just numbers. And this is what Boole did–or started, as we’ll see we need a few more people to get to the most powerful modern logics.\nBoole’s system used symbols to represent logical propositions and operations to combine them. The symbols he used were 1 and 0, which he interpreted as true and false respectively. The operations he used were AND, OR, and NOT, which he interpreted as logical conjunction, disjunction, and negation respectively.\n\nThis is the essence of what we call “first-order logic”, which is the most common form of logic used in mathematics and computer science. It is a formal system that uses symbols to represent logical propositions and operations to combine them. It allows us to represent a much wider range of arguments and to prove a much wider range of theorems than the syllogistic forms.\nAnd this is where Boole’s work comes in. Boole’s system–known today as Boolean algebra–was the first algebraic logic system. It used symbols to represent logical propositions and operations to combine them. This allowed for the representation of more complex arguments than the syllogistic forms could handle. For example, the statement “If it is raining and the temperature is below freezing, then I will stay inside” could be represented as (R ∧ T) → I, where R is the proposition “It is raining”, T is the proposition “The temperature is below freezing”, and I is the proposition “I will stay inside”.\nBoole’s system used symbols to represent logical propositions and operations to combine them. The symbols he used were 1 to represent true and 0 to represent false. The operations he used were AND, OR, and NOT. These operations allowed for the representation of more complex arguments than the syllogistic forms could handle. For example, the statement “If it is raining and the temperature is below freezing, then I will stay inside” could be represented as (R ∧ T) → I, where R is the proposition “It is raining”, T is the proposition “The temperature is below freezing”, and I is the proposition “I will stay inside”.\nAnother major contribution came from Gottlob Frege, who in the late 19th century developed a formal logical system known as predicate logic. Predicate logic allowed for the representation of more complex arguments by introducing quantifiers that could be applied to variables, allowing for the expression of statements like “There exists an X such that X is Y” or “For all X, if X is Y, then X is Z”.\nThese developments, along with the work of other logicians like George Peacock, Augustus De Morgan, and Charles Sanders Peirce, laid the foundation for modern formal logic. Today, formal logic is used in a wide variety of fields, from mathematics and computer science to philosophy and linguistics. It is a powerful tool for reasoning about the world, and it continues to be an active area of research.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#to-infinity-and-beyond",
    "href": "intro.html#to-infinity-and-beyond",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "To Infinity and Beyond",
    "text": "To Infinity and Beyond",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#the-final-frontier",
    "href": "intro.html#the-final-frontier",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "The Final Frontier",
    "text": "The Final Frontier\n\nHilbert’s program to solve all mathematical problems.\n\nDavid Hilbert, a German mathematician, presented a list of 23 mathematical problems at the Second International Congress of Mathematicians in Paris in 1900. These problems were a call to arms for mathematicians around the world, proposing challenges that could fundamentally advance the discipline.\n\nConsistency of Mathematics: Hilbert proposed that mathematicians should strive to prove the consistency of mathematical systems. This problem aims to demonstrate that a given mathematical system does not lead to contradictions. The goal is to establish that the axioms and rules of a system are well-founded and do not lead to paradoxes or contradictions.\nCompleteness of Mathematics: Hilbert also proposed that mathematicians should strive to prove the completeness of mathematical systems. This problem focuses on ensuring that every true mathematical statement can be derived from the system’s axioms and rules. The goal is to show that the system is comprehensive enough to capture all mathematical truths.\nEntscheidungsproblem (Decision Problem): The Entscheidungsproblem asks whether there exists an algorithm that can determine whether a given mathematical statement is true or false. In other words, it seeks to find a method to automatically decide the truth or falsity of any mathematical statement. This problem is about the power of algorithms in solving mathematical problems and understanding the boundaries of computability.\n\nThese problems have profound implications for the nature of mathematical knowledge, the limitations of human understanding, and the relationship between logic and computation. They continue to inspire research and exploration in mathematical logic and computability theory.\nThese problems, although not all solved, made enormous strides in the development of mathematical logic and computability theory. They highlighted the interconnections between mathematics, logic, and computability, proving to be a rich source of mathematical exploration and discovery. The investigation of these problems continues to shape modern mathematics and computer science.\nThe unsolved problems, particularly the Entscheidungsproblem, would later lead to the work of Kurt Gödel and Alan Turing, who proved that it was impossible to create a universal algorithm that could decide the truth or falsity of all mathematical statements. This marked a turning point in the understanding of the limits of mathematical and computational knowledge.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "intro.html#crisis-in-infinite-math",
    "href": "intro.html#crisis-in-infinite-math",
    "title": "Prologue: The Quest to Answer all Questions",
    "section": "Crisis in Infinite Math",
    "text": "Crisis in Infinite Math\n\nThe crisis of math that started with Russell and then Gödel We end the prologue here, just before Turing.\n\nBertrand Russell was a British philosopher, logician, mathematician, and essayist. He is best known for his contributions to mathematical logic and philosophy, particularly his work on the foundations of mathematics and the paradoxes of set theory.\nBertrand Russell is significant because he discovered the Russell paradox, which shook the foundations of mathematics. In 1901, Russell formulated a set called the “Russell set,” which included all sets that are not members of themselves. The paradox arose when Russell asked if the Russell set was a member of itself. If it was, then by definition, it should not be a member of itself, leading to a contradiction. Conversely, if it was not a member of itself, then it should be a member of itself, again leading to a contradiction.\nThe Russell paradox highlighted the limitations and inconsistencies in the informal set theory that was prevalent at the time. It demonstrated that mathematics, as it was then understood, contained internal contradictions. The paradox challenged the theoretical framework of mathematics at its core and highlighted the need for a more rigorous foundation of mathematics.\nRussell’s work on logical and mathematical foundations, including the paradox he discovered, has had a profound impact on the development of modern mathematics, logic, and philosophy. His contributions have shaped the field of mathematical logic, and his ideas continue to be studied and debated by mathematicians and philosophers today.\nThe Russell paradox highlighted the need for a more rigorous foundation of mathematics. It shed light on the potential limitations of set theory and the need for clearer definitions and rules. The paradox revealed that mathematics, like any human construct, could be subject to inconsistencies and paradoxes, calling into question the completeness and consistency of mathematical systems.\nThese concerns led to the investigation of axiomatic systems and their limitations. Axiomatic systems are mathematical frameworks that consist of a set of axioms, which are taken as self-evident truths, and a set of rules for deducing new theorems from these axioms. The hope was that by constructing strong axiomatic systems, mathematics could be made free from inconsistencies and paradoxes.\nKurt Gödel’s incompleteness theorems, published in 1931, dealt a devastating blow to this hope. Gödel proved that strong axiomatic systems, such as the one proposed by David Hilbert’s program, cannot be complete and consistent.\nGödel’s first incompleteness theorem stated that any consistent axiomatic system that includes basic arithmetic cannot prove its own consistency. In other words, if a system is consistent, it cannot prove that it is free from contradictions. If it could, then it would be possible to construct a proof of its own inconsistency, leading to a contradiction.\nGödel’s second incompleteness theorem stated that any consistent axiomatic system that includes basic arithmetic cannot prove all true arithmetic statements. In other words, there are true statements about numbers that cannot be proven within the system.\nThese theorems demonstrated the inherent limitations of axiomatic systems in capturing all mathematical truths. They highlighted the tension between completeness, the ability to prove all true statements, and consistency, the absence of contradictions. Gödel’s results undermined Hilbert’s program, which aimed to establish mathematics on a firm foundation by proving all mathematical truths using a finite set of axioms and rules.\nThe crisis in infinite math, sparked by the Russell paradox and further exacerbated by Gödel’s incompleteness theorems, forced mathematicians to reevaluate the nature of mathematical knowledge and the limits of human understanding. It led to the development of new mathematical frameworks, such as intuitionistic logic and type theory, which provided alternative approaches to handling inconsistencies and paradoxes. Ultimately, these developments deepened our understanding of the foundations of mathematics and its relationship to logic and computation.",
    "crumbs": [
      "Prologue: The Quest to Answer all Questions"
    ]
  },
  {
    "objectID": "theory/turing.html",
    "href": "theory/turing.html",
    "title": "1  What is Computation?",
    "section": "",
    "text": "The Turing machine\nTuring’s initial idea was to formalize and abstract the concept of a computer. He began by studying the nature of human computers. These women computers used a sheet of paper, often divided into cells, on which they wrote various symbols, including numbers. Turing realized the two-dimensional aspect of the paper is not essential. It could be simplified into a one-dimensional tape of cells on which symbols could be placed, deleted, or added at any given moment.\nHe then observed that these individuals followed instructions, focusing only on one specific instruction at any given time. There was no need to consider multiple instructions concurrently, as they were performed sequentially. Additionally, these instructions were finite and entirely predetermined for a given problem. Regardless of the length of the numbers being multiplied, for example, the instructions for multiplication are always the same.\nFrom these observations, Turing conceived the idea of a computation machine, which he referred to as an “alpha-machine.” He imagined this contraption as a state machine with fixed states connected by transitions. These transitions dictated actions based on the observed symbol and current state. For example, if the observed symbol was a 3 and the machine was in state 7, the instruction would specify changing the number to a 4 and transitioning to state 11.\nDespite its simplicity, this concept encapsulates the fundamental principle of following instructions and utilizing an external memory, such as a tape, to store intermediate computations. By uniting these concepts, Turing laid the foundation for the conception of a computation machine —an abstract device capable of performing any type of computation.\nThe alpha-machine, now known as a Turing machine, is an abstract representation of a concrete algorithm. For instance, there can be a Turing machine designed explicitly for multiplying two numbers, a Turing machine for computing the derivative of a function, or even a Turing machine for translating from Spanish to English —as hard as that may seem. Each machine is a distinct algorithm akin to a specific program in Python or C++.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Computation?</span>"
    ]
  },
  {
    "objectID": "theory/turing.html#the-universal-computer",
    "href": "theory/turing.html#the-universal-computer",
    "title": "1  What is Computation?",
    "section": "The universal computer",
    "text": "The universal computer\nBut Turing took a further step, introducing the concept of a universal machine. This is where the brilliance of his idea shines. Turing deduced that a Turing machine’s description can be considered a form of data as well. We can take the states and transitions of a Turing machine and convert them into a long sequence of numbers, which can then be inputted into the tape of another Turing machine.\nA universal Turing machine is designed to receive the description of any concrete Turing machine, along with its respective inputs, on its input tape. The universal Turing machine then simulates the precise actions of the specific Turing machine, replicating its behavior with the given input. Turing demonstrated this could be done, thus inventing the modern concept of a general-purpose computer.\nAnd that’s how Alan Turing almost single-handedly invented the science of Computation —the theoretical field that explores the limits of computation and its practical implementation, now somewhat confusingly called “Computer Science.”\nRemarkably, Turing machines, despite their abstract nature, serve as the practical blueprints for modern computers. In modern computers, a microprocessor contains a fixed set of instructions, while random access memory provides us with a virtually unbounded tape —though not technically infinite. Hence, the concept of a universal Turing machine aligns closely with a modern computer.\nHowever, the story does not conclude here. You see, Turing had two fundamental questions in mind. First, he sought to determine the essence of an algorithm and how we can formalize the concept of computation. That’s the Turing Machine we just talked about. But, most importantly, he targeted the most crucial question regarding the limitations of mathematics and computation at the beginning of the 20th century.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Computation?</span>"
    ]
  },
  {
    "objectID": "theory/turing.html#the-answer-to-all-questions",
    "href": "theory/turing.html#the-answer-to-all-questions",
    "title": "1  What is Computation?",
    "section": "The answer to all questions",
    "text": "The answer to all questions\nAs we say in the Prologue, during this time, mathematicians were engaged in a significant undertaking to resolve mathematics altogether. They aimed to address all the remaining critical open questions in the field. David Hilbert had compiled a list of 20 questions that he regarded as the most pivotal in mathematics. While many questions on the list pertained to specific areas of mathematics, at least two questions dealt with the fundamental limits of the discipline itself.\nThe first question asked to prove the completeness and consistency of mathematics. This meant establishing that all truths can be proven and that mathematics has no internal contradictions.\nInitially, most mathematicians believed this to be true. However, Kurt Gödel’s incompleteness theorem dealt a major blow to this belief. Gödel demonstrated that in any sufficiently strong mathematical system, there are always truths that cannot be proven within that system. To prove them, looking outside the system and introducing new axioms was necessary. This revelation undermined the mathematicians’ quest to solve mathematics definitively.\nYet, one fundamental question remained, and it was also widely expected to be possible. This question examined whether, for all provable truths, there existed a completely mechanized procedure—an algorithm—that could find and deliver a proof within a finite amount of time. In simpler terms, it asked if computers could prove all provable theorems.\nSurprisingly, the answer also turned out negative. There are problems in math and computer science that have clear answers—either true or false—yet cannot be solved algorithmically. These are known as undecidable problems: no algorithm can determine the truth or falsehood of these problems for all instances. And that is not a limitation of current technology. It’s a fundamental limitation of the very notion of algorithm.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Computation?</span>"
    ]
  },
  {
    "objectID": "theory/turing.html#undecidable-problems",
    "href": "theory/turing.html#undecidable-problems",
    "title": "1  What is Computation?",
    "section": "Undecidable problems",
    "text": "Undecidable problems\nThere are two arguments to understand why computer science must have undecidable problems, using two of the most powerful in logic. The first is a diagonalization argument, very similar to Cantor’s original proof that the natural numbers are less than the real numbers.\nEssentially, any problem in computer science can be seen as a function that takes some input and produces some output. For example, the problem of adding two numbers can be represented as a function that takes two numbers and produces their sum. The question then becomes whether there are functions that cannot be solved by any theoretical machine or algorithm.\nNow, here’s the kicker. The number of possible problems, or functions, is uncountable, while the number of Turing machines is countable. We can list them all by enumerating all binary strings and determining which ones correspond to well-formed Turing machines, in the same way in which we can enumerate all Python programs simply by enumerating all possible strings and running the Python interpreter on each one.\nSo, the number of problems corresponds to the cardinality of real numbers, while the number of programs corresponds to natural numbers. Consequently, there must be infinitely many mathematical problems that cannot be solved by an algorithm.\nHowever, even if this is true, it is conceivable that we may not care about most of these unsolvable problems. They might be unusual or random functions that we do not find significant because for almost every problem we can think of, we can devise an algorithm to solve it. So, while there may be many solvable problems, their importance is subjective. Turing thus aimed to find one specific problem that was both important and unsolvable, and for that, he used the second most powerful tool in the logician’s arsenal.\nConsider a Turing machine and a specific input. The Turing machine can either run indefinitely in a loop or eventually halt. If the machine halts after a certain number of steps when given that input, we can determine this after a finite amount of time. However, if the machine never halts, we might never be able to tell whether it will halt. It may always be that it has not yet stopped but will do so at some point in the future. Therefore, running a Turing machine that never halts on a given input cannot tell us that it will not halt.\nTuring’s question, then, is whether it is possible to determine, by examining only the code and input of a Turing machine, whether it will halt or not, without actually running the machine. This is known as the halting problem.\nTo establish the undecidability of the halting problem, Turing employs a negated self-reference, a powerful method pioneered by Bertrand Russell with his barber’s paradox. The formal proof is somewhat involved, but the basic idea is pretty straightforward.\nFollowing Russell’s template, Turing presents a thought experiment assuming the existence of a “magical” Turing machine that can determine if any other Turing machine will halt without executing it. To show this is impossible, Turing pulls a Russell and creates a new Turing machine, using that magical halting machine, that halts when another arbitrary Turing machine doesn’t. By running the new machine on itself, Turing builds an explicit contradiction: the machine must halt if and only if it doesn’t halt. This contradiction proves that the existence of the magical Turing machine for the halting problem is inherently self-contradictory and, therefore, impossible.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Computation?</span>"
    ]
  },
  {
    "objectID": "theory/turing.html#why-this-matters",
    "href": "theory/turing.html#why-this-matters",
    "title": "1  What is Computation?",
    "section": "Why this matters?",
    "text": "Why this matters?\nWhat is the significance of the halting problem? For starters, with this result, Turing not only kickstarted computer science but also established its core limitations on day one: the existence of well-defined problems that cannot be solved by algorithms.\nHowever, although the halting problem may seem abstract, it can be interpreted practically as a fundamental limitation of the kind of software we can build. When writing complex code, we often want a compiler, or linter, to determine if our code is error-free before executing it. For instance, a modern compiler for a statically typed programming language can detect and notify developers of potential type errors before runtime, like using undefined variables or calling unexisting methods.\nIdeally, we would want a super smart compiler that could answer questions like: Will this program ever result in a null reference error? Is there a possibility of infinite recursion? Can this program cause an index-out-of-range exception? Unfortunately, these questions all trace back to the fundamental nature of the halting problem; they are, in general, undecidable.\nTherefore, the undecidability of the halting problem implies that most of the problems we could expect compilers to solve are fundamentally unsolvable. Consequently, automated program verification —the process wherein programs are checked for intended functionality by other programs— is generally undoable. We must find heuristics and approximate solutions to particular cases because no single algorithm will work on all possible such problems.\nThis limitation extends to the realm of artificial intelligence as well. These findings tell us that there are fundamental tasks computers will never be able to accomplish. Maybe this means AGI is impossible, but maybe it doesn’t. Perhaps humans are also fundamentally limited in this same way. Many believe human brains are just very powerful computers, and if we are ultimately something beyond fancy Turing machines, we might never be able to know.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Computation?</span>"
    ]
  },
  {
    "objectID": "theory/languages.html",
    "href": "theory/languages.html",
    "title": "3  Languages and Computation",
    "section": "",
    "text": "Languages",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Languages and Computation</span>"
    ]
  },
  {
    "objectID": "theory/languages.html#automata",
    "href": "theory/languages.html#automata",
    "title": "3  Languages and Computation",
    "section": "Automata",
    "text": "Automata",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Languages and Computation</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html",
    "href": "theory/complexity.html",
    "title": "4  Easy and Hard Problems",
    "section": "",
    "text": "When is algorithm better?",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#when-is-algorithm-better",
    "href": "theory/complexity.html#when-is-algorithm-better",
    "title": "4  Easy and Hard Problems",
    "section": "",
    "text": "Note\n\n\n\nThis section is under construction.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#the-hardest-problems",
    "href": "theory/complexity.html#the-hardest-problems",
    "title": "4  Easy and Hard Problems",
    "section": "The Hardest Problems",
    "text": "The Hardest Problems\nThere are many problems in computer science for which, even though they seem complicated at first, if we think hard enough, we can come up with clever algorithms that solve them pretty fast.\nA very good example is the shortest path problem. This is basically the problem of finding the shortest path between two given points in any type of network. For example, you have a map with millions of places and intersections in your city, and you have to go between one point at one extreme of the map and another point at the other end of the map.\nOur cleverest algorithms, on average, have to analyze very few options to get you the shortest path. They are very fast. This is what powers your GPS route planner, for example.\nHowever, other problems can sometimes be very similar to the first type, but no matter how hard we think, we cannot find any clever algorithm that works all the time.\nThe quintessential example of this is the traveling salesman problem. Consider a similar setting: you have a city and a bunch of places you need to visit in one tour. The question is, what is the best possible tour —the one that takes the least time or travels the least distance?\nThis problem seems very similar to the shortest path problem, but if you solve it by starting at one point and traveling to the closest next location iteratively, you can end up with a cycle that is several times worse than the best possible cycle.\nThere are approximate solutions that you can try in very constrained settings. Still, in the general case, no one has ever developed a clever algorithm that solves this problem any faster than checking all possible cycles.\nAnd the thing is, all possible cycles are a huge number.\nIf you have 20 cities, all possible tours amount to something close to 20 factorial, which is … (checks numbers) … well, huge. Not only that, if you have a computer today that can solve the problem for, let’s say, 20 cities in one second, then 22 cities will take nearly 8 minutes, 25 cities will take 73 days, and 30 cities will take you… 3 and a half million years!\nThis is an exponential solution, and they become bad really fast. But for many problems, like the traveling salesman, we have nothing better —that always works.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#p-and-np-problems",
    "href": "theory/complexity.html#p-and-np-problems",
    "title": "4  Easy and Hard Problems",
    "section": "P and NP problems",
    "text": "P and NP problems\nThe first type of problem, like the shortest path, are called P-problems —technically, polynomial time complexity, but don’t worry about that. P basically means problems that are easy to solve.\nNow, for the second type of problem, we don’t really know whether they are P-problems, but there is a subset of these that has an intriguing characteristic.\nIf I ask you to find me a cycle with, say, less than 100 km in total, that will be extremely hard. However, if I tell you, “here is a cycle that takes less than 100 km” you can easily verify it —just add up the length of each road.\nThese types of problems are called NP problems —technically, non-deterministic polynomial time complexity, but again, forget about that. NP basically means problems that are easy to verify.\nSo, the most fundamental question in computer science, P versus NP, ultimately is the following:\n\nP vs NP: Are all problems that are easy to verify also easy to solve? Or are there problems that are intrinsically harder to solve than to verify?\n\nIntuitively, for many, it seems it must be the latter. Think about what it would mean to be able to solve easily any problem that is also easy to verify. In a sense, it would be as if recognizing a masterpiece would be the same difficulty as creating the masterpiece in the first place. Being a great critic would be the same as being a great artist.\nThis seems false, but intuitions in math and computer science are often wrong. We cannot use this kind of analogy to reach any sort of informed conclusion.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#are-there-really-hard-problems",
    "href": "theory/complexity.html#are-there-really-hard-problems",
    "title": "4  Easy and Hard Problems",
    "section": "Are there really hard problems?",
    "text": "Are there really hard problems?\nHowever, theoretical reasons hint at the possibility that P is indeed distinct from NP —that there are some truly, fundamentally difficult problems. The strongest one is the existence of so-called NP-complete problems. Problems, like the traveling salesman, so difficult to solve efficiently that if you could solve one of them, you would solve all the other NP problems at the same time.\nWhen we say a problem is difficult, we mean that it is exponentially easier to verify a solution’s correctness than it is to find the solution in the first place. This concept is crucial because, for most problems in computer science, such as sorting, searching for elements in an array, or solving equations, the effort required to solve the problem is roughly the same as the effort needed to verify the solution.\nIn technical terms, the advantage of verifying over solving is only polynomially easier, not exponentially. However, there are certain problems, like the traveling salesman problem, for which we have been unable to find a fast and efficient algorithm – only exponential ones. Nevertheless, these solutions are very easy to verify.\nThe heart of the P versus NP debate lies in whether these inherently difficult problems truly exist. Do problems exist that are far more challenging to solve than to verify? To answer this question fully, we would need to find a problem for which a polynomial time algorithm cannot exist.\nP vs NP remains an unsolved question, and although it has seen a lot of progress, it appears to hint at the need for a new kind of mathematics. Our current mathematical methods lack the power to tackle these challenging meta-questions. However, we do have the next best thing —a wealth of empirical and theoretical evidence suggesting that, indeed, many of these problems may be unsolvable efficiently.\nOne of the simplest forms of empirical evidence is the vast number of problems for which we lack a polynomial time algorithm to solve them. However, this evidence is not very strong, as it could simply mean we aren’t smart enough to find those algorithms.\nWhat we need is a more principled way of answering this question. When mathematicians are faced with an existential problem —in the sense of, does there exist an object with these properties, not in the sense of, you know, God is dead and all—what they do is try and find extreme cases that can represent the whole spectrum of objects to analyze.\nIn this case, we are dealing with finding problems that are difficult to solve. So it makes sense to ask, “What is the most difficult problem possible?” A problem so hard that if we crack it, we crack them all.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#one-problem-to-rule-them-all",
    "href": "theory/complexity.html#one-problem-to-rule-them-all",
    "title": "4  Easy and Hard Problems",
    "section": "One problem to rule them all",
    "text": "One problem to rule them all\nThat’s the idea behind NP-completeness. Let’s focus on these super tricky problems – the toughest ones in this field. If there are problems so tough that solving just one of them would also solve all the others, that would seriously challenge the idea of P equals NP.\nThese really tough problems are called NP-complete problems, a concept defined by Stephen Cook in the 1970s. An NP-complete problem is basically a problem that is NP —that is, easy to verify—, and a solution to it in polynomial time would also give us a solution to any other problem in NP. So, in a way, these are the only problems we need to look at. If we solve one of these, we’ve proven P equals NP. And if we can’t solve just one, we’ve proven P not equal to NP.\nIn short, we just need to focus on NP-complete problems. So, the main question then becomes: Are there problems so complex that they are essentially equivalent to all other problems?\nObviously, the hardest problems wouldn’t revolve around everyday topics like finding paths in maps, sorting numbers, or solving equations. No, these problems should be much more abstract, so that they could encompass numerous other problems in their definition.\nCook’s idea was to create a problem centered around problem-solving itself. For instance, how about simulating a computer? That would be an incredibly abstract problem. To make it even more challenging, they could turn this computer simulation into a decision problem —keep in mind that NP problems are decision problems.\nOne way to define this problem is to imagine having an electronic circuit that computes some logical formula —this is all computers do at their core, as all computable arithmetic can be reduced to logic. Let’s take the simplest logical circuit, a completely stateless one, and ask: Is there any input that will make this circuit output True? If so, we call the circuit satisfiable. Otherwise, it is deemed unsatisfiable.\nHence, given an arbitrary logical circuit, the task of determining if it’s satisfiable or not is called circuit satisfiability, or Circuit-SAT for short.\nIn 1970, Stephen Cook proved that Circuit-SAT is as hard or harder than all other NP problems. If you can solve circuit satisfiability, you can solve any other problem in NP. This means you can solve the traveling salesman problem and the knapsack problem, but also sorting, searching, and basically any easily verifiable problem.\nThe proof of this idea is quite involved and complex, and not something I can fully explain in this post, so I’ll save that for a more detailed discussion later. But basically, since logical circuits are extremely expressive and powerful, and can compute almost everything, any decision problem in computer science can be transformed into a logical circuit that simulates it solution. Cook’s proof actually involves constructing a circuit for an abstract NP problem, so it’s pretty technical. But the intuition is that these things are basically computers, so you’re just simulating any possible (decision) algorithm.\nAnd voilá! This proves the existence of at least one NP-complete problem, meaning there is one problem in NP that is as difficult as all problems in NP. Stephen Cook’s work in 1971 essentially kick-started the entire field of computational complexity and defined the most important question in computer science: P versus NP.\nThe story, however, doesn’t just end there. Circuit-SAT is great, but it is too much of an abstract problem. Just one year later, Richard Karp came along and demonstrated that 21 well-known and very concrete computer science problems were also NP-complete problems. These problems included the traveling salesman problem, the knapsack problem, various scheduling problems, and many graph coloring problems. In short, there turned out to be a whole bunch of problems that fell under this category, not just some abstract circuit simulation task.\nThese NP-complete problems aren’t just theoretical issues, either. They are practical problems that we encounter regularly in logistics, optimization, and scheduling. After Karp proved his 21 original NP-complete problems, a wave of people started proving that nearly any problem in computer science involving combinatorics could be classified as NP-complete. As a result, there are now over 4,000 papers proving different NP-complete problems, ranging from determining the best move in Tetris to folding proteins.\nThis compelling evidence has led many computer scientists to believe that P != NP and that there are, indeed, problems that are fundamentally harder to solve, not just because we lack some understanding about them but because, by their very nature, they just cannot be solved efficiently.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#problems-harder-than-np-complete",
    "href": "theory/complexity.html#problems-harder-than-np-complete",
    "title": "4  Easy and Hard Problems",
    "section": "Problems harder than NP-Complete",
    "text": "Problems harder than NP-Complete\n\n\n\n\n\n\nNote\n\n\n\nThis section is under construction.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "theory/complexity.html#what-if-pnp",
    "href": "theory/complexity.html#what-if-pnp",
    "title": "4  Easy and Hard Problems",
    "section": "What if P=NP?",
    "text": "What if P=NP?\nIf P=NP, the world would be a very weird place. It would be just as easy to understand a solution to a hard problem than finding it in the first place. In a sense, the critic and the artist would be the same. Many people find this unacceptable, but intuition is not enough to convince us about something as profound as a fundamental limit to general intelligence.\nIf it turns out that if P != NP, then there are some fundamental limits to how fast a computer can solve the majority of the most important problems in logistics, planning, simulation, etc. While we have many heuristics to solve either some cases perfectly or most cases approximately, all these problems may ultimately be unsolvable quickly. And by “computer” we don’t mean just the run-of-the-mill electronic computer you have at home. We mean, any device that can compute things like a Turing machine does.\nMany believe brains are just pretty complex computers, but computers after all. If that’s the case, then no intelligent being in the Universe, no matter how smart, can outsmart NP-Hard problems. Crucially, not even a superadvanced alien —or computer— could be exponentially faster than us. Thus, P vs NP might be our best weapon to stop a self-improving AGI from reaching superhuman capacity.\nEven gods can’t escape complexity theory.",
    "crumbs": [
      "Foundations of Computer Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Easy and Hard Problems</span>"
    ]
  },
  {
    "objectID": "ai/ai.html",
    "href": "ai/ai.html",
    "title": "5  Why we need Artificial Intelligence",
    "section": "",
    "text": "At the bottom of any working piece of software, there are often many algorithms. An algorithm, for the purposes of this post, is just a series of very well-defined instructions that a computer can follow to solve a concrete problem.1\nFor example, if you want to sort a list of numbers, the easiest way to do it is to scan the list to find the smallest one, put it at the beginning of the list, and repeat until no elements are left unsorted. This is a rather slow, but certain way to sort any list of numbers, regardless of its size and content. And we have many other such algorithms, that work better or worse in specific circumstances. Sorting, thus, is mostly a solved problem.2\nSolving a problem in Computer Science usually means finding a well-designed algorithm that we can formally prove works both reliably and fast.3 The kind of algorithms that are used throughout the vast majority of software deployed today: e.g., searching for some specific items in a database, sorting and grouping objects based on some criteria, or solving some fancy equations.\nWe have plenty of those algorithms for plenty of areas in Computer Science, but there are at least two broad areas of problems so hard, that we don’t have any good solutions.\nThe first area concerns those problems that we believe are intractable. These are problems most experts believe simply can’t be solved efficiently, or at the very least, no one in Computer Science has a freaking clue if they can. We know these from Chapter 4 and even have a fancy name for them: NP-Hard problems.\nThe gist of NP-Hard problems is that, if you solve just one of them efficiently, you would solve a huge number of other problems touching all areas of Computer Science, problems that hundreds of thousands of computer scientists have attempted to solve efficiently for decades.\nOne of the most popular examples, and probably the oldest, is the Travelling Salesman Problem (or TSP for short). In a common variation of the TSP, you’re given a bunch of cities connected by roads with some associated cost (maybe the traveling distance, or the price of a plane ticket). You have to visit all cities exactly once while minimizing the total cost of the tour.\nThe most intuitive solution is probably just going to the nearest city, and from there to the next nearest, and so on. This is called a “greedy strategy” and, as most grandmas know, it can be catastrophically bad.\nThere are exact solutions, of course. The easiest (and slowest) one consists in enumerating all possible tours and selecting the one with the lowest cost, what in CS-lingo we call a “brute force search”. Thing is, in general, no other strategy has been found that always gives you the best tour and that is qualitatively better (in terms of computational performance) than brute forcing it.4\nWhat can we do in these cases? When no exact and fast algorithm can be found, we have to sacrifice either correctness or performance. Since we’re almost always interested in performance, we end up designing “approximate” solutions: solutions that are good enough, given the time and memory constraints we have. This is the realm of search algorithms.\nThe second area of hard problems involves problems that are unstructured. These may be worse than NP-Hard, in a sense: they have so little structure that we have no idea where to even begin a brute search.\nMany of the problems in this domain are related to perception: e.g., how to distinguish a face in a photograph; others are related to higher cognitive functions that are equally opaque to our own minds, e.g., how to translate a text into another language, changing the actual words, but keeping the semantics, intentions, and style of the author; and others deal with many variables that have complex and hard to understand interactions, e.g., which are the best movies to recommend a given person.\nAll of them share a common property, though: we hint there are patterns that determine the correct result that we don’t know how to code explicitly. However, we may have access to solved examples, e.g., explicitly in the form of input/output pairs or implicitly in the form of simulations we can run.\nIn these cases, we want to find patterns in data, i.e., infer the rules that determine what a face is, the meaning of a phrase, or the things you want in a movie. In a way, the pattern is the algorithm, so the task is to discover the correct algorithm given examples of solved instances. This is the realm of learning algorithms.\nThere is a third area of interest for AI that complements these two. It deals with the problem of efficiently representing, storing, and using domain knowledge in a computationally tractable way.\nThis involves finding the correct way to organize concepts and facts, as well as the optimal way to reason about them and produce new concepts and facts that are sound. A prime example is ontologies, or the more general notion of knowledge graphs: data structures representing facts about certain entities and their relations. This is the realm of knowledge representation and reasoning.\nThese three paradigms—searching, learning, and reasoning—encompass what is known as Artificial Intelligence. The remainder of this article deals with the first paradigm, and future articles will deal with the other two.",
    "crumbs": [
      "Artificial Intelligence",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Why we need Artificial Intelligence</span>"
    ]
  },
  {
    "objectID": "ai/search.html",
    "href": "ai/search.html",
    "title": "6  Search",
    "section": "",
    "text": "Heuristic search\nA heuristic is a rule-of-thumb strategy that exploits some known properties of a search problem to improve search performance. One of the most well-known and used heuristic search algorithms is A*, which powers the GPS routing app on all phones.\nA* is a graph search algorithm that finds the shortest path between an origin and one or many equivalent destinations. At its core, it is very similar to Dijkstra’s shortest path algorithm, but one key idea makes it incredibly faster: it assumes some information about the location of destination nodes.\nFor example, in GPS routing, you don’t exactly know, without a full search, what the shortest path to get someplace from your home is. But if that place is northeast of your position, assuming some reasonable city layout, it makes sense that roads going north or east are more likely to get you there faster than roads going to the west or the south.\nOf course, this isn’t always the case since there could be detours, highways, tunnels, etc. If you blindly go toward the destination, you could end up stuck in a dead end. However, this useful knowledge can be exploited intelligently to make search much more efficient if you’re clever about it, and this is what A* does.\nInstead of blindly following the most direct route, A* keeps track of all routes, even those that extend in the opposite direction. However, contrary to traditional graph search algorithms, it doesn’t explore all routes with the same effort. Instead, when choosing the next node to explore, A* will consider with a higher probability those nodes that the heuristic favors—e.g., those that lie in a direction more closely aligned with the destination—but it takes care not to get stuck, and it’s able to backtrack when necessary.\nIn the end, A* performs no worse than traditional graph search algorithms like Dijkstra’s and often does it much better, provided your heuristic is well-designed.",
    "crumbs": [
      "Artificial Intelligence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "ai/search.html#metaheuristics",
    "href": "ai/search.html#metaheuristics",
    "title": "6  Search",
    "section": "Metaheuristics",
    "text": "Metaheuristics\nWhere heuristics are problem-specific strategies to leverage domain insights and accelerate the search in a concrete problem, metaheuristics are general-purpose search strategies that leverage knowledge about the search paradigm itself.\nMetaheuristics are domain-independent search algorithms that can be applied in any case where nothing else works. Solving TSP is one of the most common examples, but their uses extend to anything from routing to logistics and planning.\nA primer example of a metaheuristic approach to problem-solving is the field of evolutionary algorithms. These are computational strategies inspired by certain aspects of the biological process of evolution that attempt to extract key mechanisms that make evolution work and apply them in the context of, say, finding the optimal way to design a GPU.\nThe basic idea of all evolutionary algorithms is to maintain a “population” of solutions (e.g., different circuit designs for a GPU) that undergo cycles of “breeding” and “selection”, much like biological evolution. An example of a breeding scheme is taking two of these potential GPU designs and build a new design that incorporates some elements from either of the “parents”. This creates a new solution that is, in some sense, an evolved version of previous solutions, which may make it better or worse.\nThen, we apply some selection criteria, most often just evaluating the solutions on whatever problem you’re solving—e.g., which GPU design works faster at certain benchmarks—and keep the best 50%. This cycle of breeding and selection is repeated, producing a sequence of increasingly specialized GPU designs that, almost like evolution, seems to magically discover quasi-optimal design elements just by sheer luck and relentless repetition.\nMuch more can be said about evolutionary algorithms and metaheuristics in general. Still, the overall idea is as simple as that: find effective search strategies that seem to work in almost all domains—finding inspiration in nature, engineering, social networks, etc.—and build computational strategies that leverage these insights.",
    "crumbs": [
      "Artificial Intelligence",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "epilogue.html",
    "href": "epilogue.html",
    "title": "The Road Ahead",
    "section": "",
    "text": "Cracking the Turing Test\nWe started the last chapter with Turing’s definition of intelligence –or thinkking, to be precise– by emphasizing that Turing didn’t expect his imitation game to be implemented as an objective test of “intelligence” for computers. Yet, it has been interpreted as such by myriad computer experts and non-experts alike over the years, and many attempts have been made to operationalize it. It doesn’t help that Turing claims he hoped by the year 2000, we would have computers that could, in principle, beat 30% of the judges in such a setup. Many took this as an objective milestone to claim we reached strong AI.\nAny concrete implementation of the imitation game runs into the practical issues of human gullibility and biases, which makes it almost impossible to select judges who are guaranteed not to fall for cheap tricks. These issues alone explain all the occasions before 2022 when someone claimed they beat the Turing Test.\nHowever, starting in 2023, for the first time, we have technology that is eerily close to what many people would claim a worthy contender for the imitation game: large language models (LLMs). Some of the wildest claims about modern LLMs like GPT-4 seem to imply the most powerful of these models are capable of human-level reasoning, at least in some domains.\nBut can modern language models pass the Turing Test? Again, this is hard to evaluate objectively because there are so many implementation details to get right. But I, and most other experts, don’t believe we are there yet. For all their incredible skills, LLMs fail in predictable ways, allowing any sufficiently careful and well-informed judge to detect them. So no, my bet is current artificial intelligence can’t yet beat the imitation game, at least in the spirit originally proposed by Turing.\nIn any case, there’s no GPT-4 out there consistently tricking humans into believing it is one of us.\nOr is it? And how would we know?",
    "crumbs": [
      "The Road Ahead"
    ]
  },
  {
    "objectID": "theory/intro.html",
    "href": "theory/intro.html",
    "title": "Foundations of Computer Science",
    "section": "",
    "text": "We’ll start our journey by looking at the foundations of Computer Science. The fundamental question of Computer Science is something akin to “What kinds of problems can be solved with algorithms.” It was first asked formally by David Hilbert in 1901 and first answered by Alan Turing in 1936. This is the central question of computability theory, that establishes the foundational theory for what can be done with any computer.\nWe will examine this question first, in 1  What is Computation?. There we will encounter the concept of a Turing machine, an abstract computer model that we define to study the properties of computation and computable problems irrespective of any concrete machine our current technology supports. Turing machines let us answer which kinds of problems are, in principle, undecidable —which means that no algorithm can ever be devised to solve them completely. Surprisingly, there are many of those, not all esoteric; there are very practical problems in Computer Science that we know are mathematically impossible to solve.\nTuring machines also give a precise definition of algorithm, and we will tackle this notion next, in 2  Algorithms, Lots of Algorithms. We will see how algorithms can be used to systemathize the solution to computational problems, and we will learn the characteristics of good algorithms. We will also see some core ideas that help computer scientists design and analyze algorithms, such as recursion, divide-and-conquer, and dynamic programming.\nThe next ingredient in the foundational theory of Computer Science is the surprising relationship between languages and machines. Formal language theoryallows us to formalize the notion of a language, whether it is a human language like English or Spanish, a programming language like Python or C#, and any technical, abstract, or mathematical notations used in many fields. We will look at the intriguing world of computational languages in 3  Languages and Computation.\nFinally, once we lay out the limits of computation, we can ask which computable problems are easier or harder. Complexity theory deals with the complexity of different problems, and we will meet it in 4  Easy and Hard Problems. It asks how efficiently, most commonly in terms of computing time and memory, we can solve any problem. For some problems, we can even prove that we have found the most efficient algorithm anyone could ever develop. The most important problem in complexity theory, and probably in Theoretical Computer Science, is the famous question of P vs NP, which is ultimately a question about the nature of really hard problems.",
    "crumbs": [
      "Foundations of Computer Science"
    ]
  },
  {
    "objectID": "software/intro.html",
    "href": "software/intro.html",
    "title": "Software Engineering",
    "section": "",
    "text": "Building software that works is extremely hard. It’s not enough to master all the algorithms, data structures, theorems, and protocols. Software products have unique qualities among all other engineering products, especially regarding flexibility and adaptability requirements. Any software that lives long enough will have to be changed in ways that weren’t predictable when the software was first conceived. This poses unique challenges to the engineering process of making software, the domain of software engineering. In the fourth part of the book, we will look at all the intricate components that participate in the process of getting an app ready for you to use.\nSoftware construction starts with a programming language, of which there are plenty of variants. Some programming languages are designed for specific domains, while others are called general purpose and designed for any programming task. A fundamental distinction is between imperative and declarative languages. The most influential paradigms are functional (often declarative) and object-oriented (most commonly imperative) programming models.\nProgramming languages are tools, so their effectiveness in solving a concrete problem relies heavily on applying best practices. Software engineering also deals with finding principles and practices to use better the available tools, including technical tools and human resources. The most important set of software engineering principles is the SOLID principles. Other principles, such as DRY and YAGNI, emphasize specific, pragmatic mindsets about software development. Additionally, we have dozens of design patterns: concrete reusable solutions to common software development problems.\nBeyond the actual code, the most crucial resource most applications must manage is data. As the necessity to organize, query, and process larger and larger amounts of data increases, we turn from classic in-memory data structures to more advanced long-term storage solutions: databases. The relational database is the most pervasive model for representing and storing application data, especially structured data. However, several alternative non-relational paradigms are gaining relevance as less structured domains (such as human language and images) become increasingly important.\nIn the data-centric software world, information systems are some of the most complex applications we can find. They provide access to vast amounts of information, which can be from a concrete domain —such as medical or commercial information— or general purpose like search engines. These systems combine complex algorithms and data structures for efficient search with massively distributed hardware architectures to serve millions of users in real-time.\nAs we’ve seen, software ultimately runs on someone’s computer, and different computational systems pose different challenges. From a software engineering perspective, we can understand those challenges better if we think in terms of platforms. The three major software platforms are the desktop (apps installed in your computer), the mobile (apps installed in your smartphone), and the web (apps running on your browser).\nDeveloping desktop software is hard because different users will have different hardware, operating systems, and other considerations, making it hard to build robust and portable software. Mobile development brings additional challenges because mobile devices vary widely and are often more limited than desktop hardware, so performance is even more crucial. The web is the largest platform, and we’ve already discussed distributed systems. The web is an interesting platform from the software engineering perspective because it is pervasive and lets us abstract from the user’s hardware. Perhaps the most famous software development technologies are those associated with web development: HTML, CSS, and JavaScript.\nThe final component in software engineering concerns the interaction between the user and the application. Human-computer interaction (HCI) studies the design of effective user interfaces, both physical and digital. Two major concerns are designing an appropriate user experience, and ensuring accessibility for all users, fundamentally those with special needs such as limited eyesight, hearing, or movement.\nBut before moving on, we will look at one particularly interesting type of software: videogames. We will see how modern game engines work, from physics simulation to graphic rendering. But, more importantly, we will see what is involved in the process of creating a AAA videogame, which is one of the most challenging and complex types of software currently made.",
    "crumbs": [
      "Software Engineering"
    ]
  },
  {
    "objectID": "ai/intro.html",
    "href": "ai/intro.html",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Our final stop in this broad exploration of Computer Science, we will look at the amazing field of Artificial Intelligence. AI is a broad and loosely defined umbrella term encompassing different disciplines and approaches to designing and building computational systems that exhibit some form of intelligence. Examples of machines exhibiting intelligent behavior range from automatically proving new theorems to playing expert-level chess to self-driving cars to modern chatbots. We can cluster the different approaches in AI into three broad areas: reasoning, search, and learning.\nKnowledge representation and reasoning studies how to store and manipulate domain knowledge to solve reasoning and inference tasks, such as medical diagnosis. We can draw from Logic and formal languages to represent knowledge in a computationally convenient form. Ontologies are computational representations of the concepts, relations, and inference rules in a concrete domain that can be used to infer new facts or discover inconsistencies automatically via logical reasoning. Knowledge discovery encompasses the tasks for automatically creating these representations, for example, by analyzing large amounts of text and extracting the main entities and relations mentioned. Ontologies are a special case of semantic networks, graph-like representations of knowledge, often with informal or semi-formal semantics.\nSearch deals with finding solutions to complex problems, such as the best move in a chess game or the optimal distribution of delivery routes. The hardest search problems often appear in combinatorial optimization, where the space of possible solutions is exponential and thus unfeasible to explore completely. The most basic general search procedures —Depth-First Search (DFS) and Breadth-First Search (BFS)— are exact, exhaustive, and thus often impractical. Once you introduce some domain knowledge, you can apply heuristic search methods, such as A* and Monte Carlo Tree Search, which avoid searching the entire space of solutions by cleverly choosing which solutions to look at. The ultimate expression of heuristic search is metaheuristics —general-purpose search and optimization algorithms that can be applied nearly universally without requiring too much domain knowledge.\nMachine learning enables the design of computer programs that improve automatically with experience and is behind some of the most impressive AI applications, such as self-driving cars and generative art. In ML, we often say a program is “trained” instead of explicitly coded to refer to this notion of learning on its own. Ultimately, this involves finding hypotheses that can be efficiently updated with new evidence.\nThe three major paradigms in this field are supervised, unsupervised, and reinforcement learning. Each case differs in the type of experience and/or feedback the learning algorithm receives. In supervised learning, we use annotated data where the correct output for each input is known. Unsupervised learning, in contrast, doesn’t require a known output; it attempts to extract patterns from the input data solely by looking at its internal structure.\nReinforcement learning involves designing systems that can learn by interaction via trial and error. Instead of a fixed chunk of data, reinforcement learning places a learning system —also called an agent in this paradigm— in an environment, simulated or real. The agent perceives the environment, acts upon it, and receives feedback about its progress in whatever task it is being trained on. When the environment is simulated, we can rely on different paradigms, such as discrete events or Monte Carlo simulations, to build a relatively realistic simulation. Reinforcement learning is crucial in robotics and recent advances in large language models.\nAll of the above are general approaches that can be applied to various domains, from medical diagnosis to self-driving cars to robots for space exploration. However, two domains of special importance that have seen massive improvements recently are vision and language. The most successful approaches in both fields involve using artificial neural networks, a computational model loosely inspired by the brain that can be trained to perform many perceptual and generative tasks. ANNs draw heavily from algebra, calculus, probability, and statistics, representing some of the most complex computer programs ever created. The field of neural networks is alternatively called deep learning_, mostly for branding purposes.\nComputer vision deals with endowing computational systems with the ability to process and understand images and videos for tasks like automatic object segmentation and classification. Classic approaches to computer vision rely heavily on signal processing algorithms stemming from algebra and numerical analysis. Modern approaches often leverage neural networks, most commonly convolutional networks_, loosely inspired by the visual parts of animal brains.\nNatural language processing (NLP) enables computational systems to process, understand, and generate human-like language. It encompasses many problems, from low-level linguistic tasks, such as detecting the part-of-speech of words in a sentence or extracting named entities, to higher-level tasks, such as translation, summarization, or maintaining conversations with a human interlocutor.\nThe most successful approaches in modern NLP leverage transformer networks, a special type of neural network architecture that can represent some forms of contextual information more easily than other architectures. These are the mathematical underpinnings behind technologies like large language models and chatbots. So we will finish this part, and the whole book, with a look at modern LLMs, how they work, and what we can expect from them in the near future.",
    "crumbs": [
      "Artificial Intelligence"
    ]
  }
]